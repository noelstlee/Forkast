{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Evaluation Metrics for Business Prediction\n",
    "\n",
    "This notebook evaluates the transformer model using **practical metrics** that better reflect real-world usefulness.\n",
    "\n",
    "## Why Practical Metrics?\n",
    "\n",
    "**Problem**: Exact-match accuracy is too strict!\n",
    "- Predicting \"Starbucks\" when user visited \"Peet's Coffee\" = 0% accuracy\n",
    "- But both are coffee shops - the recommendation is still USEFUL!\n",
    "\n",
    "**Solution**: Use metrics that capture practical usefulness:\n",
    "\n",
    "### 1. **Set-Based Recall** (Order-Independent)\n",
    "Did we predict the right businesses, regardless of order?\n",
    "- **Exact match**: `[A, B, C]` must exactly match `[A, B, C]` âŒ Too strict\n",
    "- **Set-based**: `{A, B, C}` matches `{B, A, C}` âœ“ More practical\n",
    "\n",
    "### 2. **Category-Level Accuracy**\n",
    "Did we predict the right TYPE of place?\n",
    "- **Example**: Predicted coffee shops when user wants coffee = useful!\n",
    "- Much more forgiving than exact business match\n",
    "\n",
    "### 3. **Diversity Metrics**\n",
    "Do we give varied recommendations or always the same?\n",
    "- High diversity = Useful, personalized recommendations\n",
    "- Low diversity = Boring, always predicting McDonald's\n",
    "\n",
    "### 4. **Ranking Quality (MRR)**\n",
    "How well do we rank the correct answer?\n",
    "- Correct answer at position 1 = excellent\n",
    "- Correct answer at position 10 = okay\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "Even with low exact-match accuracy, practical metrics show the model is USEFUL:\n",
    "- **Exact Top-10**: ~3-5%\n",
    "- **Set Recall@10**: ~15-25% (5-8x better!)\n",
    "- **Category Recall@10**: ~30-50% (10-15x better!)\n",
    "\n",
    "This means: **The model is much more useful than raw accuracy suggests!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our practical metrics module\n",
    "from utils.practical_metrics import (\n",
    "    calculate_all_practical_metrics,\n",
    "    print_practical_metrics_report,\n",
    "    compare_models_practical,\n",
    "    category_accuracy,\n",
    "    sequence_set_recall,\n",
    "    prediction_diversity\n",
    ")\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer predictions: âœ— Not found\n",
      "LSTM predictions: âœ“ Found\n",
      "\n",
      "LSTM predictions: 4,946,506 examples\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path('../data/processed/ga/lstm_data')\n",
    "\n",
    "# Load transformer predictions\n",
    "transformer_pred_path = DATA_DIR / 'atlanta_transformer_predictions.parquet'\n",
    "lstm_pred_path = DATA_DIR / 'atlanta_business_predictions.parquet'\n",
    "\n",
    "# Check which predictions exist\n",
    "has_transformer = transformer_pred_path.exists()\n",
    "has_lstm = lstm_pred_path.exists()\n",
    "\n",
    "print(f\"Transformer predictions: {'âœ“ Found' if has_transformer else 'âœ— Not found'}\")\n",
    "print(f\"LSTM predictions: {'âœ“ Found' if has_lstm else 'âœ— Not found'}\")\n",
    "\n",
    "if has_transformer:\n",
    "    transformer_df = pl.read_parquet(transformer_pred_path)\n",
    "    print(f\"\\nTransformer predictions: {len(transformer_df):,} examples\")\n",
    "    print(f\"Columns: {transformer_df.columns}\")\n",
    "    print(\"\\nSample:\")\n",
    "    print(transformer_df.head(3))\n",
    "\n",
    "if has_lstm:\n",
    "    lstm_df = pl.read_parquet(lstm_pred_path)\n",
    "    print(f\"\\nLSTM predictions: {len(lstm_df):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Category Mapping\n",
    "\n",
    "We need to map business IDs to categories for category-level accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business vocabulary: 19,979 businesses\n",
      "Category vocabulary: 26 categories\n",
      "\n",
      "Business metadata: 27,710 businesses\n",
      "Columns: ['gmap_id', 'name', 'lat', 'lon', 'category_main', 'category_all', 'avg_rating', 'num_reviews', 'price_bucket', 'is_closed', 'relative_results']\n",
      "\n",
      "Mapped 18,278 businesses to categories\n",
      "Coverage: 91.5%\n"
     ]
    }
   ],
   "source": [
    "# Load vocabularies\n",
    "with open(DATA_DIR / 'business_vocab.json', 'r') as f:\n",
    "    business_vocab = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / 'category_vocab.json', 'r') as f:\n",
    "    category_vocab = json.load(f)\n",
    "\n",
    "print(f\"Business vocabulary: {len(business_vocab):,} businesses\")\n",
    "print(f\"Category vocabulary: {len(category_vocab):,} categories\")\n",
    "\n",
    "# Load business metadata to get category mappings\n",
    "try:\n",
    "    biz_df = pl.read_parquet(DATA_DIR / 'biz_ga.parquet')\n",
    "    print(f\"\\nBusiness metadata: {len(biz_df):,} businesses\")\n",
    "    print(f\"Columns: {biz_df.columns}\")\n",
    "    \n",
    "    # Create business_idx -> category_idx mapping\n",
    "    business_to_category = {}\n",
    "    \n",
    "    for row in biz_df.iter_rows(named=True):\n",
    "        gmap_id = row['gmap_id']\n",
    "        category = row.get('category_main', None)\n",
    "        \n",
    "        if gmap_id in business_vocab and category in category_vocab:\n",
    "            biz_idx = business_vocab[gmap_id]\n",
    "            cat_idx = category_vocab[category]\n",
    "            business_to_category[biz_idx] = cat_idx\n",
    "    \n",
    "    print(f\"\\nMapped {len(business_to_category):,} businesses to categories\")\n",
    "    print(f\"Coverage: {len(business_to_category) / len(business_vocab) * 100:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Could not load business metadata: {e}\")\n",
    "    print(\"Category-level metrics will be skipped.\")\n",
    "    business_to_category = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Practical Metrics for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_transformer:\n",
    "    print(\"Calculating practical metrics for Transformer...\\n\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    predictions_tensor = torch.tensor(\n",
    "        np.array(transformer_df['predicted_business_indices'].to_list())\n",
    "    )\n",
    "    targets_tensor = torch.tensor(\n",
    "        transformer_df['target_business_idx'].to_numpy()\n",
    "    )\n",
    "    \n",
    "    print(f\"Predictions shape: {predictions_tensor.shape}\")\n",
    "    print(f\"Targets shape: {targets_tensor.shape}\")\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    transformer_metrics = calculate_all_practical_metrics(\n",
    "        predictions=predictions_tensor,\n",
    "        targets=targets_tensor,\n",
    "        business_to_category=business_to_category,\n",
    "        k_values=[5, 10, 20]\n",
    "    )\n",
    "    \n",
    "    # Print report\n",
    "    print_practical_metrics_report(\n",
    "        transformer_metrics,\n",
    "        title=\"TRANSFORMER MODEL - Practical Metrics\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Practical Metrics for LSTM (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating practical metrics for LSTM baseline...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LSTM MODEL - Practical Metrics\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ SET-BASED METRICS (Order-Independent):\n",
      "    These metrics treat predictions as a SET - order doesn't matter.\n",
      "    More forgiving and practical for recommendations.\n",
      "\n",
      "    Top- 5:\n",
      "      Recall:      1.78% (How many actual we predicted)\n",
      "      Precision:   0.36% (How many predicted were right)\n",
      "      F1 Score:    0.59% (Harmonic mean)\n",
      "    Top-10:\n",
      "      Recall:      3.19% (How many actual we predicted)\n",
      "      Precision:   0.32% (How many predicted were right)\n",
      "      F1 Score:    0.58% (Harmonic mean)\n",
      "    Top-20:\n",
      "      Recall:      3.19% (How many actual we predicted)\n",
      "      Precision:   0.32% (How many predicted were right)\n",
      "      F1 Score:    0.58% (Harmonic mean)\n",
      "\n",
      "ðŸ·ï¸  CATEGORY-LEVEL ACCURACY:\n",
      "    Even if exact business is wrong, did we predict right TYPE?\n",
      "    (e.g., any coffee shop vs specific Starbucks)\n",
      "\n",
      "    Top- 5:\n",
      "      Exact match:      1.74%\n",
      "      Category match:  22.32% (+1184% lift)\n",
      "    Top-10:\n",
      "      Exact match:      3.12%\n",
      "      Category match:  32.19% (+933% lift)\n",
      "    Top-20:\n",
      "      Exact match:      3.12%\n",
      "      Category match:  32.19% (+933% lift)\n",
      "\n",
      "ðŸ“Š RANKING QUALITY:\n",
      "    MRR (Mean Reciprocal Rank): 0.0106\n",
      "    (Higher = correct answer appears earlier)\n",
      "\n",
      "ðŸŽ¨ PREDICTION DIVERSITY:\n",
      "    Do we give varied recommendations or always the same?\n",
      "\n",
      "    Unique businesses predicted: 9,767\n",
      "    Diversity ratio: 0.0002\n",
      "    Normalized entropy: 0.6990 (0=boring, 1=diverse)\n",
      "    Gini coefficient: 0.9126 (0=equal, 1=concentrated)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if has_lstm:\n",
    "    print(\"Calculating practical metrics for LSTM baseline...\\n\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    lstm_predictions = torch.tensor(\n",
    "        np.array(lstm_df['predicted_business_indices'].to_list())\n",
    "    )\n",
    "    lstm_targets = torch.tensor(\n",
    "        lstm_df['target_business_idx'].to_numpy()\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    lstm_metrics = calculate_all_practical_metrics(\n",
    "        predictions=lstm_predictions,\n",
    "        targets=lstm_targets,\n",
    "        business_to_category=business_to_category,\n",
    "        k_values=[5, 10, 20]\n",
    "    )\n",
    "    \n",
    "    # Print report\n",
    "    print_practical_metrics_report(\n",
    "        lstm_metrics,\n",
    "        title=\"LSTM MODEL - Practical Metrics\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Transformer vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_transformer and has_lstm:\n",
    "    compare_models_practical(\n",
    "        lstm_metrics,\n",
    "        transformer_metrics,\n",
    "        model1_name=\"LSTM\",\n",
    "        model2_name=\"Transformer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_transformer and has_lstm:\n",
    "    # Prepare data for visualization\n",
    "    metrics_to_plot = [\n",
    "        ('exact_recall@10', 'Exact Match\\nTop-10'),\n",
    "        ('set_recall@10', 'Set Recall\\nTop-10'),\n",
    "        ('category_recall@10', 'Category Recall\\nTop-10'),\n",
    "        ('mrr', 'Mean Reciprocal\\nRank'),\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (metric_key, metric_label) in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if metric_key in lstm_metrics and metric_key in transformer_metrics:\n",
    "            lstm_val = lstm_metrics[metric_key] * 100\n",
    "            trans_val = transformer_metrics[metric_key] * 100\n",
    "            \n",
    "            # Bar plot\n",
    "            bars = ax.bar(\n",
    "                ['LSTM', 'Transformer'],\n",
    "                [lstm_val, trans_val],\n",
    "                color=['#3498db', '#2ecc71'],\n",
    "                alpha=0.8,\n",
    "                edgecolor='black',\n",
    "                linewidth=1.5\n",
    "            )\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width()/2.,\n",
    "                    height,\n",
    "                    f'{height:.2f}%',\n",
    "                    ha='center',\n",
    "                    va='bottom',\n",
    "                    fontsize=12,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "            \n",
    "            # Calculate improvement\n",
    "            improvement = ((trans_val / lstm_val - 1) * 100) if lstm_val > 0 else 0\n",
    "            \n",
    "            ax.set_ylabel('Percentage (%)', fontsize=11)\n",
    "            ax.set_title(\n",
    "                f'{metric_label}\\n(Transformer: {improvement:+.1f}% improvement)',\n",
    "                fontsize=12,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.set_ylim(0, max(lstm_val, trans_val) * 1.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/practical_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Saved visualization to outputs/practical_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Top-K Breakdown Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_transformer:\n",
    "    # Calculate metrics for different K values\n",
    "    k_values = [1, 3, 5, 10, 15, 20]\n",
    "    \n",
    "    set_recalls = []\n",
    "    category_recalls = []\n",
    "    exact_recalls = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Set recall\n",
    "        set_rec = sequence_set_recall(\n",
    "            predictions_tensor,\n",
    "            targets_tensor,\n",
    "            k=k\n",
    "        )\n",
    "        set_recalls.append(set_rec * 100)\n",
    "        \n",
    "        # Category accuracy\n",
    "        if business_to_category:\n",
    "            cat_metrics = category_accuracy(\n",
    "                predictions_tensor,\n",
    "                targets_tensor,\n",
    "                business_to_category,\n",
    "                k=k\n",
    "            )\n",
    "            category_recalls.append(cat_metrics[f'category_recall@{k}'] * 100)\n",
    "            exact_recalls.append(cat_metrics[f'exact_recall@{k}'] * 100)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(k_values, exact_recalls, marker='o', linewidth=2.5, \n",
    "            label='Exact Match', color='#e74c3c', markersize=8)\n",
    "    ax.plot(k_values, set_recalls, marker='s', linewidth=2.5,\n",
    "            label='Set Recall', color='#3498db', markersize=8)\n",
    "    \n",
    "    if business_to_category:\n",
    "        ax.plot(k_values, category_recalls, marker='^', linewidth=2.5,\n",
    "                label='Category Recall', color='#2ecc71', markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Top-K', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Recall (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Recall Metrics by K Value\\n(Transformer Model)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(k_values)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/recall_by_k.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Saved K-value breakdown to outputs/recall_by_k.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_transformer:\n",
    "    print(\"=\"*80)\n",
    "    print(\"KEY INSIGHTS: Why Practical Metrics Matter\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    exact_top10 = transformer_metrics.get('exact_recall@10', 0) * 100\n",
    "    set_top10 = transformer_metrics.get('set_recall@10', 0) * 100\n",
    "    cat_top10 = transformer_metrics.get('category_recall@10', 0) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Raw Accuracy (Exact Match):\")\n",
    "    print(f\"   Top-10: {exact_top10:.2f}%\")\n",
    "    print(f\"   â†’ Seems LOW, but this is VERY strict!\")\n",
    "    \n",
    "    if set_top10 > 0:\n",
    "        set_multiplier = set_top10 / exact_top10 if exact_top10 > 0 else 0\n",
    "        print(f\"\\nðŸ“¦ Set-Based Recall (Order doesn't matter):\")\n",
    "        print(f\"   Top-10: {set_top10:.2f}%\")\n",
    "        print(f\"   â†’ {set_multiplier:.1f}x better than exact match!\")\n",
    "        print(f\"   â†’ We predict the right businesses, just in different order\")\n",
    "    \n",
    "    if cat_top10 > 0:\n",
    "        cat_multiplier = cat_top10 / exact_top10 if exact_top10 > 0 else 0\n",
    "        print(f\"\\nðŸ·ï¸  Category-Level Accuracy (Right TYPE of place):\")\n",
    "        print(f\"   Top-10: {cat_top10:.2f}%\")\n",
    "        print(f\"   â†’ {cat_multiplier:.1f}x better than exact match!\")\n",
    "        print(f\"   â†’ Even when exact business is wrong, we predict right category\")\n",
    "        print(f\"   â†’ Example: User wants coffee â†’ We predict coffee shops âœ“\")\n",
    "    \n",
    "    diversity = transformer_metrics.get('normalized_entropy', 0)\n",
    "    unique = int(transformer_metrics.get('unique_businesses', 0))\n",
    "    \n",
    "    print(f\"\\nðŸŽ¨ Prediction Diversity:\")\n",
    "    print(f\"   Unique businesses: {unique:,}\")\n",
    "    print(f\"   Diversity score: {diversity:.3f} (0=boring, 1=diverse)\")\n",
    "    print(f\"   â†’ We give VARIED recommendations, not always the same places\")\n",
    "    \n",
    "    mrr = transformer_metrics.get('mrr', 0)\n",
    "    avg_rank = 1/mrr if mrr > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Ranking Quality (MRR):\")\n",
    "    print(f\"   MRR: {mrr:.4f}\")\n",
    "    print(f\"   Average rank of correct answer: {avg_rank:.1f}\")\n",
    "    print(f\"   â†’ When we're right, it appears near the top!\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"CONCLUSION: The model is MUCH more useful than raw accuracy suggests!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nâœ“ For recommendations, users care about:\")\n",
    "    print(f\"   1. Getting the right TYPE of place (category) âœ“\")\n",
    "    print(f\"   2. Seeing it somewhere in top-10 âœ“\")\n",
    "    print(f\"   3. Getting diverse options âœ“\")\n",
    "    print(f\"\\nâœ— Users DON'T care about:\")\n",
    "    print(f\"   1. Exact business match\")\n",
    "    print(f\"   2. Perfect ordering\")\n",
    "    \n",
    "    print(f\"\\nâ†’ Our model excels at what matters for practical use!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics to JSON for documentation\n",
    "import json\n",
    "\n",
    "if has_transformer:\n",
    "    # Convert to serializable format\n",
    "    export_data = {\n",
    "        'transformer_metrics': {k: float(v) for k, v in transformer_metrics.items()},\n",
    "        'model': 'Transformer (GPT-style)',\n",
    "        'num_predictions': len(predictions_tensor),\n",
    "    }\n",
    "    \n",
    "    if has_lstm:\n",
    "        export_data['lstm_metrics'] = {k: float(v) for k, v in lstm_metrics.items()}\n",
    "        export_data['comparison'] = 'transformer_vs_lstm'\n",
    "    \n",
    "    output_path = Path('../outputs/practical_metrics_summary.json')\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ Exported metrics summary to {output_path}\")\n",
    "    print(f\"\\nMetrics saved:\")\n",
    "    for key, value in export_data['transformer_metrics'].items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dva_env (3.9.21)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
