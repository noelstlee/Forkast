{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Full Pipeline: Training with Enhanced Features\n",
        "\n",
        "This notebook walks through the complete XGBoost training pipeline with all the enhanced features:\n",
        "- Review sentiment analysis\n",
        "- User behavioral profiling\n",
        "- Cuisine complementarity\n",
        "- Operating hours analysis\n",
        "- Topic extraction\n",
        "- Service options\n",
        "\n",
        "**Inputs (from SSD):**\n",
        "- `/Volumes/SunnySSD/review-Georgia.json` (7.2GB)\n",
        "- `/Volumes/SunnySSD/meta-Georgia.json` (168MB)\n",
        "\n",
        "**Outputs:**\n",
        "- Processed Parquet files in `data/processed/ga/`\n",
        "- Trained XGBoost model\n",
        "- Feature importance analysis\n",
        "- Transition probability matrix\n",
        "- Evaluation metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Paths and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Setup complete\n",
            "  Base directory: /Users/sunho/Forkast\n",
            "  SSD path: /Volumes/SunnySSD\n",
            "  Processed output: /Users/sunho/Forkast/data/processed/ga\n",
            "  Review JSON exists: True\n",
            "  Meta JSON exists: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "base_dir = Path.cwd().parent\n",
        "sys.path.append(str(base_dir / 'src'))\n",
        "\n",
        "# Define paths\n",
        "SSD_PATH = Path(\"/Volumes/SunnySSD\")\n",
        "PROCESSED_DIR = base_dir / \"data\" / \"processed\" / \"ga\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# SSD data files\n",
        "REVIEW_JSON = SSD_PATH / \"review-Georgia.json\"\n",
        "META_JSON = SSD_PATH / \"meta-Georgia.json\"\n",
        "\n",
        "# Output paths\n",
        "REVIEWS_PARQUET = PROCESSED_DIR / \"reviews_ga.parquet\"\n",
        "BIZ_PARQUET = PROCESSED_DIR / \"biz_ga.parquet\"\n",
        "\n",
        "print(\"âœ“ Setup complete\")\n",
        "print(f\"  Base directory: {base_dir}\")\n",
        "print(f\"  SSD path: {SSD_PATH}\")\n",
        "print(f\"  Processed output: {PROCESSED_DIR}\")\n",
        "print(f\"  Review JSON exists: {REVIEW_JSON.exists()}\")\n",
        "print(f\"  Meta JSON exists: {META_JSON.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A1: Data Ingestion & Normalization\n",
        "\n",
        "**What this does:**\n",
        "- Loads raw JSON files from SSD\n",
        "- Filters to Georgia geographic bounds\n",
        "- Normalizes categories and prices\n",
        "- **Extracts operating hours and service options** (NEW!)\n",
        "- Converts to efficient Parquet format\n",
        "\n",
        "**Expected time:** 10-20 minutes for 7GB review file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A1: DATA INGESTION\n",
            "================================================================================\n",
            "\n",
            "[1/2] Ingesting metadata...\n",
            "\n",
            "================================================================================\n",
            "PHASE A1: INGESTING METADATA\n",
            "================================================================================\n",
            "Input: /Volumes/SunnySSD/meta-Georgia.json\n",
            "Output: /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet\n",
            "\n",
            "[1/6] Reading JSON file...\n",
            "  Loaded 166,381 raw businesses\n",
            "\n",
            "[2/6] Filtering to Georgia geographic bounds...\n",
            "  Retained 166,334 businesses in Georgia\n",
            "\n",
            "[3/6] Parsing price buckets...\n",
            "\n",
            "[4/6] Detecting closed businesses...\n",
            "\n",
            "[5/9] Normalizing categories...\n",
            "\n",
            "[6/9] Filtering to food-only businesses...\n",
            "  Retained 27,757 food-related businesses\n",
            "\n",
            "[7/9] Parsing operating hours...\n",
            "\n",
            "[8/9] Extracting service options...\n",
            "\n",
            "[9/9] Finalizing schema...\n",
            "  Final count: 27,710 unique businesses\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet...\n",
            "\n",
            "âœ“ Metadata ingestion complete!\n",
            "  Output size: 5.4 MB\n",
            "\n",
            "âœ“ Metadata ingested:\n",
            "  Total businesses: 27,710\n",
            "  Columns: ['gmap_id', 'name', 'lat', 'lon', 'category_main', 'category_all', 'avg_rating', 'num_reviews', 'price_bucket', 'is_closed', 'relative_results', 'operating_hours_parsed', 'days_open_count', 'avg_hours_per_day', 'has_late_night', 'is_24hr', 'is_weekend_only', 'has_delivery', 'has_takeout', 'has_dinein', 'accepts_reservations', 'has_quick_visit', 'requires_mask']\n",
            "  New metadata columns: operating_hours_parsed, has_delivery, has_takeout, etc.\n"
          ]
        }
      ],
      "source": [
        "# Reload the module to get the latest changes\n",
        "import importlib\n",
        "import data.ingest\n",
        "importlib.reload(data.ingest)\n",
        "from data.ingest import ingest_metadata, ingest_reviews\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A1: DATA INGESTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ingest metadata\n",
        "print(\"\\n[1/2] Ingesting metadata...\")\n",
        "biz_df, valid_ids = ingest_metadata(\n",
        "    str(META_JSON),\n",
        "    str(BIZ_PARQUET)\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Metadata ingested:\")\n",
        "print(f\"  Total businesses: {len(biz_df):,}\")\n",
        "print(f\"  Columns: {list(biz_df.columns)}\")\n",
        "print(f\"  New metadata columns: operating_hours_parsed, has_delivery, has_takeout, etc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample metadata with new columns:\n",
            "shape: (5, 8)\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ gmap_id    â”† name       â”† category_m â”† has_delive â”† has_takeou â”† has_late_ â”† is_24hr â”† days_open â”‚\n",
            "â”‚ ---        â”† ---        â”† ain        â”† ry         â”† t          â”† night     â”† ---     â”† _count    â”‚\n",
            "â”‚ str        â”† str        â”† ---        â”† ---        â”† ---        â”† ---       â”† bool    â”† ---       â”‚\n",
            "â”‚            â”†            â”† str        â”† bool       â”† bool       â”† bool      â”†         â”† i8        â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ 0x88e52650 â”† McDonald's â”† burger     â”† true       â”† true       â”† false     â”† true    â”† 7         â”‚\n",
            "â”‚ 3404ad41:0 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ x22699e0b1 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88f50610 â”† Panera     â”† breakfast  â”† true       â”† true       â”† false     â”† false   â”† 7         â”‚\n",
            "â”‚ 80f866d3:0 â”† Bread      â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xf9c365cc4 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88f5a63d â”† Sangria's  â”† cafe       â”† true       â”† false      â”† false     â”† false   â”† null      â”‚\n",
            "â”‚ 2afacd01:0 â”† Mexican    â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xe0e06bbd3 â”† Cafe       â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88f5ef81 â”† The Roman  â”† pizza      â”† true       â”† true       â”† false     â”† false   â”† 6         â”‚\n",
            "â”‚ 772a406b:0 â”† Oven       â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xc90924dee â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88eda948 â”† Harveys    â”† fast_food  â”† true       â”† false      â”† false     â”† false   â”† 7         â”‚\n",
            "â”‚ 8bb0020b:0 â”† Supermarke â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xe21abd059 â”† t          â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# Preview metadata with new columns\n",
        "print(\"\\nSample metadata with new columns:\")\n",
        "print(biz_df.select([\n",
        "    \"gmap_id\", \"name\", \"category_main\", \"has_delivery\", \n",
        "    \"has_takeout\", \"has_late_night\", \"is_24hr\", \"days_open_count\"\n",
        "]).head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[2/2] Ingesting reviews...\n",
            "  This may take 10-20 minutes for the 7GB file...\n",
            "================================================================================\n",
            "PHASE A1: INGESTING REVIEWS\n",
            "================================================================================\n",
            "Input: /Volumes/SunnySSD/review-Georgia.json\n",
            "Output: /Users/sunho/Forkast/data/processed/ga/reviews_ga.parquet\n",
            "\n",
            "[1/5] Reading JSON file...\n",
            "  Loaded 24,060,125 raw reviews\n",
            "\n",
            "[2/5] Converting timestamps...\n",
            "\n",
            "[3/5] Filtering invalid timestamps...\n",
            "  Retained 24,060,120 reviews with valid timestamps\n",
            "\n",
            "[4/5] Creating derived columns...\n",
            "\n",
            "  Filtering out reviews with missing user_id or rating...\n",
            "  Removed 167,538 reviews with null user_id or rating\n",
            "  Retained 23,892,582 reviews\n",
            "\n",
            "  Filtering to 27,710 valid businesses...\n",
            "  Retained 10,494,609 reviews\n",
            "\n",
            "[5/5] Deduplicating...\n",
            "  Final count: 10,339,035 unique reviews\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/reviews_ga.parquet...\n",
            "\n",
            "âœ“ Reviews ingestion complete!\n",
            "  Output size: 787.1 MB\n",
            "\n",
            "âœ“ Reviews ingested:\n",
            "  Total reviews: 10,339,035\n",
            "  Unique users: 2,546,362\n",
            "  Unique restaurants: 27,710\n",
            "  Date range: 2001-01-06 00:00:00 to 2021-09-08 01:43:37\n"
          ]
        }
      ],
      "source": [
        "# Ingest reviews\n",
        "print(\"\\n[2/2] Ingesting reviews...\")\n",
        "print(\"  This may take 10-20 minutes for the 7GB file...\")\n",
        "\n",
        "reviews_df = ingest_reviews(\n",
        "    str(REVIEW_JSON),\n",
        "    str(REVIEWS_PARQUET),\n",
        "    biz_gmap_ids=valid_ids\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Reviews ingested:\")\n",
        "print(f\"  Total reviews: {len(reviews_df):,}\")\n",
        "print(f\"  Unique users: {reviews_df['user_id'].n_unique():,}\")\n",
        "print(f\"  Unique restaurants: {reviews_df['gmap_id'].n_unique():,}\")\n",
        "print(f\"  Date range: {reviews_df['ts'].min()} to {reviews_df['ts'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A2: User Sequence Derivation\n",
        "\n",
        "**What this does:**\n",
        "- Creates user visit sequences sorted by timestamp\n",
        "- Generates consecutive visit pairs (A â†’ B)\n",
        "- Filters pairs within 0-168 hours (1 week window)\n",
        "\n",
        "**Expected time:** 5-10 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A2: USER SEQUENCE DERIVATION\n",
            "================================================================================\n",
            "Loading data...\n",
            "  Loaded 10,339,035 reviews\n",
            "  Loaded 27,710 businesses\n",
            "================================================================================\n",
            "DERIVING USER SEQUENCES\n",
            "================================================================================\n",
            "\n",
            "[1/4] Joining reviews with business metadata...\n",
            "  Joined 10,339,035 reviews with business data\n",
            "\n",
            "[2/4] Sorting by user and timestamp...\n",
            "\n",
            "[3/4] Creating sequence indices...\n",
            "\n",
            "[4/4] Final sequence count: 10,339,035\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/user_sequences_ga.parquet...\n",
            "  Output size: 256.8 MB\n",
            "\n",
            "================================================================================\n",
            "DERIVING CONSECUTIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/5] Creating shifted columns for next visit...\n",
            "\n",
            "[2/5] Filtering out null destinations (last visit in sequence)...\n",
            "  Retained 7,792,673 pairs\n",
            "\n",
            "[3/5] Calculating time delta...\n",
            "\n",
            "[4/5] Filtering by time window...\n",
            "  Retained 4,152,155 pairs within 0-168 hour window\n",
            "\n",
            "[5/5] Final pair count: 4,152,155\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/pairs_ga.parquet...\n",
            "  Output size: 193.6 MB\n",
            "\n",
            "================================================================================\n",
            "SEQUENCE STATISTICS\n",
            "================================================================================\n",
            "\n",
            "USER SEQUENCES:\n",
            "  Total visits: 10,339,035\n",
            "  Unique users: 2,546,362\n",
            "  Unique businesses: 27,710\n",
            "\n",
            "SEQUENCE LENGTH DISTRIBUTION:\n",
            "  Mean: 4.1\n",
            "  Median: 1\n",
            "  Max: 648\n",
            "  Users with 2+ visits: 1,135,876 (44.6%)\n",
            "\n",
            "CONSECUTIVE PAIRS:\n",
            "  Total pairs: 4,152,155\n",
            "  Unique users: 744,680\n",
            "  Unique src businesses: 26,893\n",
            "  Unique dst businesses: 26,891\n",
            "\n",
            "TIME DELTA DISTRIBUTION:\n",
            "  Mean: 16.5 hours\n",
            "  Median: 0.0 hours\n",
            "  Min: 0.00 hours\n",
            "  Max: 168.0 hours\n",
            "\n",
            "TOP CATEGORY TRANSITIONS:\n",
            "  burger          â†’ burger         : 159,029\n",
            "  burger          â†’ american       : 102,702\n",
            "  american        â†’ american       : 102,405\n",
            "  american        â†’ burger         : 101,937\n",
            "  burger          â†’ fast_food      : 94,718\n",
            "  fast_food       â†’ burger         : 76,100\n",
            "  fast_food       â†’ fast_food      : 72,137\n",
            "  burger          â†’ mexican        : 63,425\n",
            "  mexican         â†’ burger         : 63,038\n",
            "  american        â†’ fast_food      : 59,156\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âœ“âœ“âœ“ PHASE A2 COMPLETE âœ“âœ“âœ“\n",
            "\n",
            "\n",
            "âœ“ Sequences created:\n",
            "  Total pairs: 4,152,155\n",
            "  Unique source restaurants: 26,893\n",
            "  Unique destination restaurants: 26,891\n",
            "  Average time gap: 16.5 hours\n"
          ]
        }
      ],
      "source": [
        "from data.sequences import main as sequences_main\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A2: USER SEQUENCE DERIVATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run sequence derivation\n",
        "sequences_main()\n",
        "\n",
        "# Load and preview results\n",
        "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_ga.parquet\")\n",
        "\n",
        "print(f\"\\nâœ“ Sequences created:\")\n",
        "print(f\"  Total pairs: {len(pairs_df):,}\")\n",
        "print(f\"  Unique source restaurants: {pairs_df['src_gmap_id'].n_unique():,}\")\n",
        "print(f\"  Unique destination restaurants: {pairs_df['dst_gmap_id'].n_unique():,}\")\n",
        "print(f\"  Average time gap: {pairs_df['delta_hours'].mean():.1f} hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample consecutive visit pairs:\n",
            "shape: (10, 6)\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ user_id        â”† src_gmap_id    â”† dst_gmap_id    â”† delta_hours â”† src_ts         â”† dst_ts         â”‚\n",
            "â”‚ ---            â”† ---            â”† ---            â”† ---         â”† ---            â”† ---            â”‚\n",
            "â”‚ str            â”† str            â”† str            â”† f64         â”† datetime[Î¼s]   â”† datetime[Î¼s]   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ 10000000713488 â”† 0x88f5758473a8 â”† 0x88f5759cc6ea â”† 0.019444    â”† 2019-05-14     â”† 2019-05-14     â”‚\n",
            "â”‚ 6560887        â”† bc37:0x9367635 â”† f6a9:0x16a87fc â”†             â”† 02:00:43       â”† 02:01:53       â”‚\n",
            "â”‚                â”† abâ€¦            â”† b6â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000000713488 â”† 0x88f5759cc6ea â”† 0x88f59b65e7b2 â”† 0.013333    â”† 2019-05-14     â”† 2019-05-14     â”‚\n",
            "â”‚ 6560887        â”† f6a9:0x16a87fc â”† e153:0x208b4ad â”†             â”† 02:01:53       â”† 02:02:41       â”‚\n",
            "â”‚                â”† b6â€¦            â”† 7bâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000002095889 â”† 0x886069e02d76 â”† 0x886068a3bba5 â”† 0.010833    â”† 2019-01-18     â”† 2019-01-18     â”‚\n",
            "â”‚ 5295779        â”† edc3:0x3edd947 â”† 2f37:0x7a7c672 â”†             â”† 08:54:52       â”† 08:55:31       â”‚\n",
            "â”‚                â”† 21â€¦            â”† c1â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000002095889 â”† 0x886068a3bba5 â”† 0x8860662a7353 â”† 0.030833    â”† 2019-01-18     â”† 2019-01-18     â”‚\n",
            "â”‚ 5295779        â”† 2f37:0x7a7c672 â”† 7617:0x555a125 â”†             â”† 08:55:31       â”† 08:57:22       â”‚\n",
            "â”‚                â”† c1â€¦            â”† 33â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000002095889 â”† 0x88606899517f â”† 0x88f4e9c68ee5 â”† 0.074444    â”† 2019-06-13     â”† 2019-06-13     â”‚\n",
            "â”‚ 5295779        â”† 7c39:0x33b774d â”† c59d:0x1e1d13e â”†             â”† 16:46:52       â”† 16:51:20       â”‚\n",
            "â”‚                â”† f5â€¦            â”† baâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000002095889 â”† 0x88f4e9c68ee5 â”† 0x88f540feacdf â”† 0.035833    â”† 2019-06-13     â”† 2019-06-13     â”‚\n",
            "â”‚ 5295779        â”† c59d:0x1e1d13e â”† ed3f:0x18f90c6 â”†             â”† 16:51:20       â”† 16:53:29       â”‚\n",
            "â”‚                â”† baâ€¦            â”† 79â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003646453 â”† 0x88f5095f563d â”† 0x88f538384621 â”† 130.966389  â”† 2018-07-06     â”† 2018-07-11     â”‚\n",
            "â”‚ 0353686        â”† f8cb:0x4ebe1ca â”† c095:0x5ffd453 â”†             â”† 04:33:52       â”† 15:31:51       â”‚\n",
            "â”‚                â”† ccâ€¦            â”† c5â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003984331 â”† 0x88f5be87ef51 â”† 0x88f5bc57a0f7 â”† 142.004167  â”† 2018-01-19     â”† 2018-01-25     â”‚\n",
            "â”‚ 3841630        â”† 1c85:0x92f9ad1 â”† 8431:0xde92824 â”†             â”† 17:52:58       â”† 15:53:13       â”‚\n",
            "â”‚                â”† a4â€¦            â”† 81â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003984331 â”† 0x88f5bc57a0f7 â”† 0x88f5ba23a8bb â”† 0.013056    â”† 2018-01-25     â”† 2018-01-25     â”‚\n",
            "â”‚ 3841630        â”† 8431:0xde92824 â”† d8b9:0xe391237 â”†             â”† 15:53:13       â”† 15:54:00       â”‚\n",
            "â”‚                â”† 81â€¦            â”† 7fâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003984331 â”† 0x88f5ba23a8bb â”† 0x88f5953d7e77 â”† 0.006389    â”† 2018-01-25     â”† 2018-01-25     â”‚\n",
            "â”‚ 3841630        â”† d8b9:0xe391237 â”† 4547:0xa3adf61 â”†             â”† 15:54:00       â”† 15:54:23       â”‚\n",
            "â”‚                â”† 7fâ€¦            â”† 09â€¦            â”†             â”†                â”†                â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# Preview sample pairs\n",
        "print(\"\\nSample consecutive visit pairs:\")\n",
        "print(pairs_df.select([\n",
        "    \"user_id\", \"src_gmap_id\", \"dst_gmap_id\", \n",
        "    \"delta_hours\", \"src_ts\", \"dst_ts\"\n",
        "]).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A2.5: Data Quality Filtering\n",
        "\n",
        "**What this does:**\n",
        "- Re-categorizes generic 'restaurant' businesses\n",
        "- Filters to users with 5+ visits\n",
        "- Removes pairs with very short time deltas\n",
        "\n",
        "**Expected time:** 2-5 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A2.5: DATA QUALITY FILTERING\n",
            "================================================================================\n",
            "================================================================================\n",
            "PHASE A2.5: DATA QUALITY FILTERING\n",
            "================================================================================\n",
            "\n",
            "Inputs:\n",
            "  - /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet\n",
            "  - /Users/sunho/Forkast/data/processed/ga/user_sequences_ga.parquet\n",
            "  - /Users/sunho/Forkast/data/processed/ga/pairs_ga.parquet\n",
            "\n",
            "Outputs:\n",
            "  - /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet (updated)\n",
            "  - /Users/sunho/Forkast/data/processed/ga/user_sequences_filtered_ga.parquet\n",
            "  - /Users/sunho/Forkast/data/processed/ga/pairs_filtered_ga.parquet\n",
            "\n",
            "[Loading data...]\n",
            "\n",
            "================================================================================\n",
            "STEP 1: RE-CATEGORIZING 'RESTAURANT' BUSINESSES\n",
            "================================================================================\n",
            "\n",
            "Original 'restaurant' businesses: 4,490\n",
            "\n",
            "[1/3] Inferring categories from business names...\n",
            "\n",
            "[2/3] Re-categorization results:\n",
            "  Successfully re-categorized: 1,883\n",
            "  Moved to 'other': 2,607\n",
            "\n",
            "[3/3] New category distribution (top 15):\n",
            "   1. fast_food           :  3,396 ( 12.3%)\n",
            "   2. american            :  2,759 ( 10.0%)\n",
            "   3. other               :  2,607 (  9.4%)\n",
            "   4. mexican             :  2,301 (  8.3%)\n",
            "   5. pizza               :  2,200 (  7.9%)\n",
            "   6. burger              :  2,136 (  7.7%)\n",
            "   7. bar                 :  1,587 (  5.7%)\n",
            "   8. breakfast           :  1,285 (  4.6%)\n",
            "   9. seafood             :  1,131 (  4.1%)\n",
            "  10. bbq                 :  1,056 (  3.8%)\n",
            "  11. chinese             :  1,032 (  3.7%)\n",
            "  12. bakery              :    871 (  3.1%)\n",
            "  13. asian               :    829 (  3.0%)\n",
            "  14. sushi               :    822 (  3.0%)\n",
            "  15. cafe                :    741 (  2.7%)\n",
            "\n",
            "Saving updated business data to /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet...\n",
            "  Output size: 5.4 MB\n",
            "\n",
            "================================================================================\n",
            "STEP 2: FILTERING SEQUENCES (MIN 5 VISITS)\n",
            "================================================================================\n",
            "\n",
            "Original sequences: 10,339,035\n",
            "Original users: 2,546,362\n",
            "\n",
            "[1/2] Counting visits per user...\n",
            "  Users with 5+ visits: 471,601\n",
            "\n",
            "[2/2] Filtering sequences...\n",
            "\n",
            "Filtered sequences: 7,198,744\n",
            "Filtered users: 471,601\n",
            "Retention: 69.6% of sequences\n",
            "\n",
            "Saving filtered sequences to /Users/sunho/Forkast/data/processed/ga/user_sequences_filtered_ga.parquet...\n",
            "  Output size: 155.0 MB\n",
            "\n",
            "================================================================================\n",
            "STEP 3: FILTERING PAIRS (MIN 0.2 HOURS)\n",
            "================================================================================\n",
            "\n",
            "Original pairs: 4,152,155\n",
            "\n",
            "[1/2] Time delta distribution (before):\n",
            "  <= 0.2 hours: 3,039,034\n",
            "  0.2-1 hours: 91,204\n",
            "  1-6 hours: 69,331\n",
            "  6-24 hours: 151,274\n",
            "  1-7 days: 801,312\n",
            "\n",
            "[2/2] Filtering pairs with delta_hours > 0.2...\n",
            "\n",
            "Filtered pairs: 1,113,121\n",
            "Removed: 3,039,034 (73.2%)\n",
            "Retention: 26.8%\n",
            "\n",
            "================================================================================\n",
            "STEP 4: REGENERATING PAIRS FROM FILTERED SEQUENCES\n",
            "================================================================================\n",
            "\n",
            "[1/4] Creating shifted columns for next visit...\n",
            "\n",
            "[2/4] Filtering out null destinations...\n",
            "  Retained 6,727,143 pairs\n",
            "\n",
            "[3/4] Calculating time delta and filtering...\n",
            "  Retained 985,006 pairs within 0.2-168 hour window\n",
            "\n",
            "[4/4] Final pair count: 985,006\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/pairs_filtered_ga.parquet...\n",
            "  Output size: 49.7 MB\n",
            "\n",
            "================================================================================\n",
            "FINAL STATISTICS AFTER PHASE A2.5\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š BUSINESSES:\n",
            "  Total: 27,710\n",
            "  Categories: 26\n",
            "  'other' category: 2,607\n",
            "\n",
            "ğŸ‘¥ USER SEQUENCES:\n",
            "  Total visits: 7,198,744\n",
            "  Unique users: 471,601\n",
            "  Unique businesses: 27,654\n",
            "\n",
            "  Sequence length distribution:\n",
            "    Mean: 15.3\n",
            "    Median: 10\n",
            "    Min: 5\n",
            "    Max: 648\n",
            "\n",
            "ğŸ”— CONSECUTIVE PAIRS:\n",
            "  Total pairs: 985,006\n",
            "  Unique users: 274,432\n",
            "  Unique src businesses: 25,604\n",
            "  Unique dst businesses: 25,597\n",
            "\n",
            "  Time delta distribution:\n",
            "    Mean: 62.9 hours\n",
            "    Median: 49.6 hours\n",
            "    Min: 0.20 hours\n",
            "    Max: 168.0 hours\n",
            "\n",
            "  Top 10 category transitions:\n",
            "     1. burger          â†’ burger         : 28,508\n",
            "     2. american        â†’ american       : 23,116\n",
            "     3. american        â†’ burger         : 21,230\n",
            "     4. burger          â†’ american       : 20,960\n",
            "     5. fast_food       â†’ burger         : 18,158\n",
            "     6. burger          â†’ fast_food      : 16,978\n",
            "     7. mexican         â†’ burger         : 13,323\n",
            "     8. fast_food       â†’ american       : 13,103\n",
            "     9. burger          â†’ mexican        : 12,927\n",
            "    10. fast_food       â†’ fast_food      : 12,851\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âœ“âœ“âœ“ PHASE A2.5 COMPLETE âœ“âœ“âœ“\n",
            "\n",
            "\n",
            "âœ“ Quality filtering complete:\n",
            "  Filtered pairs: 985,006\n",
            "  Retention rate: 23.7%\n"
          ]
        }
      ],
      "source": [
        "from data.filter_quality import main as filter_main\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A2.5: DATA QUALITY FILTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run quality filtering\n",
        "filter_main()\n",
        "\n",
        "# Load filtered pairs\n",
        "filtered_pairs = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
        "\n",
        "print(f\"\\nâœ“ Quality filtering complete:\")\n",
        "print(f\"  Filtered pairs: {len(filtered_pairs):,}\")\n",
        "print(f\"  Retention rate: {len(filtered_pairs)/len(pairs_df)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A3: Feature Engineering (Enhanced!)\n",
        "\n",
        "**What this does:**\n",
        "\n",
        "**Original features (1-7):**\n",
        "1. Spatial: distance, neighborhood, direction\n",
        "2. Temporal: time gaps, day of week, meal type\n",
        "3. Quality: ratings, rating differences\n",
        "4. Price: price levels, price differences\n",
        "5. Category: cuisine type matching\n",
        "6. Relationship: relative_results ranking\n",
        "\n",
        "**NEW Enhanced features (8-14):**\n",
        "7. **Review Sentiment**: Average sentiment scores, sentiment transitions, review length\n",
        "8. **User Behavioral**: User preferences, explorer flags, loyalty scores\n",
        "9. **Cuisine Complementarity**: Complementary pairs (pizzaâ†’dessert), meal progressions\n",
        "10. **Operating Hours**: Open/closed checks, hours overlap, late night transitions\n",
        "11. **Topic Features**: Dessert/drinks mentions, topic transitions\n",
        "12. **Service Options**: Delivery/takeout flags, service matching\n",
        "\n",
        "**Expected time:** 15-30 minutes (includes sentiment analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A3: ENHANCED FEATURE ENGINEERING\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded data:\n",
            "  Pairs: 985,006\n",
            "  Businesses: 27,710\n",
            "  Reviews: 10,339,035\n",
            "\n",
            "[Step 1/3] Adding all features to positive pairs...\n",
            "  This includes sentiment analysis and user profiling...\n",
            "\n",
            "================================================================================\n",
            "ADDING FEATURES TO POSITIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Adding spatial features...\n",
            "  - Calculating haversine distances...\n",
            "  - Creating distance buckets...\n",
            "  - Calculating directions...\n",
            "  âœ“ Added spatial features\n",
            "\n",
            "[2/7] Adding temporal features...\n",
            "  - Creating time delta buckets...\n",
            "  - Extracting hour and day of week...\n",
            "  - Identifying meal times...\n",
            "  âœ“ Added temporal features\n",
            "\n",
            "[3/7] Adding quality features...\n",
            "  âœ“ Added quality features\n",
            "\n",
            "[4/7] Adding price features...\n",
            "  - Joining with business data for prices...\n",
            "  âœ“ Added price features\n",
            "\n",
            "[5/7] Adding category features...\n",
            "  âœ“ Added category features\n",
            "\n",
            "[6/7] Adding relationship features...\n",
            "  - Joining with business data for relative_results...\n",
            "  - Checking relative_results membership...\n",
            "  âœ“ Added relationship features\n",
            "\n",
            "[8/14] Adding review sentiment features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added sentiment features\n",
            "\n",
            "[9/14] Adding user behavioral features...\n",
            "\n",
            "============================================================\n",
            "USER PROFILING\n",
            "============================================================\n",
            "  [1/6] Calculating basic user statistics...\n",
            "  [2/6] Calculating visit frequency and time patterns...\n",
            "  [3/6] Analyzing cuisine diversity and preferences...\n",
            "  [4/6] Calculating loyalty and exploration patterns...\n",
            "  [5/6] Analyzing rating patterns and sentiment...\n",
            "  [6/6] Combining and deriving final features...\n",
            "\n",
            "âœ“ User profiling complete!\n",
            "  Processed 2,546,362 users\n",
            "  Features per user: 22\n",
            "\n",
            "  User behavior summary:\n",
            "    Explorers: 1,135,876 (44.6%)\n",
            "    Frequent visitors: 455,777 (17.9%)\n",
            "    Diverse eaters: 1,081,160 (42.5%)\n",
            "    High standards: 1,487,609 (58.4%)\n",
            "    Price sensitive: 530,379 (20.8%)\n",
            "  âœ“ Added user behavioral features\n",
            "\n",
            "[12/14] Adding review topic features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added review topic features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "[14/14] Feature engineering complete!\n",
            "  Total features: 122\n",
            "  Total positive pairs: 985,006\n",
            "\n",
            "âœ“ Features added to 985,006 positive pairs\n",
            "  Total features: 122\n"
          ]
        }
      ],
      "source": [
        "# Reload features module to get latest changes\n",
        "import importlib\n",
        "import data.features\n",
        "importlib.reload(data.features)\n",
        "from data.features import add_all_features, generate_negative_samples\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A3: ENHANCED FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
        "biz_df = pl.read_parquet(BIZ_PARQUET)\n",
        "reviews_df = pl.read_parquet(REVIEWS_PARQUET)\n",
        "\n",
        "print(f\"\\nLoaded data:\")\n",
        "print(f\"  Pairs: {len(pairs_df):,}\")\n",
        "print(f\"  Businesses: {len(biz_df):,}\")\n",
        "print(f\"  Reviews: {len(reviews_df):,}\")\n",
        "\n",
        "print(f\"\\n[Step 1/3] Adding all features to positive pairs...\")\n",
        "print(\"  This includes sentiment analysis and user profiling...\")\n",
        "pairs_with_features = add_all_features(pairs_df, biz_df, reviews_df)\n",
        "\n",
        "print(f\"\\nâœ“ Features added to {len(pairs_with_features):,} positive pairs\")\n",
        "print(f\"  Total features: {len(pairs_with_features.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New Enhanced Features Preview:\n",
            "shape: (5, 11)\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ src_senti â”† dst_senti â”† sentiment â”† user_is_e â”† â€¦ â”† hours_ove â”† is_late_n â”† topic_tra â”† service_ â”‚\n",
            "â”‚ ment_scor â”† ment_scor â”† _transiti â”† xplorer   â”†   â”† rlap      â”† ight_tran â”† nsition_d â”† match    â”‚\n",
            "â”‚ e         â”† e         â”† on        â”† ---       â”†   â”† ---       â”† sition    â”† essert    â”† ---      â”‚\n",
            "â”‚ ---       â”† ---       â”† ---       â”† bool      â”†   â”† bool      â”† ---       â”† ---       â”† bool     â”‚\n",
            "â”‚ f32       â”† f32       â”† f32       â”†           â”†   â”†           â”† bool      â”† bool      â”†          â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ 0.43012   â”† 0.220048  â”† -0.210072 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† true     â”‚\n",
            "â”‚ 0.423093  â”† 0.383186  â”† -0.039906 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
            "â”‚ 0.347771  â”† 0.339981  â”† -0.007791 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
            "â”‚ 0.199572  â”† 0.390999  â”† 0.191428  â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
            "â”‚ 0.507037  â”† 0.456341  â”† -0.050696 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† true     â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# Preview new features\n",
        "print(\"\\nNew Enhanced Features Preview:\")\n",
        "new_feature_cols = [\n",
        "    \"src_sentiment_score\", \"dst_sentiment_score\", \"sentiment_transition\",\n",
        "    \"user_is_explorer\", \"user_visit_frequency\", \"cuisine_pair_type\",\n",
        "    \"is_dessert_followup\", \"hours_overlap\", \"is_late_night_transition\",\n",
        "    \"topic_transition_dessert\", \"service_match\"\n",
        "]\n",
        "\n",
        "# Get columns that exist\n",
        "existing_cols = [col for col in new_feature_cols if col in pairs_with_features.columns]\n",
        "\n",
        "if existing_cols:\n",
        "    print(pairs_with_features.select(existing_cols).head(5))\n",
        "else:\n",
        "    print(\"  Checking available columns...\")\n",
        "    all_cols = pairs_with_features.columns\n",
        "    print(f\"  Total columns: {len(all_cols)}\")\n",
        "    print(f\"  Sample columns: {all_cols[:20]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 2/3] Generating negative samples with features...\n",
            "  Generating 4 negative samples per positive pair...\n",
            "\n",
            "================================================================================\n",
            "GENERATING NEGATIVE SAMPLES (4:1 ratio)\n",
            "================================================================================\n",
            "\n",
            "Negative sample distribution per positive:\n",
            "  - Geographic (10km): 2\n",
            "  - Relative results: 1\n",
            "  - Same category: 1\n",
            "  - Total: 4\n",
            "\n",
            "[1/4] Creating business lookup structures...\n",
            "  âœ“ Indexed 27,710 businesses\n",
            "\n",
            "[2/4] Generating 3,940,024 negative samples...\n",
            "  Progress: 10,000/985,006 (1.0%)\n",
            "  Progress: 20,000/985,006 (2.0%)\n",
            "  Progress: 30,000/985,006 (3.0%)\n",
            "  Progress: 40,000/985,006 (4.1%)\n",
            "  Progress: 50,000/985,006 (5.1%)\n",
            "  Progress: 60,000/985,006 (6.1%)\n",
            "  Progress: 70,000/985,006 (7.1%)\n",
            "  Progress: 80,000/985,006 (8.1%)\n",
            "  Progress: 90,000/985,006 (9.1%)\n",
            "  Progress: 100,000/985,006 (10.2%)\n",
            "  Progress: 110,000/985,006 (11.2%)\n",
            "  Progress: 120,000/985,006 (12.2%)\n",
            "  Progress: 130,000/985,006 (13.2%)\n",
            "  Progress: 140,000/985,006 (14.2%)\n",
            "  Progress: 150,000/985,006 (15.2%)\n",
            "  Progress: 160,000/985,006 (16.2%)\n",
            "  Progress: 170,000/985,006 (17.3%)\n",
            "  Progress: 180,000/985,006 (18.3%)\n",
            "  Progress: 190,000/985,006 (19.3%)\n",
            "  Progress: 200,000/985,006 (20.3%)\n",
            "  Progress: 210,000/985,006 (21.3%)\n",
            "  Progress: 220,000/985,006 (22.3%)\n",
            "  Progress: 230,000/985,006 (23.4%)\n",
            "  Progress: 240,000/985,006 (24.4%)\n",
            "  Progress: 250,000/985,006 (25.4%)\n",
            "  Progress: 260,000/985,006 (26.4%)\n",
            "  Progress: 270,000/985,006 (27.4%)\n",
            "  Progress: 280,000/985,006 (28.4%)\n",
            "  Progress: 290,000/985,006 (29.4%)\n",
            "  Progress: 300,000/985,006 (30.5%)\n",
            "  Progress: 310,000/985,006 (31.5%)\n",
            "  Progress: 320,000/985,006 (32.5%)\n",
            "  Progress: 330,000/985,006 (33.5%)\n",
            "  Progress: 340,000/985,006 (34.5%)\n",
            "  Progress: 350,000/985,006 (35.5%)\n",
            "  Progress: 360,000/985,006 (36.5%)\n",
            "  Progress: 370,000/985,006 (37.6%)\n",
            "  Progress: 380,000/985,006 (38.6%)\n",
            "  Progress: 390,000/985,006 (39.6%)\n",
            "  Progress: 400,000/985,006 (40.6%)\n",
            "  Progress: 410,000/985,006 (41.6%)\n",
            "  Progress: 420,000/985,006 (42.6%)\n",
            "  Progress: 430,000/985,006 (43.7%)\n",
            "  Progress: 440,000/985,006 (44.7%)\n",
            "  Progress: 450,000/985,006 (45.7%)\n",
            "  Progress: 460,000/985,006 (46.7%)\n",
            "  Progress: 470,000/985,006 (47.7%)\n",
            "  Progress: 480,000/985,006 (48.7%)\n",
            "  Progress: 490,000/985,006 (49.7%)\n",
            "  Progress: 500,000/985,006 (50.8%)\n",
            "  Progress: 510,000/985,006 (51.8%)\n",
            "  Progress: 520,000/985,006 (52.8%)\n",
            "  Progress: 530,000/985,006 (53.8%)\n",
            "  Progress: 540,000/985,006 (54.8%)\n",
            "  Progress: 550,000/985,006 (55.8%)\n",
            "  Progress: 560,000/985,006 (56.9%)\n",
            "  Progress: 570,000/985,006 (57.9%)\n",
            "  Progress: 580,000/985,006 (58.9%)\n",
            "  Progress: 590,000/985,006 (59.9%)\n",
            "  Progress: 600,000/985,006 (60.9%)\n",
            "  Progress: 610,000/985,006 (61.9%)\n",
            "  Progress: 620,000/985,006 (62.9%)\n",
            "  Progress: 630,000/985,006 (64.0%)\n",
            "  Progress: 640,000/985,006 (65.0%)\n",
            "  Progress: 650,000/985,006 (66.0%)\n",
            "  Progress: 660,000/985,006 (67.0%)\n",
            "  Progress: 670,000/985,006 (68.0%)\n",
            "  Progress: 680,000/985,006 (69.0%)\n",
            "  Progress: 690,000/985,006 (70.1%)\n",
            "  Progress: 700,000/985,006 (71.1%)\n",
            "  Progress: 710,000/985,006 (72.1%)\n",
            "  Progress: 720,000/985,006 (73.1%)\n",
            "  Progress: 730,000/985,006 (74.1%)\n",
            "  Progress: 740,000/985,006 (75.1%)\n",
            "  Progress: 750,000/985,006 (76.1%)\n",
            "  Progress: 760,000/985,006 (77.2%)\n",
            "  Progress: 770,000/985,006 (78.2%)\n",
            "  Progress: 780,000/985,006 (79.2%)\n",
            "  Progress: 790,000/985,006 (80.2%)\n",
            "  Progress: 800,000/985,006 (81.2%)\n",
            "  Progress: 810,000/985,006 (82.2%)\n",
            "  Progress: 820,000/985,006 (83.2%)\n",
            "  Progress: 830,000/985,006 (84.3%)\n",
            "  Progress: 840,000/985,006 (85.3%)\n",
            "  Progress: 850,000/985,006 (86.3%)\n",
            "  Progress: 860,000/985,006 (87.3%)\n",
            "  Progress: 870,000/985,006 (88.3%)\n",
            "  Progress: 880,000/985,006 (89.3%)\n",
            "  Progress: 890,000/985,006 (90.4%)\n",
            "  Progress: 900,000/985,006 (91.4%)\n",
            "  Progress: 910,000/985,006 (92.4%)\n",
            "  Progress: 920,000/985,006 (93.4%)\n",
            "  Progress: 930,000/985,006 (94.4%)\n",
            "  Progress: 940,000/985,006 (95.4%)\n",
            "  Progress: 950,000/985,006 (96.4%)\n",
            "  Progress: 960,000/985,006 (97.5%)\n",
            "  Progress: 970,000/985,006 (98.5%)\n",
            "  Progress: 980,000/985,006 (99.5%)\n",
            "  âœ“ Generated 3,083,302 negative samples\n",
            "\n",
            "[3/4] Converting to DataFrame...\n",
            "\n",
            "[4/4] Adding features to negative samples...\n",
            "\n",
            "================================================================================\n",
            "ADDING FEATURES TO POSITIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Adding spatial features...\n",
            "  - Calculating haversine distances...\n",
            "  - Creating distance buckets...\n",
            "  - Calculating directions...\n",
            "  âœ“ Added spatial features\n",
            "\n",
            "[2/7] Adding temporal features...\n",
            "  - Creating time delta buckets...\n",
            "  - Extracting hour and day of week...\n",
            "  - Identifying meal times...\n",
            "  âœ“ Added temporal features\n",
            "\n",
            "[3/7] Adding quality features...\n",
            "  âœ“ Added quality features\n",
            "\n",
            "[4/7] Adding price features...\n",
            "  - Joining with business data for prices...\n",
            "  âœ“ Added price features\n",
            "\n",
            "[5/7] Adding category features...\n",
            "  âœ“ Added category features\n",
            "\n",
            "[6/7] Adding relationship features...\n",
            "  - Joining with business data for relative_results...\n",
            "  - Checking relative_results membership...\n",
            "  âœ“ Added relationship features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "[14/14] Feature engineering complete!\n",
            "  Total features: 72\n",
            "  Total positive pairs: 3,083,302\n",
            "\n",
            "âœ“ Negative sampling complete!\n",
            "  Total negative pairs: 3,083,302\n",
            "  Negative:Positive ratio: 3.1:1\n",
            "\n",
            "âœ“ Generated 3,083,302 negative pairs\n",
            "  Adding enhanced features to negative samples...\n",
            "\n",
            "[8/14] Adding review sentiment features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added sentiment features\n",
            "\n",
            "[9/14] Adding user behavioral features...\n",
            "\n",
            "============================================================\n",
            "USER PROFILING\n",
            "============================================================\n",
            "  [1/6] Calculating basic user statistics...\n",
            "  [2/6] Calculating visit frequency and time patterns...\n",
            "  [3/6] Analyzing cuisine diversity and preferences...\n",
            "  [4/6] Calculating loyalty and exploration patterns...\n",
            "  [5/6] Analyzing rating patterns and sentiment...\n",
            "  [6/6] Combining and deriving final features...\n",
            "\n",
            "âœ“ User profiling complete!\n",
            "  Processed 2,546,362 users\n",
            "  Features per user: 22\n",
            "\n",
            "  User behavior summary:\n",
            "    Explorers: 1,135,876 (44.6%)\n",
            "    Frequent visitors: 455,777 (17.9%)\n",
            "    Diverse eaters: 1,081,160 (42.5%)\n",
            "    High standards: 1,487,609 (58.4%)\n",
            "    Price sensitive: 530,379 (20.8%)\n",
            "  âœ“ Added user behavioral features\n",
            "\n",
            "[12/14] Adding review topic features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added review topic features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "  âœ“ Features added to negative samples\n"
          ]
        }
      ],
      "source": [
        "# Generate negative samples and add features (full pipeline)\n",
        "print(\"\\n[Step 2/3] Generating negative samples with features...\")\n",
        "print(\"  Generating 4 negative samples per positive pair...\")\n",
        "\n",
        "# This will be handled by the main features.py pipeline\n",
        "# For now, we'll use the direct approach\n",
        "negative_pairs = generate_negative_samples(\n",
        "    pairs_with_features, \n",
        "    biz_df, \n",
        "    n_negatives=4,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Generated {len(negative_pairs):,} negative pairs\")\n",
        "\n",
        "# Add features to negatives (need to call feature functions)\n",
        "from data.features import (\n",
        "    add_review_sentiment_features, add_user_behavioral_features,\n",
        "    add_cuisine_complementarity, add_operating_hours_features,\n",
        "    add_review_topic_features, add_service_options_features\n",
        ")\n",
        "\n",
        "if len(negative_pairs) > 0:\n",
        "    print(\"  Adding enhanced features to negative samples...\")\n",
        "    negative_pairs = add_review_sentiment_features(negative_pairs, reviews_df)\n",
        "    negative_pairs = add_user_behavioral_features(negative_pairs, reviews_df, biz_df)\n",
        "    negative_pairs = add_review_topic_features(negative_pairs, reviews_df)\n",
        "    negative_pairs = add_cuisine_complementarity(negative_pairs)\n",
        "    negative_pairs = add_operating_hours_features(negative_pairs, biz_df)\n",
        "    negative_pairs = add_service_options_features(negative_pairs, biz_df)\n",
        "    \n",
        "    print(\"  âœ“ Features added to negative samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 3/3] Combining positive and negative samples...\n",
            "\n",
            "âœ“ Features saved:\n",
            "  Total pairs (pos + neg): 4,068,308\n",
            "  Positive: 985,006\n",
            "  Negative: 3,083,302\n",
            "  File: /Users/sunho/Forkast/data/processed/ga/features_ga.parquet\n",
            "  Size: 668.0 MB\n"
          ]
        }
      ],
      "source": [
        "# Combine and save features\n",
        "print(\"\\n[Step 3/3] Combining positive and negative samples...\")\n",
        "\n",
        "# Ensure schema compatibility\n",
        "target_schema = pairs_with_features.schema\n",
        "target_columns = set(target_schema.keys())\n",
        "\n",
        "# First, select only columns that exist in target schema (drop extras)\n",
        "columns_to_select = [col for col in negative_pairs.columns if col in target_columns]\n",
        "negative_pairs = negative_pairs.select(columns_to_select)\n",
        "\n",
        "# Now ensure all target columns exist and have correct types\n",
        "cast_exprs = []\n",
        "for col_name, dtype in target_schema.items():\n",
        "    if col_name in negative_pairs.columns:\n",
        "        if negative_pairs[col_name].dtype != dtype:\n",
        "            cast_exprs.append(pl.col(col_name).cast(dtype))\n",
        "        else:\n",
        "            cast_exprs.append(pl.col(col_name))\n",
        "    else:\n",
        "        # Fill missing columns with defaults\n",
        "        if dtype == pl.Boolean:\n",
        "            cast_exprs.append(pl.lit(False).cast(dtype).alias(col_name))\n",
        "        elif dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:\n",
        "            cast_exprs.append(pl.lit(0).cast(dtype).alias(col_name))\n",
        "        elif dtype in [pl.Float32, pl.Float64]:\n",
        "            cast_exprs.append(pl.lit(0.0).cast(dtype).alias(col_name))\n",
        "        else:\n",
        "            cast_exprs.append(pl.lit(None).cast(dtype).alias(col_name))\n",
        "\n",
        "if cast_exprs:\n",
        "    negative_pairs = negative_pairs.with_columns(cast_exprs)\n",
        "\n",
        "# Ensure column order matches target schema\n",
        "negative_pairs = negative_pairs.select(list(target_schema.keys()))\n",
        "\n",
        "# Combine\n",
        "all_features_df = pl.concat([pairs_with_features, negative_pairs])\n",
        "\n",
        "features_output = PROCESSED_DIR / \"features_ga.parquet\"\n",
        "all_features_df.write_parquet(features_output, compression=\"snappy\")\n",
        "\n",
        "print(f\"\\nâœ“ Features saved:\")\n",
        "print(f\"  Total pairs (pos + neg): {len(all_features_df):,}\")\n",
        "print(f\"  Positive: {len(pairs_with_features):,}\")\n",
        "print(f\"  Negative: {len(negative_pairs):,}\")\n",
        "print(f\"  File: {features_output}\")\n",
        "print(f\"  Size: {features_output.stat().st_size / 1024 / 1024:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A4: Temporal Data Splitting\n",
        "\n",
        "**What this does:**\n",
        "- Splits data chronologically by `src_ts` timestamp\n",
        "- 70% train, 15% validation, 15% test\n",
        "- Ensures no data leakage (future data not in training)\n",
        "\n",
        "**Expected time:** 2-5 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A4: TEMPORAL DATA SPLITTING (XGBOOST ONLY)\n",
            "================================================================================\n",
            "\n",
            "[Loading data...]\n",
            "  XGBoost: 4,068,308 samples\n",
            "\n",
            "[Splitting XGBoost data...]\n",
            "\n",
            "================================================================================\n",
            "SPLITTING XGBOOST DATA (TEMPORAL)\n",
            "================================================================================\n",
            "\n",
            "[1/4] Sorting by timestamp...\n",
            "\n",
            "[2/4] Calculating split indices...\n",
            "  Total samples: 4,068,308\n",
            "  Train size: 2,847,815 (70.0%)\n",
            "  Val size: 610,246 (15.0%)\n",
            "  Test size: 610,247 (15.0%)\n",
            "\n",
            "[3/4] Splitting data...\n",
            "\n",
            "[4/4] Temporal ranges:\n",
            "  Train: 2005-12-09 00:00:00 to 2019-11-10 14:14:18\n",
            "  Val:   2019-11-10 14:14:18 to 2020-08-22 14:46:26\n",
            "  Test:  2020-08-22 14:46:26 to 2021-09-04 12:21:12\n",
            "\n",
            "  Label distribution:\n",
            "  Train - Pos: 685,073, Neg: 2,162,742\n",
            "  Val   - Pos: 148,160, Neg: 462,086\n",
            "  Test  - Pos: 151,773, Neg: 458,474\n",
            "\n",
            "[Saving XGBoost splits...]\n",
            "  âœ“ Saved train.parquet (2,847,815 samples)\n",
            "  âœ“ Saved val.parquet (610,246 samples)\n",
            "  âœ“ Saved test.parquet (610,247 samples)\n",
            "\n",
            "[Copying business metadata...]\n",
            "  âœ“ Copied biz_ga.parquet to xgboost_data/\n",
            "\n",
            "âœ“ Data split complete:\n",
            "  Train: 2,847,815 pairs (70.0%)\n",
            "  Validation: 610,246 pairs (15.0%)\n",
            "  Test: 610,247 pairs (15.0%)\n",
            "\n",
            "  Positive/negative ratio:\n",
            "    Train: 685,073 positive, 2,162,742 negative (24.1% positive)\n",
            "    Val: 148,160 positive, 462,086 negative (24.3% positive)\n",
            "    Test: 151,773 positive, 458,474 negative (24.9% positive)\n"
          ]
        }
      ],
      "source": [
        "from data.split_data import split_xgboost_only\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A4: TEMPORAL DATA SPLITTING (XGBOOST ONLY)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run data splitting (XGBoost only - no LSTM data)\n",
        "split_xgboost_only()\n",
        "\n",
        "# Load split data\n",
        "train_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"train.parquet\")\n",
        "val_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"val.parquet\")\n",
        "test_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"test.parquet\")\n",
        "\n",
        "print(f\"\\nâœ“ Data split complete:\")\n",
        "print(f\"  Train: {len(train_df):,} pairs ({len(train_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs ({len(val_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_df):,} pairs ({len(test_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"\\n  Positive/negative ratio:\")\n",
        "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "    pos_count = df[\"label\"].sum() if \"label\" in df.columns else 0\n",
        "    neg_count = len(df) - pos_count\n",
        "    print(f\"    {name}: {pos_count:,} positive, {neg_count:,} negative ({pos_count/len(df)*100:.1f}% positive)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš¨ FIXES APPLIED - Addressing Data Leakage Issues\n",
        "\n",
        "**Problems Identified:**\n",
        "1. **Negative Sampling Data Leakage**: Negative samples used same timestamps as positives, making them easily distinguishable\n",
        "2. **No Geographic Separation**: All data from Georgia used for both training and testing\n",
        "3. **Perfect Recall = Red Flag**: Model achieved 1.0000 recall indicating overfitting/leakage\n",
        "\n",
        "**Fixes Applied:**\n",
        "1. âœ… **Fixed Negative Sampling**: Generate realistic timestamps (0.2-168 hours) for negative samples\n",
        "2. âœ… **Geographic Splitting**: Train on non-Atlanta restaurants, test on Atlanta restaurants  \n",
        "3. âœ… **Enhanced Evaluation**: Added diagnostic information to detect data leakage\n",
        "4. âœ… **Proper Validation**: Realistic evaluation scenario prevents overfitting\n",
        "\n",
        "**Next Steps:**\n",
        "- Regenerate features with fixed negative sampling\n",
        "- Use geographic splitting instead of temporal splitting\n",
        "- Retrain model and expect realistic (not perfect) performance metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "REGENERATING FEATURES WITH FIXED NEGATIVE SAMPLING\n",
            "================================================================================\n",
            "\n",
            "Loaded data:\n",
            "  Pairs: 985,006\n",
            "  Businesses: 27,710\n",
            "  Reviews: 10,339,035\n",
            "\n",
            "[1/3] Adding features to positive pairs...\n",
            "\n",
            "================================================================================\n",
            "ADDING FEATURES TO POSITIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Adding spatial features...\n",
            "  - Calculating haversine distances...\n",
            "  - Creating distance buckets...\n",
            "  - Calculating directions...\n",
            "  âœ“ Added spatial features\n",
            "\n",
            "[2/7] Adding temporal features...\n",
            "  - Creating time delta buckets...\n",
            "  - Extracting hour and day of week...\n",
            "  - Identifying meal times...\n",
            "  âœ“ Added temporal features\n",
            "\n",
            "[3/7] Adding quality features...\n",
            "  âœ“ Added quality features\n",
            "\n",
            "[4/7] Adding price features...\n",
            "  - Joining with business data for prices...\n",
            "  âœ“ Added price features\n",
            "\n",
            "[5/7] Adding category features...\n",
            "  âœ“ Added category features\n",
            "\n",
            "[6/7] Adding relationship features...\n",
            "  - Joining with business data for relative_results...\n",
            "  - Checking relative_results membership...\n",
            "  âœ“ Added relationship features\n",
            "\n",
            "[8/14] Adding review sentiment features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added sentiment features\n",
            "\n",
            "[9/14] Adding user behavioral features...\n",
            "\n",
            "============================================================\n",
            "USER PROFILING\n",
            "============================================================\n",
            "  [1/6] Calculating basic user statistics...\n",
            "  [2/6] Calculating visit frequency and time patterns...\n",
            "  [3/6] Analyzing cuisine diversity and preferences...\n",
            "  [4/6] Calculating loyalty and exploration patterns...\n",
            "  [5/6] Analyzing rating patterns and sentiment...\n",
            "  [6/6] Combining and deriving final features...\n",
            "\n",
            "âœ“ User profiling complete!\n",
            "  Processed 2,546,362 users\n",
            "  Features per user: 22\n",
            "\n",
            "  User behavior summary:\n",
            "    Explorers: 1,135,876 (44.6%)\n",
            "    Frequent visitors: 455,777 (17.9%)\n",
            "    Diverse eaters: 1,081,160 (42.5%)\n",
            "    High standards: 1,487,609 (58.4%)\n",
            "    Price sensitive: 530,379 (20.8%)\n",
            "  âœ“ Added user behavioral features\n",
            "\n",
            "[12/14] Adding review topic features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added review topic features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "[14/14] Feature engineering complete!\n",
            "  Total features: 122\n",
            "  Total positive pairs: 985,006\n",
            "\n",
            "[2/3] Generating negative samples with realistic timestamps...\n",
            "\n",
            "================================================================================\n",
            "GENERATING NEGATIVE SAMPLES (4:1 ratio)\n",
            "================================================================================\n",
            "\n",
            "Negative sample distribution per positive:\n",
            "  - Geographic (10km): 2\n",
            "  - Relative results: 1\n",
            "  - Same category: 1\n",
            "  - Total: 4\n",
            "\n",
            "[1/4] Creating business lookup structures...\n",
            "  âœ“ Indexed 27,710 businesses\n",
            "\n",
            "[2/4] Generating 3,940,024 negative samples...\n",
            "  Progress: 10,000/985,006 (1.0%)\n",
            "  Progress: 20,000/985,006 (2.0%)\n",
            "  Progress: 30,000/985,006 (3.0%)\n",
            "  Progress: 40,000/985,006 (4.1%)\n",
            "  Progress: 50,000/985,006 (5.1%)\n",
            "  Progress: 60,000/985,006 (6.1%)\n",
            "  Progress: 70,000/985,006 (7.1%)\n",
            "  Progress: 80,000/985,006 (8.1%)\n",
            "  Progress: 90,000/985,006 (9.1%)\n",
            "  Progress: 100,000/985,006 (10.2%)\n",
            "  Progress: 110,000/985,006 (11.2%)\n",
            "  Progress: 120,000/985,006 (12.2%)\n",
            "  Progress: 130,000/985,006 (13.2%)\n",
            "  Progress: 140,000/985,006 (14.2%)\n",
            "  Progress: 150,000/985,006 (15.2%)\n",
            "  Progress: 160,000/985,006 (16.2%)\n",
            "  Progress: 170,000/985,006 (17.3%)\n",
            "  Progress: 180,000/985,006 (18.3%)\n",
            "  Progress: 190,000/985,006 (19.3%)\n",
            "  Progress: 200,000/985,006 (20.3%)\n",
            "  Progress: 210,000/985,006 (21.3%)\n",
            "  Progress: 220,000/985,006 (22.3%)\n",
            "  Progress: 230,000/985,006 (23.4%)\n",
            "  Progress: 240,000/985,006 (24.4%)\n",
            "  Progress: 250,000/985,006 (25.4%)\n",
            "  Progress: 260,000/985,006 (26.4%)\n",
            "  Progress: 270,000/985,006 (27.4%)\n",
            "  Progress: 280,000/985,006 (28.4%)\n",
            "  Progress: 290,000/985,006 (29.4%)\n",
            "  Progress: 300,000/985,006 (30.5%)\n",
            "  Progress: 310,000/985,006 (31.5%)\n",
            "  Progress: 320,000/985,006 (32.5%)\n",
            "  Progress: 330,000/985,006 (33.5%)\n",
            "  Progress: 340,000/985,006 (34.5%)\n",
            "  Progress: 350,000/985,006 (35.5%)\n",
            "  Progress: 360,000/985,006 (36.5%)\n",
            "  Progress: 370,000/985,006 (37.6%)\n",
            "  Progress: 380,000/985,006 (38.6%)\n",
            "  Progress: 390,000/985,006 (39.6%)\n",
            "  Progress: 400,000/985,006 (40.6%)\n",
            "  Progress: 410,000/985,006 (41.6%)\n",
            "  Progress: 420,000/985,006 (42.6%)\n",
            "  Progress: 430,000/985,006 (43.7%)\n",
            "  Progress: 440,000/985,006 (44.7%)\n",
            "  Progress: 450,000/985,006 (45.7%)\n",
            "  Progress: 460,000/985,006 (46.7%)\n",
            "  Progress: 470,000/985,006 (47.7%)\n",
            "  Progress: 480,000/985,006 (48.7%)\n",
            "  Progress: 490,000/985,006 (49.7%)\n",
            "  Progress: 500,000/985,006 (50.8%)\n",
            "  Progress: 510,000/985,006 (51.8%)\n",
            "  Progress: 520,000/985,006 (52.8%)\n",
            "  Progress: 530,000/985,006 (53.8%)\n",
            "  Progress: 540,000/985,006 (54.8%)\n",
            "  Progress: 550,000/985,006 (55.8%)\n",
            "  Progress: 560,000/985,006 (56.9%)\n",
            "  Progress: 570,000/985,006 (57.9%)\n",
            "  Progress: 580,000/985,006 (58.9%)\n",
            "  Progress: 590,000/985,006 (59.9%)\n",
            "  Progress: 600,000/985,006 (60.9%)\n",
            "  Progress: 610,000/985,006 (61.9%)\n",
            "  Progress: 620,000/985,006 (62.9%)\n",
            "  Progress: 630,000/985,006 (64.0%)\n",
            "  Progress: 640,000/985,006 (65.0%)\n",
            "  Progress: 650,000/985,006 (66.0%)\n",
            "  Progress: 660,000/985,006 (67.0%)\n",
            "  Progress: 670,000/985,006 (68.0%)\n",
            "  Progress: 680,000/985,006 (69.0%)\n",
            "  Progress: 690,000/985,006 (70.1%)\n",
            "  Progress: 700,000/985,006 (71.1%)\n",
            "  Progress: 710,000/985,006 (72.1%)\n",
            "  Progress: 720,000/985,006 (73.1%)\n",
            "  Progress: 730,000/985,006 (74.1%)\n",
            "  Progress: 740,000/985,006 (75.1%)\n",
            "  Progress: 750,000/985,006 (76.1%)\n",
            "  Progress: 760,000/985,006 (77.2%)\n",
            "  Progress: 770,000/985,006 (78.2%)\n",
            "  Progress: 780,000/985,006 (79.2%)\n",
            "  Progress: 790,000/985,006 (80.2%)\n",
            "  Progress: 800,000/985,006 (81.2%)\n",
            "  Progress: 810,000/985,006 (82.2%)\n",
            "  Progress: 820,000/985,006 (83.2%)\n",
            "  Progress: 830,000/985,006 (84.3%)\n",
            "  Progress: 840,000/985,006 (85.3%)\n",
            "  Progress: 850,000/985,006 (86.3%)\n",
            "  Progress: 860,000/985,006 (87.3%)\n",
            "  Progress: 870,000/985,006 (88.3%)\n",
            "  Progress: 880,000/985,006 (89.3%)\n",
            "  Progress: 890,000/985,006 (90.4%)\n",
            "  Progress: 900,000/985,006 (91.4%)\n",
            "  Progress: 910,000/985,006 (92.4%)\n",
            "  Progress: 920,000/985,006 (93.4%)\n",
            "  Progress: 930,000/985,006 (94.4%)\n",
            "  Progress: 940,000/985,006 (95.4%)\n",
            "  Progress: 950,000/985,006 (96.4%)\n",
            "  Progress: 960,000/985,006 (97.5%)\n",
            "  Progress: 970,000/985,006 (98.5%)\n",
            "  Progress: 980,000/985,006 (99.5%)\n",
            "  âœ“ Generated 3,083,302 negative samples\n",
            "\n",
            "[3/4] Converting to DataFrame...\n",
            "\n",
            "[4/4] Adding features to negative samples...\n",
            "\n",
            "================================================================================\n",
            "ADDING FEATURES TO POSITIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Adding spatial features...\n",
            "  - Calculating haversine distances...\n",
            "  - Creating distance buckets...\n",
            "  - Calculating directions...\n",
            "  âœ“ Added spatial features\n",
            "\n",
            "[2/7] Adding temporal features...\n",
            "  - Creating time delta buckets...\n",
            "  - Extracting hour and day of week...\n",
            "  - Identifying meal times...\n",
            "  âœ“ Added temporal features\n",
            "\n",
            "[3/7] Adding quality features...\n",
            "  âœ“ Added quality features\n",
            "\n",
            "[4/7] Adding price features...\n",
            "  - Joining with business data for prices...\n",
            "  âœ“ Added price features\n",
            "\n",
            "[5/7] Adding category features...\n",
            "  âœ“ Added category features\n",
            "\n",
            "[6/7] Adding relationship features...\n",
            "  - Joining with business data for relative_results...\n",
            "  - Checking relative_results membership...\n",
            "  âœ“ Added relationship features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "[14/14] Feature engineering complete!\n",
            "  Total features: 72\n",
            "  Total positive pairs: 3,083,302\n",
            "\n",
            "âœ“ Negative sampling complete!\n",
            "  Total negative pairs: 3,083,302\n",
            "  Negative:Positive ratio: 3.1:1\n",
            "\n",
            "[3/3] Adding enhanced features to negative samples...\n",
            "\n",
            "[8/14] Adding review sentiment features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added sentiment features\n",
            "\n",
            "[9/14] Adding user behavioral features...\n",
            "\n",
            "============================================================\n",
            "USER PROFILING\n",
            "============================================================\n",
            "  [1/6] Calculating basic user statistics...\n",
            "  [2/6] Calculating visit frequency and time patterns...\n",
            "  [3/6] Analyzing cuisine diversity and preferences...\n",
            "  [4/6] Calculating loyalty and exploration patterns...\n",
            "  [5/6] Analyzing rating patterns and sentiment...\n",
            "  [6/6] Combining and deriving final features...\n",
            "\n",
            "âœ“ User profiling complete!\n",
            "  Processed 2,546,362 users\n",
            "  Features per user: 22\n",
            "\n",
            "  User behavior summary:\n",
            "    Explorers: 1,135,876 (44.6%)\n",
            "    Frequent visitors: 455,777 (17.9%)\n",
            "    Diverse eaters: 1,081,160 (42.5%)\n",
            "    High standards: 1,487,609 (58.4%)\n",
            "    Price sensitive: 530,379 (20.8%)\n",
            "  âœ“ Added user behavioral features\n",
            "\n",
            "[12/14] Adding review topic features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added review topic features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "Combining positive and negative samples...\n",
            "\n",
            "âœ… Fixed features saved:\n",
            "  Total pairs: 4,068,308\n",
            "  Positive: 985,006\n",
            "  Negative: 3,083,302\n",
            "  File: /Users/sunho/Forkast/data/processed/ga/features_ga_fixed.parquet\n",
            "  Size: 668.0 MB\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Regenerate features with fixed negative sampling\n",
        "print(\"=\" * 80)\n",
        "print(\"REGENERATING FEATURES WITH FIXED NEGATIVE SAMPLING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Reload the updated modules\n",
        "import importlib\n",
        "import data.features\n",
        "importlib.reload(data.features)\n",
        "from data.features import add_all_features, generate_negative_samples\n",
        "\n",
        "# Load the filtered pairs (before feature engineering)\n",
        "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
        "biz_df = pl.read_parquet(BIZ_PARQUET)\n",
        "reviews_df = pl.read_parquet(REVIEWS_PARQUET)\n",
        "\n",
        "print(f\"\\nLoaded data:\")\n",
        "print(f\"  Pairs: {len(pairs_df):,}\")\n",
        "print(f\"  Businesses: {len(biz_df):,}\")\n",
        "print(f\"  Reviews: {len(reviews_df):,}\")\n",
        "\n",
        "# Add features to positive pairs\n",
        "print(f\"\\n[1/3] Adding features to positive pairs...\")\n",
        "pairs_with_features = add_all_features(pairs_df, biz_df, reviews_df)\n",
        "\n",
        "# Generate negative samples with FIXED timestamps\n",
        "print(f\"\\n[2/3] Generating negative samples with realistic timestamps...\")\n",
        "negative_pairs = generate_negative_samples(pairs_with_features, biz_df, n_negatives=4, random_seed=42)\n",
        "\n",
        "# Add enhanced features to negative samples\n",
        "print(f\"\\n[3/3] Adding enhanced features to negative samples...\")\n",
        "from data.features import (\n",
        "    add_review_sentiment_features, add_user_behavioral_features,\n",
        "    add_cuisine_complementarity, add_operating_hours_features,\n",
        "    add_review_topic_features, add_service_options_features\n",
        ")\n",
        "\n",
        "if len(negative_pairs) > 0:\n",
        "    negative_pairs = add_review_sentiment_features(negative_pairs, reviews_df)\n",
        "    negative_pairs = add_user_behavioral_features(negative_pairs, reviews_df, biz_df)\n",
        "    negative_pairs = add_review_topic_features(negative_pairs, reviews_df)\n",
        "    negative_pairs = add_cuisine_complementarity(negative_pairs)\n",
        "    negative_pairs = add_operating_hours_features(negative_pairs, biz_df)\n",
        "    negative_pairs = add_service_options_features(negative_pairs, biz_df)\n",
        "\n",
        "# Combine and save\n",
        "print(f\"\\nCombining positive and negative samples...\")\n",
        "# Ensure schema compatibility\n",
        "target_schema = pairs_with_features.schema\n",
        "target_columns = set(target_schema.keys())\n",
        "\n",
        "# Select only columns that exist in target schema\n",
        "columns_to_select = [col for col in negative_pairs.columns if col in target_columns]\n",
        "negative_pairs = negative_pairs.select(columns_to_select)\n",
        "\n",
        "# Ensure all target columns exist and have correct types\n",
        "cast_exprs = []\n",
        "for col_name, dtype in target_schema.items():\n",
        "    if col_name in negative_pairs.columns:\n",
        "        if negative_pairs[col_name].dtype != dtype:\n",
        "            cast_exprs.append(pl.col(col_name).cast(dtype))\n",
        "        else:\n",
        "            cast_exprs.append(pl.col(col_name))\n",
        "    else:\n",
        "        # Fill missing columns with defaults\n",
        "        if dtype == pl.Boolean:\n",
        "            cast_exprs.append(pl.lit(False).cast(dtype).alias(col_name))\n",
        "        elif dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:\n",
        "            cast_exprs.append(pl.lit(0).cast(dtype).alias(col_name))\n",
        "        elif dtype in [pl.Float32, pl.Float64]:\n",
        "            cast_exprs.append(pl.lit(0.0).cast(dtype).alias(col_name))\n",
        "        else:\n",
        "            cast_exprs.append(pl.lit(None).cast(dtype).alias(col_name))\n",
        "\n",
        "if cast_exprs:\n",
        "    negative_pairs = negative_pairs.with_columns(cast_exprs)\n",
        "\n",
        "# Ensure column order matches target schema\n",
        "negative_pairs = negative_pairs.select(list(target_schema.keys()))\n",
        "\n",
        "# Combine\n",
        "all_features_df = pl.concat([pairs_with_features, negative_pairs])\n",
        "\n",
        "# Save updated features\n",
        "features_output = PROCESSED_DIR / \"features_ga_fixed.parquet\"\n",
        "all_features_df.write_parquet(features_output, compression=\"snappy\")\n",
        "\n",
        "print(f\"\\nâœ… Fixed features saved:\")\n",
        "print(f\"  Total pairs: {len(all_features_df):,}\")\n",
        "print(f\"  Positive: {len(pairs_with_features):,}\")\n",
        "print(f\"  Negative: {len(negative_pairs):,}\")\n",
        "print(f\"  File: {features_output}\")\n",
        "print(f\"  Size: {features_output.stat().st_size / 1024 / 1024:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GEOGRAPHIC DATA SPLITTING\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reload the updated split_data module\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit_data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(data\u001b[38;5;241m.\u001b[39msplit_data)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_xgboost_geographic\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
          ]
        }
      ],
      "source": [
        "# STEP 2: Geographic splitting (Non-Atlanta train â†’ Atlanta test)\n",
        "print(\"=\" * 80)\n",
        "print(\"GEOGRAPHIC DATA SPLITTING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Reload the updated split_data module\n",
        "import data.split_data\n",
        "importlib.reload(data.split_data)\n",
        "from data.split_data import split_xgboost_geographic\n",
        "\n",
        "# Use the fixed features file\n",
        "import shutil\n",
        "fixed_features = PROCESSED_DIR / \"features_ga_fixed.parquet\"\n",
        "original_features = PROCESSED_DIR / \"features_ga.parquet\"\n",
        "\n",
        "# Backup original and use fixed version\n",
        "if original_features.exists():\n",
        "    shutil.copy(original_features, PROCESSED_DIR / \"features_ga_backup.parquet\")\n",
        "    print(\"  âœ“ Backed up original features\")\n",
        "\n",
        "shutil.copy(fixed_features, original_features)\n",
        "print(\"  âœ“ Using fixed features for splitting\")\n",
        "\n",
        "# Run geographic splitting\n",
        "train_df, val_df, test_df = split_xgboost_geographic()\n",
        "\n",
        "print(f\"\\nâœ… Geographic split complete:\")\n",
        "print(f\"  Train (non-Atlanta): {len(train_df):,} pairs\")\n",
        "print(f\"  Val (non-Atlanta): {len(val_df):,} pairs\")\n",
        "print(f\"  Test (Atlanta): {len(test_df):,} pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RETRAINING XGBOOST MODEL WITH FIXES\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reload the updated XGBoost module\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost_ranker\u001b[39;00m\n\u001b[1;32m      8\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(models\u001b[38;5;241m.\u001b[39mxgboost_ranker)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxgboost_ranker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     load_data, prepare_features, train_model, predict_ranking, \n\u001b[1;32m     11\u001b[0m     evaluate_ranking, analyze_feature_importance\n\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
          ]
        }
      ],
      "source": [
        "# STEP 3: Retrain model with fixed data\n",
        "print(\"=\" * 80)\n",
        "print(\"RETRAINING XGBOOST MODEL WITH FIXES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Reload the updated XGBoost module\n",
        "import models.xgboost_ranker\n",
        "importlib.reload(models.xgboost_ranker)\n",
        "from models.xgboost_ranker import (\n",
        "    load_data, prepare_features, train_model, predict_ranking, \n",
        "    evaluate_ranking, analyze_feature_importance\n",
        ")\n",
        "\n",
        "# Load the geographically split data\n",
        "data_dir = PROCESSED_DIR / \"xgboost_data\"\n",
        "train_df, val_df, test_df = load_data(data_dir)\n",
        "\n",
        "print(f\"\\nâœ“ Data loaded:\")\n",
        "print(f\"  Train: {len(train_df):,} pairs\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs\")\n",
        "print(f\"  Test: {len(test_df):,} pairs\")\n",
        "\n",
        "# Prepare features\n",
        "print(f\"\\n[1/4] Preparing features...\")\n",
        "X_train, y_train, group_train, feature_names = prepare_features(train_df)\n",
        "X_val, y_val, group_val, _ = prepare_features(val_df)\n",
        "X_test, y_test, group_test, _ = prepare_features(test_df)\n",
        "\n",
        "print(f\"  Train shape: {X_train.shape}\")\n",
        "print(f\"  Validation shape: {X_val.shape}\")\n",
        "print(f\"  Test shape: {X_test.shape}\")\n",
        "\n",
        "# Train model\n",
        "print(f\"\\n[2/4] Training model...\")\n",
        "model = train_model(X_train, y_train, group_train, X_val, y_val, group_val)\n",
        "\n",
        "# Generate predictions\n",
        "print(f\"\\n[3/4] Generating predictions...\")\n",
        "predictions_df = predict_ranking(model, test_df, top_k=10)\n",
        "\n",
        "# Evaluate model (with enhanced diagnostics)\n",
        "print(f\"\\n[4/4] Evaluating model...\")\n",
        "metrics = evaluate_ranking(predictions_df, k_values=[1, 5, 10])\n",
        "\n",
        "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
        "print(f\"  Recall@1: {metrics.get('recall@1', 0):.4f}\")\n",
        "print(f\"  Recall@5: {metrics.get('recall@5', 0):.4f}\")\n",
        "print(f\"  Recall@10: {metrics.get('recall@10', 0):.4f}\")\n",
        "print(f\"  MRR: {metrics.get('mrr', 0):.4f}\")\n",
        "print(f\"  nDCG@10: {metrics.get('ndcg@10', 0):.4f}\")\n",
        "\n",
        "# Expected results: Much lower than 1.0 (realistic performance)\n",
        "if metrics.get('recall@10', 0) < 0.9:\n",
        "    print(f\"\\nâœ… SUCCESS: Realistic performance metrics indicate fixes worked!\")\n",
        "    print(f\"   No more data leakage - model performance is now realistic.\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  WARNING: Still getting very high performance - may need further investigation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GEOGRAPHIC DATA SPLITTING\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reload the updated split_data module\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit_data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(data\u001b[38;5;241m.\u001b[39msplit_data)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_xgboost_geographic\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
          ]
        }
      ],
      "source": [
        "# STEP 2: Geographic splitting (Non-Atlanta train â†’ Atlanta test)\n",
        "print(\"=\" * 80)\n",
        "print(\"GEOGRAPHIC DATA SPLITTING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Reload the updated split_data module\n",
        "import data.split_data\n",
        "importlib.reload(data.split_data)\n",
        "from data.split_data import split_xgboost_geographic\n",
        "\n",
        "# Use the fixed features file\n",
        "import shutil\n",
        "fixed_features = PROCESSED_DIR / \"features_ga_fixed.parquet\"\n",
        "original_features = PROCESSED_DIR / \"features_ga.parquet\"\n",
        "\n",
        "# Backup original and use fixed version\n",
        "if original_features.exists():\n",
        "    shutil.copy(original_features, PROCESSED_DIR / \"features_ga_backup.parquet\")\n",
        "    print(\"  âœ“ Backed up original features\")\n",
        "\n",
        "shutil.copy(fixed_features, original_features)\n",
        "print(\"  âœ“ Using fixed features for splitting\")\n",
        "\n",
        "# Run geographic splitting\n",
        "train_df, val_df, test_df = split_xgboost_geographic()\n",
        "\n",
        "print(f\"\\nâœ… Geographic split complete:\")\n",
        "print(f\"  Train (non-Atlanta): {len(train_df):,} pairs\")\n",
        "print(f\"  Val (non-Atlanta): {len(val_df):,} pairs\")\n",
        "print(f\"  Test (Atlanta): {len(test_df):,} pairs\")\n",
        "\n",
        "# Verify the split worked correctly\n",
        "atlanta_bounds = {\n",
        "    'lat_min': 33.6, 'lat_max': 34.0,\n",
        "    'lon_min': -84.6, 'lon_max': -84.2\n",
        "}\n",
        "\n",
        "# Check test set is Atlanta-only\n",
        "test_biz_ids = set(test_df['src_gmap_id'].to_list() + test_df['dst_gmap_id'].to_list())\n",
        "atlanta_test_biz = biz_df.filter(\n",
        "    pl.col('gmap_id').is_in(list(test_biz_ids)) &\n",
        "    (pl.col('lat') >= atlanta_bounds['lat_min']) &\n",
        "    (pl.col('lat') <= atlanta_bounds['lat_max']) &\n",
        "    (pl.col('lon') >= atlanta_bounds['lon_min']) &\n",
        "    (pl.col('lon') <= atlanta_bounds['lon_max'])\n",
        ")\n",
        "\n",
        "print(f\"\\nValidation:\")\n",
        "print(f\"  Test businesses in Atlanta bounds: {len(atlanta_test_biz):,}\")\n",
        "print(f\"  Total test businesses: {len(test_biz_ids):,}\")\n",
        "print(f\"  Atlanta percentage in test: {len(atlanta_test_biz)/len(test_biz_ids)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase B1: XGBoost Ranking Model Training\n",
        "\n",
        "**What this does:**\n",
        "- Loads split data and prepares features\n",
        "- Trains XGBoost ranking model with `rank:pairwise` objective\n",
        "- Evaluates using Recall@K, MRR, nDCG@K\n",
        "- Generates feature importance analysis\n",
        "- Creates transition probability matrix\n",
        "\n",
        "**Expected time:** 20-40 minutes (depending on data size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE B1: XGBOOST RANKING MODEL TRAINING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "[1/3] Loading training data...\n",
            "  âœ“ 2,847,708 samples\n",
            "  Positive: 684,924\n",
            "  Negative: 2,162,784\n",
            "\n",
            "[2/3] Loading validation data...\n",
            "  âœ“ 610,223 samples\n",
            "  Positive: 148,321\n",
            "  Negative: 461,902\n",
            "\n",
            "[3/3] Loading test data...\n",
            "  âœ“ 610,224 samples\n",
            "  Positive: 151,761\n",
            "  Negative: 458,463\n",
            "\n",
            "âœ“ Data loaded:\n",
            "  Train: 2,847,708 pairs\n",
            "  Validation: 610,223 pairs\n",
            "  Test: 610,224 pairs\n"
          ]
        }
      ],
      "source": [
        "# Reload the module to get the latest changes\n",
        "import importlib\n",
        "import models.xgboost_ranker\n",
        "importlib.reload(models.xgboost_ranker)\n",
        "from models.xgboost_ranker import (\n",
        "    load_data, prepare_features, train_model, predict_ranking, \n",
        "    evaluate_ranking, analyze_feature_importance, save_model_and_results,\n",
        "    CATEGORICAL_FEATURES, NUMERICAL_FEATURES, BOOLEAN_FEATURES\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE B1: XGBOOST RANKING MODEL TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "data_dir = PROCESSED_DIR / \"xgboost_data\"\n",
        "train_df, val_df, test_df = load_data(data_dir)\n",
        "\n",
        "print(f\"\\nâœ“ Data loaded:\")\n",
        "print(f\"  Train: {len(train_df):,} pairs\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs\")\n",
        "print(f\"  Test: {len(test_df):,} pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 1/5] Preparing features...\n",
            "\n",
            "[Preparing ranking data...]\n",
            "  - One-hot encoding categorical features...\n",
            "  - Total features after encoding: 129\n",
            "  Features: (2847708, 129)\n",
            "  Labels: (2847708,)\n",
            "  Number of queries (source visits): 684,924\n",
            "  Avg candidates per query: 4.2\n",
            "  Total samples: 2,847,708\n",
            "\n",
            "[Preparing ranking data...]\n",
            "  - One-hot encoding categorical features...\n",
            "  - Total features after encoding: 129\n",
            "  Features: (610223, 129)\n",
            "  Labels: (610223,)\n",
            "  Number of queries (source visits): 148,322\n",
            "  Avg candidates per query: 4.1\n",
            "  Total samples: 610,223\n",
            "\n",
            "[Preparing ranking data...]\n",
            "  - One-hot encoding categorical features...\n",
            "  - Total features after encoding: 129\n",
            "  Features: (610224, 129)\n",
            "  Labels: (610224,)\n",
            "  Number of queries (source visits): 151,761\n",
            "  Avg candidates per query: 4.0\n",
            "  Total samples: 610,224\n",
            "\n",
            "âœ“ Features prepared:\n",
            "  Total features: 129\n",
            "    - Categorical: 6\n",
            "    - Numerical: 48\n",
            "    - Boolean: 52\n",
            "  Train shape: (2847708, 129)\n",
            "  Validation shape: (610223, 129)\n",
            "  Test shape: (610224, 129)\n"
          ]
        }
      ],
      "source": [
        "# Prepare features\n",
        "print(\"\\n[Step 1/5] Preparing features...\")\n",
        "\n",
        "X_train, y_train, group_train, feature_names = prepare_features(train_df)\n",
        "X_val, y_val, group_val, _ = prepare_features(val_df)\n",
        "X_test, y_test, group_test, _ = prepare_features(test_df)\n",
        "\n",
        "print(f\"\\nâœ“ Features prepared:\")\n",
        "print(f\"  Total features: {len(feature_names)}\")\n",
        "print(f\"    - Categorical: {len(CATEGORICAL_FEATURES)}\")\n",
        "print(f\"    - Numerical: {len(NUMERICAL_FEATURES)}\")\n",
        "print(f\"    - Boolean: {len(BOOLEAN_FEATURES)}\")\n",
        "print(f\"  Train shape: {X_train.shape}\")\n",
        "print(f\"  Validation shape: {X_val.shape}\")\n",
        "print(f\"  Test shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample feature names (showing enhanced features):\n",
            "\n",
            "  New Sentiment Features:\n",
            "    - src_sentiment_score\n",
            "    - dst_sentiment_score\n",
            "    - sentiment_transition\n",
            "    - sentiment_similarity\n",
            "    - is_sentiment_upgrade\n",
            "\n",
            "  New User Behavioral Features:\n",
            "    - user_avg_rating\n",
            "    - user_rating_std\n",
            "    - user_total_reviews\n",
            "    - user_unique_restaurants\n",
            "    - user_visit_frequency\n",
            "\n",
            "  New Operating Hours Features:\n",
            "    - delta_hours\n",
            "    - src_is_open_at_time\n",
            "    - dst_is_open_at_time\n",
            "    - hours_overlap\n",
            "    - delta_hours_bucket_0-3h\n"
          ]
        }
      ],
      "source": [
        "# Show sample of feature names\n",
        "print(\"\\nSample feature names (showing enhanced features):\")\n",
        "print(\"\\n  New Sentiment Features:\")\n",
        "sentiment_features = [f for f in feature_names if 'sentiment' in f.lower()]\n",
        "for f in sentiment_features[:5]:\n",
        "    print(f\"    - {f}\")\n",
        "\n",
        "print(\"\\n  New User Behavioral Features:\")\n",
        "user_features = [f for f in feature_names if f.startswith('user_')]\n",
        "for f in user_features[:5]:\n",
        "    print(f\"    - {f}\")\n",
        "\n",
        "print(\"\\n  New Operating Hours Features:\")\n",
        "hours_features = [f for f in feature_names if 'hours' in f.lower() or 'open' in f.lower()]\n",
        "for f in hours_features[:5]:\n",
        "    print(f\"    - {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 2/5] Training XGBoost ranking model...\n",
            "  This may take 20-40 minutes depending on data size...\n",
            "\n",
            "================================================================================\n",
            "TRAINING XGBOOST RANKER\n",
            "================================================================================\n",
            "  ğŸš€ GPU detected! Using CUDA for training\n",
            "\n",
            "Model parameters:\n",
            "  objective: rank:pairwise\n",
            "  eval_metric: ['ndcg@10', 'map@10']\n",
            "  eta: 0.1\n",
            "  max_depth: 6\n",
            "  min_child_weight: 1\n",
            "  subsample: 0.8\n",
            "  colsample_bytree: 0.8\n",
            "  lambda: 1.0\n",
            "  alpha: 0.1\n",
            "  seed: 42\n",
            "  tree_method: hist\n",
            "  device: cuda\n",
            "\n",
            "[1/3] Creating DMatrix objects...\n",
            "  âœ“ Training DMatrix: (2847708, 129)\n",
            "  âœ“ Validation DMatrix: (610223, 129)\n",
            "\n",
            "[2/3] Training XGBoost model...\n",
            "  (This may take several minutes...)\n",
            "[0]\ttrain-ndcg@10:1.00000\ttrain-map@10:1.00000\tval-ndcg@10:1.00000\tval-map@10:1.00000\n",
            "[19]\ttrain-ndcg@10:1.00000\ttrain-map@10:1.00000\tval-ndcg@10:1.00000\tval-map@10:1.00000\n",
            "\n",
            "[3/3] Training complete!\n",
            "  Best iteration: 0\n",
            "  Best NDCG@10: 1.0000\n",
            "  Best MAP@10: 1.0000\n",
            "\n",
            "âœ“ Model training complete!\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "print(\"\\n[Step 2/5] Training XGBoost ranking model...\")\n",
        "print(\"  This may take 20-40 minutes depending on data size...\")\n",
        "\n",
        "model = train_model(\n",
        "    X_train, y_train, group_train,\n",
        "    X_val, y_val, group_val\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ Model training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 3/5] Generating predictions...\n",
            "\n",
            "================================================================================\n",
            "GENERATING TOP-10 PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "[1/4] One-hot encoding categorical features...\n",
            "\n",
            "[2/4] Predicting scores...\n",
            "\n",
            "[3/4] Selecting top-10 per source visit...\n",
            "  âœ“ Generated 610,224 predictions\n",
            "  âœ“ For 151,761 queries\n",
            "\n",
            "[4/4] Adding rank column...\n",
            "\n",
            "âœ“ Predictions generated:\n",
            "  Total predictions: 610,224\n",
            "  Unique source restaurants: 19,143\n",
            "  Average predictions per source: 31.9\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions on test set\n",
        "print(\"\\n[Step 3/5] Generating predictions...\")\n",
        "predictions_df = predict_ranking(model, test_df, top_k=10)\n",
        "\n",
        "print(f\"\\nâœ“ Predictions generated:\")\n",
        "print(f\"  Total predictions: {len(predictions_df):,}\")\n",
        "print(f\"  Unique source restaurants: {predictions_df['src_gmap_id'].n_unique():,}\")\n",
        "print(f\"  Average predictions per source: {len(predictions_df)/predictions_df['src_gmap_id'].n_unique():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 4/5] Evaluating model performance...\n",
            "\n",
            "================================================================================\n",
            "EVALUATION METRICS\n",
            "================================================================================\n",
            "\n",
            "Evaluating 151,761 queries...\n",
            "\n",
            "[1/3] Calculating Recall@K...\n",
            "  Recall@1: 1.0000 (151,760/151,761 queries)\n",
            "  Recall@5: 1.0000 (151,760/151,761 queries)\n",
            "  Recall@10: 1.0000 (151,760/151,761 queries)\n",
            "\n",
            "[2/3] Calculating MRR...\n",
            "  MRR: 1.0000\n",
            "\n",
            "[3/3] Calculating nDCG@K...\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model\n",
        "print(\"\\n[Step 4/5] Evaluating model performance...\")\n",
        "metrics = evaluate_ranking(predictions_df, k_values=[1, 5, 10])\n",
        "\n",
        "print(\"\\nâœ“ Evaluation complete:\")\n",
        "print(\"\\n  Performance Metrics:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n  Interpretation:\")\n",
        "print(f\"    - Recall@10: {metrics.get('recall@10', 0):.1%} of actual next visits are in top-10 predictions\")\n",
        "print(f\"    - MRR: {metrics.get('mrr', 0):.2f} average rank of first correct prediction\")\n",
        "print(f\"    - nDCG@10: {metrics.get('ndcg@10', 0):.4f} ranking quality (higher is better)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature importance\n",
        "print(\"\\n[Step 5/5] Analyzing feature importance...\")\n",
        "feature_importance = analyze_feature_importance(model, feature_names)\n",
        "\n",
        "print(\"\\nâœ“ Feature importance analysis complete!\")\n",
        "print(\"\\n  Top feature categories:\")\n",
        "sorted_groups = sorted(feature_importance['group_totals'].items(), \n",
        "                      key=lambda x: x[1], reverse=True)\n",
        "for group, total_score in sorted_groups[:5]:\n",
        "    print(f\"    {group:20s}: {total_score:8.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "**What we've accomplished:**\n",
        "\n",
        "1. âœ… **Data Ingestion**: Loaded and normalized raw JSON data with enhanced metadata extraction\n",
        "2. âœ… **Sequence Creation**: Generated consecutive visit pairs (Aâ†’B)\n",
        "3. âœ… **Quality Filtering**: Filtered to high-quality user sequences\n",
        "4. âœ… **Enhanced Feature Engineering**: Created 75+ features including:\n",
        "   - Review sentiment analysis\n",
        "   - User behavioral profiles\n",
        "   - Cuisine complementarity\n",
        "   - Operating hours analysis\n",
        "   - Topic extraction\n",
        "   - Service options\n",
        "5. âœ… **Temporal Splitting**: Split data chronologically for proper validation\n",
        "6. âœ… **Model Training**: Trained XGBoost ranking model\n",
        "7. âœ… **Evaluation**: Calculated Recall@K, MRR, nDCG@K metrics\n",
        "8. âœ… **Feature Importance**: Analyzed which features matter most\n",
        "9. âœ… **Transition Matrix**: Generated Aâ†’B probability matrix for visualization\n",
        "\n",
        "**Output Files:**\n",
        "- `data/processed/ga/models/xgboost_ranker.json` - Trained model\n",
        "- `data/processed/ga/models/predictions/xgboost_predictions.parquet` - Test predictions\n",
        "- `data/processed/ga/models/metrics/xgboost_metrics.json` - Evaluation metrics\n",
        "- `data/processed/ga/models/metrics/feature_importance.json` - Feature importance\n",
        "- `data/processed/ga/transition_matrix/` - Transition matrices for visualization\n",
        "\n",
        "**Next Steps:**\n",
        "1. Review feature importance to understand what drives transitions\n",
        "2. Explore transition matrix to see top Aâ†’B predictions\n",
        "3. Use trained model for inference on new restaurant pairs\n",
        "4. Integrate with visualization dashboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase B2: Transition Matrix Generation (Optional)\n",
        "\n",
        "**What this does:**\n",
        "- Generates Aâ†’B transition probability matrix for visualization\n",
        "- Creates top-K predictions for each source restaurant\n",
        "- Exports in multiple formats (Parquet, JSON, sparse matrix)\n",
        "\n",
        "**Expected time:** 10-30 minutes (depending on number of restaurants)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 80)\n",
        "print(\"PIPELINE COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nâœ“ All phases completed successfully!\")\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  - Recall@10: {metrics.get('recall@10', 0):.4f}\")\n",
        "print(f\"  - MRR: {metrics.get('mrr', 0):.4f}\")\n",
        "print(f\"  - nDCG@10: {metrics.get('ndcg@10', 0):.4f}\")\n",
        "\n",
        "print(f\"\\nOutput Files:\")\n",
        "print(f\"  - Model: {output_dir / 'models' / 'xgboost_ranker.json'}\")\n",
        "print(f\"  - Metrics: {output_dir / 'models' / 'metrics' / 'xgboost_metrics.json'}\")\n",
        "print(f\"  - Feature Importance: {output_dir / 'models' / 'metrics' / 'feature_importance.json'}\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Ready for visualization and inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
