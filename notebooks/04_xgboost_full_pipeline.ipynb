{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Full Pipeline: Training with Enhanced Features\n",
        "\n",
        "This notebook walks through the complete XGBoost training pipeline with all the enhanced features:\n",
        "- Review sentiment analysis\n",
        "- User behavioral profiling\n",
        "- Cuisine complementarity\n",
        "- Operating hours analysis\n",
        "- Topic extraction\n",
        "- Service options\n",
        "\n",
        "**Inputs (from SSD):**\n",
        "- `/Volumes/SunnySSD/review-Georgia.json` (7.2GB)\n",
        "- `/Volumes/SunnySSD/meta-Georgia.json` (168MB)\n",
        "\n",
        "**Outputs:**\n",
        "- Processed Parquet files in `data/processed/ga/`\n",
        "- Trained XGBoost model\n",
        "- Feature importance analysis\n",
        "- Transition probability matrix\n",
        "- Evaluation metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Paths and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Setup complete\n",
            "  Base directory: /Users/sunho/Forkast\n",
            "  SSD path: /Volumes/SunnySSD\n",
            "  Processed output: /Users/sunho/Forkast/data/processed/ga\n",
            "  Review JSON exists: True\n",
            "  Meta JSON exists: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "base_dir = Path.cwd().parent\n",
        "sys.path.append(str(base_dir / 'src'))\n",
        "\n",
        "# Define paths\n",
        "SSD_PATH = Path(\"/Volumes/SunnySSD\")\n",
        "PROCESSED_DIR = base_dir / \"data\" / \"processed\" / \"ga\"\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# SSD data files\n",
        "REVIEW_JSON = SSD_PATH / \"review-Georgia.json\"\n",
        "META_JSON = SSD_PATH / \"meta-Georgia.json\"\n",
        "\n",
        "# Output paths\n",
        "REVIEWS_PARQUET = PROCESSED_DIR / \"reviews_ga.parquet\"\n",
        "BIZ_PARQUET = PROCESSED_DIR / \"biz_ga.parquet\"\n",
        "\n",
        "print(\"âœ“ Setup complete\")\n",
        "print(f\"  Base directory: {base_dir}\")\n",
        "print(f\"  SSD path: {SSD_PATH}\")\n",
        "print(f\"  Processed output: {PROCESSED_DIR}\")\n",
        "print(f\"  Review JSON exists: {REVIEW_JSON.exists()}\")\n",
        "print(f\"  Meta JSON exists: {META_JSON.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A1: Data Ingestion & Normalization\n",
        "\n",
        "**What this does:**\n",
        "- Loads raw JSON files from SSD\n",
        "- Filters to Georgia geographic bounds\n",
        "- Normalizes categories and prices\n",
        "- **Extracts operating hours and service options** (NEW!)\n",
        "- Converts to efficient Parquet format\n",
        "\n",
        "**Expected time:** 10-20 minutes for 7GB review file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A1: DATA INGESTION\n",
            "================================================================================\n",
            "\n",
            "[1/2] Ingesting metadata...\n",
            "\n",
            "================================================================================\n",
            "PHASE A1: INGESTING METADATA\n",
            "================================================================================\n",
            "Input: /Volumes/SunnySSD/meta-Georgia.json\n",
            "Output: /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet\n",
            "\n",
            "[1/6] Reading JSON file...\n",
            "  Loaded 166,381 raw businesses\n",
            "\n",
            "[2/6] Filtering to Georgia geographic bounds...\n",
            "  Retained 166,334 businesses in Georgia\n",
            "\n",
            "[3/6] Parsing price buckets...\n",
            "\n",
            "[4/6] Detecting closed businesses...\n",
            "\n",
            "[5/9] Normalizing categories...\n",
            "\n",
            "[6/9] Filtering to food-only businesses...\n",
            "  Retained 27,757 food-related businesses\n",
            "\n",
            "[7/9] Parsing operating hours...\n",
            "\n",
            "[8/9] Extracting service options...\n",
            "\n",
            "[9/9] Finalizing schema...\n",
            "  Final count: 27,710 unique businesses\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet...\n",
            "\n",
            "âœ“ Metadata ingestion complete!\n",
            "  Output size: 5.4 MB\n",
            "\n",
            "âœ“ Metadata ingested:\n",
            "  Total businesses: 27,710\n",
            "  Columns: ['gmap_id', 'name', 'lat', 'lon', 'category_main', 'category_all', 'avg_rating', 'num_reviews', 'price_bucket', 'is_closed', 'relative_results', 'operating_hours_parsed', 'days_open_count', 'avg_hours_per_day', 'has_late_night', 'is_24hr', 'is_weekend_only', 'has_delivery', 'has_takeout', 'has_dinein', 'accepts_reservations', 'has_quick_visit', 'requires_mask']\n",
            "  New metadata columns: operating_hours_parsed, has_delivery, has_takeout, etc.\n"
          ]
        }
      ],
      "source": [
        "# Reload the module to get the latest changes\n",
        "import importlib\n",
        "import data.ingest\n",
        "importlib.reload(data.ingest)\n",
        "from data.ingest import ingest_metadata, ingest_reviews\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A1: DATA INGESTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ingest metadata\n",
        "print(\"\\n[1/2] Ingesting metadata...\")\n",
        "biz_df, valid_ids = ingest_metadata(\n",
        "    str(META_JSON),\n",
        "    str(BIZ_PARQUET)\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Metadata ingested:\")\n",
        "print(f\"  Total businesses: {len(biz_df):,}\")\n",
        "print(f\"  Columns: {list(biz_df.columns)}\")\n",
        "print(f\"  New metadata columns: operating_hours_parsed, has_delivery, has_takeout, etc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample metadata with new columns:\n",
            "shape: (5, 8)\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ gmap_id    â”† name       â”† category_m â”† has_delive â”† has_takeou â”† has_late_ â”† is_24hr â”† days_open â”‚\n",
            "â”‚ ---        â”† ---        â”† ain        â”† ry         â”† t          â”† night     â”† ---     â”† _count    â”‚\n",
            "â”‚ str        â”† str        â”† ---        â”† ---        â”† ---        â”† ---       â”† bool    â”† ---       â”‚\n",
            "â”‚            â”†            â”† str        â”† bool       â”† bool       â”† bool      â”†         â”† i8        â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ 0x888ccd99 â”† Chester's  â”† bbq        â”† true       â”† true       â”† false     â”† false   â”† 6         â”‚\n",
            "â”‚ 3cb5ba2b:0 â”† Barbeque   â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xede8da3ad â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88f5b288 â”† Chick-fil- â”† breakfast  â”† true       â”† true       â”† false     â”† false   â”† 6         â”‚\n",
            "â”‚ 3a0e56ef:0 â”† A          â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ x2ee4774c5 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88ee67b0 â”† Pizza Hut  â”† pizza      â”† true       â”† true       â”† true      â”† false   â”† 7         â”‚\n",
            "â”‚ 19357227:0 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ x3cd58347c â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88f53940 â”† Marble     â”† bakery     â”† true       â”† true       â”† false     â”† false   â”† 7         â”‚\n",
            "â”‚ 1cbdad83:0 â”† Slab       â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xb7c5c3156 â”† Creamery & â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”† Great Aâ€¦   â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ 0x88f5bd30 â”† Maple      â”† breakfast  â”† true       â”† true       â”† false     â”† false   â”† 7         â”‚\n",
            "â”‚ 83712be3:0 â”† Street     â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ xb68abeefc â”† Biscuit    â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â”‚ â€¦          â”† Company -â€¦ â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# Preview metadata with new columns\n",
        "print(\"\\nSample metadata with new columns:\")\n",
        "print(biz_df.select([\n",
        "    \"gmap_id\", \"name\", \"category_main\", \"has_delivery\", \n",
        "    \"has_takeout\", \"has_late_night\", \"is_24hr\", \"days_open_count\"\n",
        "]).head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[2/2] Ingesting reviews...\n",
            "  This may take 10-20 minutes for the 7GB file...\n",
            "================================================================================\n",
            "PHASE A1: INGESTING REVIEWS\n",
            "================================================================================\n",
            "Input: /Volumes/SunnySSD/review-Georgia.json\n",
            "Output: /Users/sunho/Forkast/data/processed/ga/reviews_ga.parquet\n",
            "\n",
            "[1/5] Reading JSON file...\n",
            "  Loaded 24,060,125 raw reviews\n",
            "\n",
            "[2/5] Converting timestamps...\n",
            "\n",
            "[3/5] Filtering invalid timestamps...\n",
            "  Retained 24,060,120 reviews with valid timestamps\n",
            "\n",
            "[4/5] Creating derived columns...\n",
            "\n",
            "  Filtering out reviews with missing user_id or rating...\n",
            "  Removed 167,538 reviews with null user_id or rating\n",
            "  Retained 23,892,582 reviews\n",
            "\n",
            "  Filtering to 27,710 valid businesses...\n",
            "  Retained 10,494,609 reviews\n",
            "\n",
            "[5/5] Deduplicating...\n",
            "  Final count: 10,339,035 unique reviews\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/reviews_ga.parquet...\n",
            "\n",
            "âœ“ Reviews ingestion complete!\n",
            "  Output size: 787.1 MB\n",
            "\n",
            "âœ“ Reviews ingested:\n",
            "  Total reviews: 10,339,035\n",
            "  Unique users: 2,546,362\n",
            "  Unique restaurants: 27,710\n",
            "  Date range: 2001-01-06 00:00:00 to 2021-09-08 01:43:37\n"
          ]
        }
      ],
      "source": [
        "# Ingest reviews\n",
        "print(\"\\n[2/2] Ingesting reviews...\")\n",
        "print(\"  This may take 10-20 minutes for the 7GB file...\")\n",
        "\n",
        "reviews_df = ingest_reviews(\n",
        "    str(REVIEW_JSON),\n",
        "    str(REVIEWS_PARQUET),\n",
        "    biz_gmap_ids=valid_ids\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Reviews ingested:\")\n",
        "print(f\"  Total reviews: {len(reviews_df):,}\")\n",
        "print(f\"  Unique users: {reviews_df['user_id'].n_unique():,}\")\n",
        "print(f\"  Unique restaurants: {reviews_df['gmap_id'].n_unique():,}\")\n",
        "print(f\"  Date range: {reviews_df['ts'].min()} to {reviews_df['ts'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A2: User Sequence Derivation\n",
        "\n",
        "**What this does:**\n",
        "- Creates user visit sequences sorted by timestamp\n",
        "- Generates consecutive visit pairs (A â†’ B)\n",
        "- Filters pairs within 0-168 hours (1 week window)\n",
        "\n",
        "**Expected time:** 5-10 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A2: USER SEQUENCE DERIVATION\n",
            "================================================================================\n",
            "Loading data...\n",
            "  Loaded 10,339,035 reviews\n",
            "  Loaded 27,710 businesses\n",
            "================================================================================\n",
            "DERIVING USER SEQUENCES\n",
            "================================================================================\n",
            "\n",
            "[1/4] Joining reviews with business metadata...\n",
            "  Joined 10,339,035 reviews with business data\n",
            "\n",
            "[2/4] Sorting by user and timestamp...\n",
            "\n",
            "[3/4] Creating sequence indices...\n",
            "\n",
            "[4/4] Final sequence count: 10,339,035\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/user_sequences_ga.parquet...\n",
            "  Output size: 256.8 MB\n",
            "\n",
            "================================================================================\n",
            "DERIVING CONSECUTIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/5] Creating shifted columns for next visit...\n",
            "\n",
            "[2/5] Filtering out null destinations (last visit in sequence)...\n",
            "  Retained 7,792,673 pairs\n",
            "\n",
            "[3/5] Calculating time delta...\n",
            "\n",
            "[4/5] Filtering by time window...\n",
            "  Retained 4,152,155 pairs within 0-168 hour window\n",
            "\n",
            "[5/5] Final pair count: 4,152,155\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/pairs_ga.parquet...\n",
            "  Output size: 193.6 MB\n",
            "\n",
            "================================================================================\n",
            "SEQUENCE STATISTICS\n",
            "================================================================================\n",
            "\n",
            "USER SEQUENCES:\n",
            "  Total visits: 10,339,035\n",
            "  Unique users: 2,546,362\n",
            "  Unique businesses: 27,710\n",
            "\n",
            "SEQUENCE LENGTH DISTRIBUTION:\n",
            "  Mean: 4.1\n",
            "  Median: 1\n",
            "  Max: 648\n",
            "  Users with 2+ visits: 1,135,876 (44.6%)\n",
            "\n",
            "CONSECUTIVE PAIRS:\n",
            "  Total pairs: 4,152,155\n",
            "  Unique users: 744,680\n",
            "  Unique src businesses: 26,893\n",
            "  Unique dst businesses: 26,890\n",
            "\n",
            "TIME DELTA DISTRIBUTION:\n",
            "  Mean: 16.5 hours\n",
            "  Median: 0.0 hours\n",
            "  Min: 0.00 hours\n",
            "  Max: 168.0 hours\n",
            "\n",
            "TOP CATEGORY TRANSITIONS:\n",
            "  burger          â†’ burger         : 159,035\n",
            "  burger          â†’ american       : 102,696\n",
            "  american        â†’ american       : 102,406\n",
            "  american        â†’ burger         : 101,942\n",
            "  burger          â†’ fast_food      : 94,724\n",
            "  fast_food       â†’ burger         : 76,096\n",
            "  fast_food       â†’ fast_food      : 72,129\n",
            "  burger          â†’ mexican        : 63,417\n",
            "  mexican         â†’ burger         : 63,033\n",
            "  american        â†’ fast_food      : 59,162\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âœ“âœ“âœ“ PHASE A2 COMPLETE âœ“âœ“âœ“\n",
            "\n",
            "\n",
            "âœ“ Sequences created:\n",
            "  Total pairs: 4,152,155\n",
            "  Unique source restaurants: 26,893\n",
            "  Unique destination restaurants: 26,890\n",
            "  Average time gap: 16.5 hours\n"
          ]
        }
      ],
      "source": [
        "from data.sequences import main as sequences_main\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A2: USER SEQUENCE DERIVATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run sequence derivation\n",
        "sequences_main()\n",
        "\n",
        "# Load and preview results\n",
        "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_ga.parquet\")\n",
        "\n",
        "print(f\"\\nâœ“ Sequences created:\")\n",
        "print(f\"  Total pairs: {len(pairs_df):,}\")\n",
        "print(f\"  Unique source restaurants: {pairs_df['src_gmap_id'].n_unique():,}\")\n",
        "print(f\"  Unique destination restaurants: {pairs_df['dst_gmap_id'].n_unique():,}\")\n",
        "print(f\"  Average time gap: {pairs_df['delta_hours'].mean():.1f} hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample consecutive visit pairs:\n",
            "shape: (10, 6)\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ user_id        â”† src_gmap_id    â”† dst_gmap_id    â”† delta_hours â”† src_ts         â”† dst_ts         â”‚\n",
            "â”‚ ---            â”† ---            â”† ---            â”† ---         â”† ---            â”† ---            â”‚\n",
            "â”‚ str            â”† str            â”† str            â”† f64         â”† datetime[Î¼s]   â”† datetime[Î¼s]   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ 10000003984331 â”† 0x88f5be87ef51 â”† 0x88f5bc57a0f7 â”† 142.004167  â”† 2018-01-19     â”† 2018-01-25     â”‚\n",
            "â”‚ 3841630        â”† 1c85:0x92f9ad1 â”† 8431:0xde92824 â”†             â”† 17:52:58       â”† 15:53:13       â”‚\n",
            "â”‚                â”† a4â€¦            â”† 81â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003984331 â”† 0x88f5953d7e77 â”† 0x88f5bc1fce4e â”† 79.808611   â”† 2018-01-25     â”† 2018-01-28     â”‚\n",
            "â”‚ 3841630        â”† 4547:0xa3adf61 â”† 622b:0x42171e6 â”†             â”† 15:54:23       â”† 23:42:54       â”‚\n",
            "â”‚                â”† 09â€¦            â”† b0â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003984331 â”† 0x88f5bca0aada â”† 0x88f5bb58adb1 â”† 31.296389   â”† 2018-02-07     â”† 2018-02-09     â”‚\n",
            "â”‚ 3841630        â”† 81dd:0xee68d09 â”† d857:0x41df634 â”†             â”† 22:32:35       â”† 05:50:22       â”‚\n",
            "â”‚                â”† 89â€¦            â”† fdâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000003984331 â”† 0x88f5a4f6a6e6 â”† 0x88f5b97c0859 â”† 55.618889   â”† 2018-08-23     â”† 2018-08-25     â”‚\n",
            "â”‚ 3841630        â”† 1dd9:0x7c3fcb8 â”† b6f3:0xffa3fc8 â”†             â”† 12:02:26       â”† 19:39:34       â”‚\n",
            "â”‚                â”† c7â€¦            â”† 9aâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000010601165 â”† 0x88f5945c138c â”† 0x88f5963b9be7 â”† 11.176111   â”† 2020-11-07     â”† 2020-11-08     â”‚\n",
            "â”‚ 1682309        â”† c411:0x69acb52 â”† b4ad:0x7008fd1 â”†             â”† 15:27:40       â”† 02:38:14       â”‚\n",
            "â”‚                â”† 02â€¦            â”† aaâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000010601165 â”† 0x88f5a1a6012d â”† 0x88f5a30774e1 â”† 39.748611   â”† 2020-12-18     â”† 2020-12-19     â”‚\n",
            "â”‚ 1682309        â”† 1f77:0xd7bd44d â”† df1d:0x845c7e2 â”†             â”† 01:56:31       â”† 17:41:26       â”‚\n",
            "â”‚                â”† 43â€¦            â”† b8â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000010601165 â”† 0x88f599670679 â”† 0x88f5a6bf0f35 â”† 24.53       â”† 2020-12-19     â”† 2020-12-20     â”‚\n",
            "â”‚ 1682309        â”† 82a9:0x346dda4 â”† 4d07:0x9984fb9 â”†             â”† 17:47:17       â”† 18:19:05       â”‚\n",
            "â”‚                â”† ffâ€¦            â”† 30â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000010601165 â”† 0x88f5a6bf0f35 â”† 0x88f5a0e98120 â”† 7.370278    â”† 2020-12-20     â”† 2020-12-21     â”‚\n",
            "â”‚ 1682309        â”† 4d07:0x9984fb9 â”† 0c8b:0x5c3624b â”†             â”† 18:19:05       â”† 01:41:18       â”‚\n",
            "â”‚                â”† 30â€¦            â”† 25â€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000010601165 â”† 0x88f5a0e98120 â”† 0x88f5a30a2f97 â”† 15.606944   â”† 2020-12-21     â”† 2020-12-21     â”‚\n",
            "â”‚ 1682309        â”† 0c8b:0x5c3624b â”† 8091:0x7d70a13 â”†             â”† 01:41:18       â”† 17:17:43       â”‚\n",
            "â”‚                â”† 25â€¦            â”† 0aâ€¦            â”†             â”†                â”†                â”‚\n",
            "â”‚ 10000010601165 â”† 0x88f5a30a2f97 â”† 0x88f5a30a2f97 â”† 0.5175      â”† 2020-12-21     â”† 2020-12-21     â”‚\n",
            "â”‚ 1682309        â”† 8091:0x7d70a13 â”† 8091:0xbd46956 â”†             â”† 17:17:43       â”† 17:48:46       â”‚\n",
            "â”‚                â”† 0aâ€¦            â”† 0eâ€¦            â”†             â”†                â”†                â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# Preview sample pairs\n",
        "print(\"\\nSample consecutive visit pairs:\")\n",
        "print(pairs_df.select([\n",
        "    \"user_id\", \"src_gmap_id\", \"dst_gmap_id\", \n",
        "    \"delta_hours\", \"src_ts\", \"dst_ts\"\n",
        "]).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A2.5: Data Quality Filtering\n",
        "\n",
        "**What this does:**\n",
        "- Re-categorizes generic 'restaurant' businesses\n",
        "- Filters to users with 5+ visits\n",
        "- Removes pairs with very short time deltas\n",
        "\n",
        "**Expected time:** 2-5 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A2.5: DATA QUALITY FILTERING\n",
            "================================================================================\n",
            "================================================================================\n",
            "PHASE A2.5: DATA QUALITY FILTERING\n",
            "================================================================================\n",
            "\n",
            "Inputs:\n",
            "  - /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet\n",
            "  - /Users/sunho/Forkast/data/processed/ga/user_sequences_ga.parquet\n",
            "  - /Users/sunho/Forkast/data/processed/ga/pairs_ga.parquet\n",
            "\n",
            "Outputs:\n",
            "  - /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet (updated)\n",
            "  - /Users/sunho/Forkast/data/processed/ga/user_sequences_filtered_ga.parquet\n",
            "  - /Users/sunho/Forkast/data/processed/ga/pairs_filtered_ga.parquet\n",
            "\n",
            "[Loading data...]\n",
            "\n",
            "================================================================================\n",
            "STEP 1: RE-CATEGORIZING 'RESTAURANT' BUSINESSES\n",
            "================================================================================\n",
            "\n",
            "Original 'restaurant' businesses: 4,490\n",
            "\n",
            "[1/3] Inferring categories from business names...\n",
            "\n",
            "[2/3] Re-categorization results:\n",
            "  Successfully re-categorized: 1,883\n",
            "  Moved to 'other': 2,607\n",
            "\n",
            "[3/3] New category distribution (top 15):\n",
            "   1. fast_food           :  3,396 ( 12.3%)\n",
            "   2. american            :  2,759 ( 10.0%)\n",
            "   3. other               :  2,607 (  9.4%)\n",
            "   4. mexican             :  2,301 (  8.3%)\n",
            "   5. pizza               :  2,200 (  7.9%)\n",
            "   6. burger              :  2,136 (  7.7%)\n",
            "   7. bar                 :  1,587 (  5.7%)\n",
            "   8. breakfast           :  1,285 (  4.6%)\n",
            "   9. seafood             :  1,131 (  4.1%)\n",
            "  10. bbq                 :  1,056 (  3.8%)\n",
            "  11. chinese             :  1,032 (  3.7%)\n",
            "  12. bakery              :    871 (  3.1%)\n",
            "  13. asian               :    829 (  3.0%)\n",
            "  14. sushi               :    822 (  3.0%)\n",
            "  15. cafe                :    741 (  2.7%)\n",
            "\n",
            "Saving updated business data to /Users/sunho/Forkast/data/processed/ga/biz_ga.parquet...\n",
            "  Output size: 5.4 MB\n",
            "\n",
            "================================================================================\n",
            "STEP 2: FILTERING SEQUENCES (MIN 5 VISITS)\n",
            "================================================================================\n",
            "\n",
            "Original sequences: 10,339,035\n",
            "Original users: 2,546,362\n",
            "\n",
            "[1/2] Counting visits per user...\n",
            "  Users with 5+ visits: 471,601\n",
            "\n",
            "[2/2] Filtering sequences...\n",
            "\n",
            "Filtered sequences: 7,198,744\n",
            "Filtered users: 471,601\n",
            "Retention: 69.6% of sequences\n",
            "\n",
            "Saving filtered sequences to /Users/sunho/Forkast/data/processed/ga/user_sequences_filtered_ga.parquet...\n",
            "  Output size: 155.0 MB\n",
            "\n",
            "================================================================================\n",
            "STEP 3: FILTERING PAIRS (MIN 0.2 HOURS)\n",
            "================================================================================\n",
            "\n",
            "Original pairs: 4,152,155\n",
            "\n",
            "[1/2] Time delta distribution (before):\n",
            "  <= 0.2 hours: 3,039,034\n",
            "  0.2-1 hours: 91,204\n",
            "  1-6 hours: 69,331\n",
            "  6-24 hours: 151,274\n",
            "  1-7 days: 801,312\n",
            "\n",
            "[2/2] Filtering pairs with delta_hours > 0.2...\n",
            "\n",
            "Filtered pairs: 1,113,121\n",
            "Removed: 3,039,034 (73.2%)\n",
            "Retention: 26.8%\n",
            "\n",
            "================================================================================\n",
            "STEP 4: REGENERATING PAIRS FROM FILTERED SEQUENCES\n",
            "================================================================================\n",
            "\n",
            "[1/4] Creating shifted columns for next visit...\n",
            "\n",
            "[2/4] Filtering out null destinations...\n",
            "  Retained 6,727,143 pairs\n",
            "\n",
            "[3/4] Calculating time delta and filtering...\n",
            "  Retained 985,006 pairs within 0.2-168 hour window\n",
            "\n",
            "[4/4] Final pair count: 985,006\n",
            "\n",
            "Writing to /Users/sunho/Forkast/data/processed/ga/pairs_filtered_ga.parquet...\n",
            "  Output size: 49.7 MB\n",
            "\n",
            "================================================================================\n",
            "FINAL STATISTICS AFTER PHASE A2.5\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š BUSINESSES:\n",
            "  Total: 27,710\n",
            "  Categories: 26\n",
            "  'other' category: 2,607\n",
            "\n",
            "ğŸ‘¥ USER SEQUENCES:\n",
            "  Total visits: 7,198,744\n",
            "  Unique users: 471,601\n",
            "  Unique businesses: 27,654\n",
            "\n",
            "  Sequence length distribution:\n",
            "    Mean: 15.3\n",
            "    Median: 10\n",
            "    Min: 5\n",
            "    Max: 648\n",
            "\n",
            "ğŸ”— CONSECUTIVE PAIRS:\n",
            "  Total pairs: 985,006\n",
            "  Unique users: 274,432\n",
            "  Unique src businesses: 25,604\n",
            "  Unique dst businesses: 25,596\n",
            "\n",
            "  Time delta distribution:\n",
            "    Mean: 62.9 hours\n",
            "    Median: 49.6 hours\n",
            "    Min: 0.20 hours\n",
            "    Max: 168.0 hours\n",
            "\n",
            "  Top 10 category transitions:\n",
            "     1. burger          â†’ burger         : 28,509\n",
            "     2. american        â†’ american       : 23,114\n",
            "     3. american        â†’ burger         : 21,231\n",
            "     4. burger          â†’ american       : 20,960\n",
            "     5. fast_food       â†’ burger         : 18,155\n",
            "     6. burger          â†’ fast_food      : 16,979\n",
            "     7. mexican         â†’ burger         : 13,323\n",
            "     8. fast_food       â†’ american       : 13,104\n",
            "     9. burger          â†’ mexican        : 12,927\n",
            "    10. fast_food       â†’ fast_food      : 12,850\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âœ“âœ“âœ“ PHASE A2.5 COMPLETE âœ“âœ“âœ“\n",
            "\n",
            "\n",
            "âœ“ Quality filtering complete:\n",
            "  Filtered pairs: 985,006\n",
            "  Retention rate: 100.0%\n"
          ]
        }
      ],
      "source": [
        "from data.filter_quality import main as filter_main\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A2.5: DATA QUALITY FILTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run quality filtering\n",
        "filter_main()\n",
        "\n",
        "# Load filtered pairs\n",
        "filtered_pairs = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
        "\n",
        "print(f\"\\nâœ“ Quality filtering complete:\")\n",
        "print(f\"  Filtered pairs: {len(filtered_pairs):,}\")\n",
        "print(f\"  Retention rate: {len(filtered_pairs)/len(pairs_df)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A3: Feature Engineering (Enhanced!)\n",
        "\n",
        "**What this does:**\n",
        "\n",
        "**Original features (1-7):**\n",
        "1. Spatial: distance, neighborhood, direction\n",
        "2. Temporal: time gaps, day of week, meal type\n",
        "3. Quality: ratings, rating differences\n",
        "4. Price: price levels, price differences\n",
        "5. Category: cuisine type matching\n",
        "6. Relationship: relative_results ranking\n",
        "\n",
        "**NEW Enhanced features (8-14):**\n",
        "7. **Review Sentiment**: Average sentiment scores, sentiment transitions, review length\n",
        "8. **User Behavioral**: User preferences, explorer flags, loyalty scores\n",
        "9. **Cuisine Complementarity**: Complementary pairs (pizzaâ†’dessert), meal progressions\n",
        "10. **Operating Hours**: Open/closed checks, hours overlap, late night transitions\n",
        "11. **Topic Features**: Dessert/drinks mentions, topic transitions\n",
        "12. **Service Options**: Delivery/takeout flags, service matching\n",
        "\n",
        "**Expected time:** 15-30 minutes (includes sentiment analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A3: ENHANCED FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "Loaded data:\n",
            "  Pairs: 985,006\n",
            "  Businesses: 27,710\n",
            "  Reviews: 10,339,035\n",
            "\n",
            "[Step 1/3] Adding all features to positive pairs...\n",
            "  This includes sentiment analysis and user profiling...\n",
            "\n",
            "================================================================================\n",
            "ADDING FEATURES TO POSITIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Adding spatial features...\n",
            "  - Calculating haversine distances...\n",
            "  - Creating distance buckets...\n",
            "  - Calculating directions...\n",
            "  âœ“ Added spatial features\n",
            "\n",
            "[2/7] Adding temporal features...\n",
            "  - Creating time delta buckets...\n",
            "  - Extracting hour and day of week...\n",
            "  - Identifying meal times...\n",
            "  âœ“ Added temporal features\n",
            "\n",
            "[3/7] Adding quality features...\n",
            "  âœ“ Added quality features\n",
            "\n",
            "[4/7] Adding price features...\n",
            "  - Joining with business data for prices...\n",
            "  âœ“ Added price features\n",
            "\n",
            "[5/7] Adding category features...\n",
            "  âœ“ Added category features\n",
            "\n",
            "[6/7] Adding relationship features...\n",
            "  - Joining with business data for relative_results...\n",
            "  - Checking relative_results membership...\n",
            "  âœ“ Added relationship features\n",
            "\n",
            "[8/14] Adding review sentiment features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added sentiment features\n",
            "\n",
            "[9/14] Adding user behavioral features...\n",
            "\n",
            "============================================================\n",
            "USER PROFILING\n",
            "============================================================\n",
            "  [1/6] Calculating basic user statistics...\n",
            "  [2/6] Calculating visit frequency and time patterns...\n",
            "  [3/6] Analyzing cuisine diversity and preferences...\n",
            "  [4/6] Calculating loyalty and exploration patterns...\n",
            "  [5/6] Analyzing rating patterns and sentiment...\n",
            "  [6/6] Combining and deriving final features...\n",
            "\n",
            "âœ“ User profiling complete!\n",
            "  Processed 2,546,362 users\n",
            "  Features per user: 22\n",
            "\n",
            "  User behavior summary:\n",
            "    Explorers: 1,135,876 (44.6%)\n",
            "    Frequent visitors: 455,777 (17.9%)\n",
            "    Diverse eaters: 1,081,160 (42.5%)\n",
            "    High standards: 1,487,609 (58.4%)\n",
            "    Price sensitive: 530,362 (20.8%)\n",
            "  âœ“ Added user behavioral features\n",
            "\n",
            "[12/14] Adding review topic features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added review topic features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "[14/14] Feature engineering complete!\n",
            "  Total features: 122\n",
            "  Total positive pairs: 985,006\n",
            "\n",
            "âœ“ Features added to 985,006 positive pairs\n",
            "  Total features: 122\n"
          ]
        }
      ],
      "source": [
        "# Reload features module to get latest changes\n",
        "import importlib\n",
        "import data.features\n",
        "importlib.reload(data.features)\n",
        "from data.features import add_all_features, generate_negative_samples\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A3: ENHANCED FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
        "biz_df = pl.read_parquet(BIZ_PARQUET)\n",
        "reviews_df = pl.read_parquet(REVIEWS_PARQUET)\n",
        "\n",
        "print(f\"\\nLoaded data:\")\n",
        "print(f\"  Pairs: {len(pairs_df):,}\")\n",
        "print(f\"  Businesses: {len(biz_df):,}\")\n",
        "print(f\"  Reviews: {len(reviews_df):,}\")\n",
        "\n",
        "print(f\"\\n[Step 1/3] Adding all features to positive pairs...\")\n",
        "print(\"  This includes sentiment analysis and user profiling...\")\n",
        "pairs_with_features = add_all_features(pairs_df, biz_df, reviews_df)\n",
        "\n",
        "print(f\"\\nâœ“ Features added to {len(pairs_with_features):,} positive pairs\")\n",
        "print(f\"  Total features: {len(pairs_with_features.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New Enhanced Features Preview:\n",
            "shape: (5, 11)\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚ src_senti â”† dst_senti â”† sentiment â”† user_is_e â”† â€¦ â”† hours_ove â”† is_late_n â”† topic_tra â”† service_ â”‚\n",
            "â”‚ ment_scor â”† ment_scor â”† _transiti â”† xplorer   â”†   â”† rlap      â”† ight_tran â”† nsition_d â”† match    â”‚\n",
            "â”‚ e         â”† e         â”† on        â”† ---       â”†   â”† ---       â”† sition    â”† essert    â”† ---      â”‚\n",
            "â”‚ ---       â”† ---       â”† ---       â”† bool      â”†   â”† bool      â”† ---       â”† ---       â”† bool     â”‚\n",
            "â”‚ f32       â”† f32       â”† f32       â”†           â”†   â”†           â”† bool      â”† bool      â”†          â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ 0.43012   â”† 0.220048  â”† -0.210072 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† true     â”‚\n",
            "â”‚ 0.423093  â”† 0.383186  â”† -0.039906 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
            "â”‚ 0.347771  â”† 0.339981  â”† -0.007791 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
            "â”‚ 0.199572  â”† 0.390999  â”† 0.191428  â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
            "â”‚ 0.507037  â”† 0.456341  â”† -0.050696 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† true     â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# Preview new features\n",
        "print(\"\\nNew Enhanced Features Preview:\")\n",
        "new_feature_cols = [\n",
        "    \"src_sentiment_score\", \"dst_sentiment_score\", \"sentiment_transition\",\n",
        "    \"user_is_explorer\", \"user_visit_frequency\", \"cuisine_pair_type\",\n",
        "    \"is_dessert_followup\", \"hours_overlap\", \"is_late_night_transition\",\n",
        "    \"topic_transition_dessert\", \"service_match\"\n",
        "]\n",
        "\n",
        "# Get columns that exist\n",
        "existing_cols = [col for col in new_feature_cols if col in pairs_with_features.columns]\n",
        "\n",
        "if existing_cols:\n",
        "    print(pairs_with_features.select(existing_cols).head(5))\n",
        "else:\n",
        "    print(\"  Checking available columns...\")\n",
        "    all_cols = pairs_with_features.columns\n",
        "    print(f\"  Total columns: {len(all_cols)}\")\n",
        "    print(f\"  Sample columns: {all_cols[:20]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 2/3] Generating negative samples with features...\n",
            "  Generating 4 negative samples per positive pair...\n",
            "\n",
            "================================================================================\n",
            "GENERATING NEGATIVE SAMPLES (4:1 ratio)\n",
            "================================================================================\n",
            "\n",
            "Negative sample distribution per positive:\n",
            "  - Geographic (10km): 2\n",
            "  - Relative results: 1\n",
            "  - Same category: 1\n",
            "  - Total: 4\n",
            "\n",
            "[1/4] Creating business lookup structures...\n",
            "  âœ“ Indexed 27,710 businesses\n",
            "\n",
            "[2/4] Generating 3,940,024 negative samples...\n",
            "  Progress: 10,000/985,006 (1.0%)\n",
            "  Progress: 20,000/985,006 (2.0%)\n",
            "  Progress: 30,000/985,006 (3.0%)\n",
            "  Progress: 40,000/985,006 (4.1%)\n",
            "  Progress: 50,000/985,006 (5.1%)\n",
            "  Progress: 60,000/985,006 (6.1%)\n",
            "  Progress: 70,000/985,006 (7.1%)\n",
            "  Progress: 80,000/985,006 (8.1%)\n",
            "  Progress: 90,000/985,006 (9.1%)\n",
            "  Progress: 100,000/985,006 (10.2%)\n",
            "  Progress: 110,000/985,006 (11.2%)\n",
            "  Progress: 120,000/985,006 (12.2%)\n",
            "  Progress: 130,000/985,006 (13.2%)\n",
            "  Progress: 140,000/985,006 (14.2%)\n",
            "  Progress: 150,000/985,006 (15.2%)\n",
            "  Progress: 160,000/985,006 (16.2%)\n",
            "  Progress: 170,000/985,006 (17.3%)\n",
            "  Progress: 180,000/985,006 (18.3%)\n",
            "  Progress: 190,000/985,006 (19.3%)\n",
            "  Progress: 200,000/985,006 (20.3%)\n",
            "  Progress: 210,000/985,006 (21.3%)\n",
            "  Progress: 220,000/985,006 (22.3%)\n",
            "  Progress: 230,000/985,006 (23.4%)\n",
            "  Progress: 240,000/985,006 (24.4%)\n",
            "  Progress: 250,000/985,006 (25.4%)\n",
            "  Progress: 260,000/985,006 (26.4%)\n",
            "  Progress: 270,000/985,006 (27.4%)\n",
            "  Progress: 280,000/985,006 (28.4%)\n",
            "  Progress: 290,000/985,006 (29.4%)\n",
            "  Progress: 300,000/985,006 (30.5%)\n",
            "  Progress: 310,000/985,006 (31.5%)\n",
            "  Progress: 320,000/985,006 (32.5%)\n",
            "  Progress: 330,000/985,006 (33.5%)\n",
            "  Progress: 340,000/985,006 (34.5%)\n",
            "  Progress: 350,000/985,006 (35.5%)\n",
            "  Progress: 360,000/985,006 (36.5%)\n",
            "  Progress: 370,000/985,006 (37.6%)\n",
            "  Progress: 380,000/985,006 (38.6%)\n",
            "  Progress: 390,000/985,006 (39.6%)\n",
            "  Progress: 400,000/985,006 (40.6%)\n",
            "  Progress: 410,000/985,006 (41.6%)\n",
            "  Progress: 420,000/985,006 (42.6%)\n",
            "  Progress: 430,000/985,006 (43.7%)\n",
            "  Progress: 440,000/985,006 (44.7%)\n",
            "  Progress: 450,000/985,006 (45.7%)\n",
            "  Progress: 460,000/985,006 (46.7%)\n",
            "  Progress: 470,000/985,006 (47.7%)\n",
            "  Progress: 480,000/985,006 (48.7%)\n",
            "  Progress: 490,000/985,006 (49.7%)\n",
            "  Progress: 500,000/985,006 (50.8%)\n",
            "  Progress: 510,000/985,006 (51.8%)\n",
            "  Progress: 520,000/985,006 (52.8%)\n",
            "  Progress: 530,000/985,006 (53.8%)\n",
            "  Progress: 540,000/985,006 (54.8%)\n",
            "  Progress: 550,000/985,006 (55.8%)\n",
            "  Progress: 560,000/985,006 (56.9%)\n",
            "  Progress: 570,000/985,006 (57.9%)\n",
            "  Progress: 580,000/985,006 (58.9%)\n",
            "  Progress: 590,000/985,006 (59.9%)\n",
            "  Progress: 600,000/985,006 (60.9%)\n",
            "  Progress: 610,000/985,006 (61.9%)\n",
            "  Progress: 620,000/985,006 (62.9%)\n",
            "  Progress: 630,000/985,006 (64.0%)\n",
            "  Progress: 640,000/985,006 (65.0%)\n",
            "  Progress: 650,000/985,006 (66.0%)\n",
            "  Progress: 660,000/985,006 (67.0%)\n",
            "  Progress: 670,000/985,006 (68.0%)\n",
            "  Progress: 680,000/985,006 (69.0%)\n",
            "  Progress: 690,000/985,006 (70.1%)\n",
            "  Progress: 700,000/985,006 (71.1%)\n",
            "  Progress: 710,000/985,006 (72.1%)\n",
            "  Progress: 720,000/985,006 (73.1%)\n",
            "  Progress: 730,000/985,006 (74.1%)\n",
            "  Progress: 740,000/985,006 (75.1%)\n",
            "  Progress: 750,000/985,006 (76.1%)\n",
            "  Progress: 760,000/985,006 (77.2%)\n",
            "  Progress: 770,000/985,006 (78.2%)\n",
            "  Progress: 780,000/985,006 (79.2%)\n",
            "  Progress: 790,000/985,006 (80.2%)\n",
            "  Progress: 800,000/985,006 (81.2%)\n",
            "  Progress: 810,000/985,006 (82.2%)\n",
            "  Progress: 820,000/985,006 (83.2%)\n",
            "  Progress: 830,000/985,006 (84.3%)\n",
            "  Progress: 840,000/985,006 (85.3%)\n",
            "  Progress: 850,000/985,006 (86.3%)\n",
            "  Progress: 860,000/985,006 (87.3%)\n",
            "  Progress: 870,000/985,006 (88.3%)\n",
            "  Progress: 880,000/985,006 (89.3%)\n",
            "  Progress: 890,000/985,006 (90.4%)\n",
            "  Progress: 900,000/985,006 (91.4%)\n",
            "  Progress: 910,000/985,006 (92.4%)\n",
            "  Progress: 920,000/985,006 (93.4%)\n",
            "  Progress: 930,000/985,006 (94.4%)\n",
            "  Progress: 940,000/985,006 (95.4%)\n",
            "  Progress: 950,000/985,006 (96.4%)\n",
            "  Progress: 960,000/985,006 (97.5%)\n",
            "  Progress: 970,000/985,006 (98.5%)\n",
            "  Progress: 980,000/985,006 (99.5%)\n",
            "  âœ“ Generated 3,083,421 negative samples\n",
            "\n",
            "[3/4] Converting to DataFrame...\n",
            "\n",
            "[4/4] Adding features to negative samples...\n",
            "\n",
            "================================================================================\n",
            "ADDING FEATURES TO POSITIVE PAIRS\n",
            "================================================================================\n",
            "\n",
            "[1/7] Adding spatial features...\n",
            "  - Calculating haversine distances...\n",
            "  - Creating distance buckets...\n",
            "  - Calculating directions...\n",
            "  âœ“ Added spatial features\n",
            "\n",
            "[2/7] Adding temporal features...\n",
            "  - Creating time delta buckets...\n",
            "  - Extracting hour and day of week...\n",
            "  - Identifying meal times...\n",
            "  âœ“ Added temporal features\n",
            "\n",
            "[3/7] Adding quality features...\n",
            "  âœ“ Added quality features\n",
            "\n",
            "[4/7] Adding price features...\n",
            "  - Joining with business data for prices...\n",
            "  âœ“ Added price features\n",
            "\n",
            "[5/7] Adding category features...\n",
            "  âœ“ Added category features\n",
            "\n",
            "[6/7] Adding relationship features...\n",
            "  - Joining with business data for relative_results...\n",
            "  - Checking relative_results membership...\n",
            "  âœ“ Added relationship features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "\n",
            "[14/14] Feature engineering complete!\n",
            "  Total features: 72\n",
            "  Total positive pairs: 3,083,421\n",
            "\n",
            "âœ“ Negative sampling complete!\n",
            "  Total negative pairs: 3,083,421\n",
            "  Negative:Positive ratio: 3.1:1\n",
            "\n",
            "âœ“ Generated 3,083,421 negative pairs\n",
            "  Adding enhanced features to negative samples...\n",
            "\n",
            "[8/14] Adding review sentiment features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added sentiment features\n",
            "\n",
            "[9/14] Adding user behavioral features...\n",
            "\n",
            "============================================================\n",
            "USER PROFILING\n",
            "============================================================\n",
            "  [1/6] Calculating basic user statistics...\n",
            "  [2/6] Calculating visit frequency and time patterns...\n",
            "  [3/6] Analyzing cuisine diversity and preferences...\n",
            "  [4/6] Calculating loyalty and exploration patterns...\n",
            "  [5/6] Analyzing rating patterns and sentiment...\n",
            "  [6/6] Combining and deriving final features...\n",
            "\n",
            "âœ“ User profiling complete!\n",
            "  Processed 2,546,362 users\n",
            "  Features per user: 22\n",
            "\n",
            "  User behavior summary:\n",
            "    Explorers: 1,135,876 (44.6%)\n",
            "    Frequent visitors: 455,777 (17.9%)\n",
            "    Diverse eaters: 1,081,160 (42.5%)\n",
            "    High standards: 1,487,609 (58.4%)\n",
            "    Price sensitive: 530,362 (20.8%)\n",
            "  âœ“ Added user behavioral features\n",
            "\n",
            "[12/14] Adding review topic features...\n",
            "\n",
            "============================================================\n",
            "REVIEW ANALYSIS\n",
            "============================================================\n",
            "  âœ“ TextBlob available for sentiment analysis\n",
            "  [1/3] Calculating sentiment scores...\n",
            "  [2/3] Aggregating by restaurant...\n",
            "  âœ“ Processed sentiment for 27,710 restaurants\n",
            "  [3/3] Extracting topic mentions...\n",
            "  âœ“ Processed topics for 27,710 restaurants\n",
            "\n",
            "âœ“ Review analysis complete!\n",
            "  Sentiment features: 27,710 restaurants\n",
            "  Topic features: 27,710 restaurants\n",
            "  âœ“ Added review topic features\n",
            "\n",
            "[10/14] Adding cuisine complementarity features...\n",
            "  âœ“ Added cuisine complementarity features\n",
            "\n",
            "[11/14] Adding operating hours features...\n",
            "  âœ“ Added operating hours features\n",
            "\n",
            "[13/14] Adding service options features...\n",
            "  âœ“ Added service options features\n",
            "  âœ“ Features added to negative samples\n"
          ]
        }
      ],
      "source": [
        "# Generate negative samples and add features (full pipeline)\n",
        "print(\"\\n[Step 2/3] Generating negative samples with features...\")\n",
        "print(\"  Generating 4 negative samples per positive pair...\")\n",
        "\n",
        "# This will be handled by the main features.py pipeline\n",
        "# For now, we'll use the direct approach\n",
        "negative_pairs = generate_negative_samples(\n",
        "    pairs_with_features, \n",
        "    biz_df, \n",
        "    n_negatives=4,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Generated {len(negative_pairs):,} negative pairs\")\n",
        "\n",
        "# Add features to negatives (need to call feature functions)\n",
        "from data.features import (\n",
        "    add_review_sentiment_features, add_user_behavioral_features,\n",
        "    add_cuisine_complementarity, add_operating_hours_features,\n",
        "    add_review_topic_features, add_service_options_features\n",
        ")\n",
        "\n",
        "if len(negative_pairs) > 0:\n",
        "    print(\"  Adding enhanced features to negative samples...\")\n",
        "    negative_pairs = add_review_sentiment_features(negative_pairs, reviews_df)\n",
        "    negative_pairs = add_user_behavioral_features(negative_pairs, reviews_df, biz_df)\n",
        "    negative_pairs = add_review_topic_features(negative_pairs, reviews_df)\n",
        "    negative_pairs = add_cuisine_complementarity(negative_pairs)\n",
        "    negative_pairs = add_operating_hours_features(negative_pairs, biz_df)\n",
        "    negative_pairs = add_service_options_features(negative_pairs, biz_df)\n",
        "    \n",
        "    print(\"  âœ“ Features added to negative samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 3/3] Combining positive and negative samples...\n",
            "\n",
            "âœ“ Features saved:\n",
            "  Total pairs (pos + neg): 4,068,427\n",
            "  Positive: 985,006\n",
            "  Negative: 3,083,421\n",
            "  File: /Users/sunho/Forkast/data/processed/ga/features_ga.parquet\n",
            "  Size: 641.9 MB\n"
          ]
        }
      ],
      "source": [
        "# Combine and save features\n",
        "print(\"\\n[Step 3/3] Combining positive and negative samples...\")\n",
        "\n",
        "# Ensure schema compatibility\n",
        "target_schema = pairs_with_features.schema\n",
        "target_columns = set(target_schema.keys())\n",
        "\n",
        "# First, select only columns that exist in target schema (drop extras)\n",
        "columns_to_select = [col for col in negative_pairs.columns if col in target_columns]\n",
        "negative_pairs = negative_pairs.select(columns_to_select)\n",
        "\n",
        "# Now ensure all target columns exist and have correct types\n",
        "cast_exprs = []\n",
        "for col_name, dtype in target_schema.items():\n",
        "    if col_name in negative_pairs.columns:\n",
        "        if negative_pairs[col_name].dtype != dtype:\n",
        "            cast_exprs.append(pl.col(col_name).cast(dtype))\n",
        "        else:\n",
        "            cast_exprs.append(pl.col(col_name))\n",
        "    else:\n",
        "        # Fill missing columns with defaults\n",
        "        if dtype == pl.Boolean:\n",
        "            cast_exprs.append(pl.lit(False).cast(dtype).alias(col_name))\n",
        "        elif dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:\n",
        "            cast_exprs.append(pl.lit(0).cast(dtype).alias(col_name))\n",
        "        elif dtype in [pl.Float32, pl.Float64]:\n",
        "            cast_exprs.append(pl.lit(0.0).cast(dtype).alias(col_name))\n",
        "        else:\n",
        "            cast_exprs.append(pl.lit(None).cast(dtype).alias(col_name))\n",
        "\n",
        "if cast_exprs:\n",
        "    negative_pairs = negative_pairs.with_columns(cast_exprs)\n",
        "\n",
        "# Ensure column order matches target schema\n",
        "negative_pairs = negative_pairs.select(list(target_schema.keys()))\n",
        "\n",
        "# Combine\n",
        "all_features_df = pl.concat([pairs_with_features, negative_pairs])\n",
        "\n",
        "features_output = PROCESSED_DIR / \"features_ga.parquet\"\n",
        "all_features_df.write_parquet(features_output, compression=\"snappy\")\n",
        "\n",
        "print(f\"\\nâœ“ Features saved:\")\n",
        "print(f\"  Total pairs (pos + neg): {len(all_features_df):,}\")\n",
        "print(f\"  Positive: {len(pairs_with_features):,}\")\n",
        "print(f\"  Negative: {len(negative_pairs):,}\")\n",
        "print(f\"  File: {features_output}\")\n",
        "print(f\"  Size: {features_output.stat().st_size / 1024 / 1024:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase A4: Temporal Data Splitting\n",
        "\n",
        "**What this does:**\n",
        "- Splits data chronologically by `src_ts` timestamp\n",
        "- 70% train, 15% validation, 15% test\n",
        "- Ensures no data leakage (future data not in training)\n",
        "\n",
        "**Expected time:** 2-5 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE A4: TEMPORAL DATA SPLITTING (XGBOOST ONLY)\n",
            "================================================================================\n",
            "\n",
            "[Loading data...]\n",
            "  XGBoost: 4,068,427 samples\n",
            "\n",
            "[Splitting XGBoost data...]\n",
            "\n",
            "================================================================================\n",
            "SPLITTING XGBOOST DATA (TEMPORAL)\n",
            "================================================================================\n",
            "\n",
            "[1/4] Sorting by timestamp...\n",
            "\n",
            "[2/4] Calculating split indices...\n",
            "  Total samples: 4,068,427\n",
            "  Train size: 2,847,898 (70.0%)\n",
            "  Val size: 610,264 (15.0%)\n",
            "  Test size: 610,265 (15.0%)\n",
            "\n",
            "[3/4] Splitting data...\n",
            "\n",
            "[4/4] Temporal ranges:\n",
            "  Train: 2005-12-09 00:00:00 to 2019-11-10 09:07:16\n",
            "  Val:   2019-11-10 09:07:16 to 2020-08-22 16:41:02\n",
            "  Test:  2020-08-22 16:41:48 to 2021-09-04 12:21:12\n",
            "\n",
            "  Label distribution:\n",
            "  Train - Pos: 684,981, Neg: 2,162,917\n",
            "  Val   - Pos: 148,300, Neg: 461,964\n",
            "  Test  - Pos: 151,725, Neg: 458,540\n",
            "\n",
            "[Saving XGBoost splits...]\n",
            "  âœ“ Saved train.parquet (2,847,898 samples)\n",
            "  âœ“ Saved val.parquet (610,264 samples)\n",
            "  âœ“ Saved test.parquet (610,265 samples)\n",
            "\n",
            "[Copying business metadata...]\n",
            "  âœ“ Copied biz_ga.parquet to xgboost_data/\n",
            "\n",
            "âœ“ Data split complete:\n",
            "  Train: 2,847,898 pairs (70.0%)\n",
            "  Validation: 610,264 pairs (15.0%)\n",
            "  Test: 610,265 pairs (15.0%)\n",
            "\n",
            "  Positive/negative ratio:\n",
            "    Train: 684,981 positive, 2,162,917 negative (24.1% positive)\n",
            "    Val: 148,300 positive, 461,964 negative (24.3% positive)\n",
            "    Test: 151,725 positive, 458,540 negative (24.9% positive)\n"
          ]
        }
      ],
      "source": [
        "from data.split_data import split_xgboost_only\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE A4: TEMPORAL DATA SPLITTING (XGBOOST ONLY)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run data splitting (XGBoost only - no LSTM data)\n",
        "split_xgboost_only()\n",
        "\n",
        "# Load split data\n",
        "train_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"train.parquet\")\n",
        "val_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"val.parquet\")\n",
        "test_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"test.parquet\")\n",
        "\n",
        "print(f\"\\nâœ“ Data split complete:\")\n",
        "print(f\"  Train: {len(train_df):,} pairs ({len(train_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs ({len(val_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_df):,} pairs ({len(test_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
        "print(f\"\\n  Positive/negative ratio:\")\n",
        "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
        "    pos_count = df[\"label\"].sum() if \"label\" in df.columns else 0\n",
        "    neg_count = len(df) - pos_count\n",
        "    print(f\"    {name}: {pos_count:,} positive, {neg_count:,} negative ({pos_count/len(df)*100:.1f}% positive)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase B1: XGBoost Ranking Model Training\n",
        "\n",
        "**What this does:**\n",
        "- Loads split data and prepares features\n",
        "- Trains XGBoost ranking model with `rank:pairwise` objective\n",
        "- Evaluates using Recall@K, MRR, nDCG@K\n",
        "- Generates feature importance analysis\n",
        "- Creates transition probability matrix\n",
        "\n",
        "**Expected time:** 20-40 minutes (depending on data size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "PHASE B1: XGBOOST RANKING MODEL TRAINING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "[1/3] Loading training data...\n",
            "  âœ“ 2,847,898 samples\n",
            "  Positive: 684,981\n",
            "  Negative: 2,162,917\n",
            "\n",
            "[2/3] Loading validation data...\n",
            "  âœ“ 610,264 samples\n",
            "  Positive: 148,300\n",
            "  Negative: 461,964\n",
            "\n",
            "[3/3] Loading test data...\n",
            "  âœ“ 610,265 samples\n",
            "  Positive: 151,725\n",
            "  Negative: 458,540\n",
            "\n",
            "âœ“ Data loaded:\n",
            "  Train: 2,847,898 pairs\n",
            "  Validation: 610,264 pairs\n",
            "  Test: 610,265 pairs\n"
          ]
        }
      ],
      "source": [
        "# Reload the module to get the latest changes\n",
        "import importlib\n",
        "import models.xgboost_ranker\n",
        "importlib.reload(models.xgboost_ranker)\n",
        "from models.xgboost_ranker import (\n",
        "    load_data, prepare_features, train_model, predict_ranking, \n",
        "    evaluate_ranking, analyze_feature_importance, save_model_and_results,\n",
        "    CATEGORICAL_FEATURES, NUMERICAL_FEATURES, BOOLEAN_FEATURES\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE B1: XGBOOST RANKING MODEL TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "data_dir = PROCESSED_DIR / \"xgboost_data\"\n",
        "train_df, val_df, test_df = load_data(data_dir)\n",
        "\n",
        "print(f\"\\nâœ“ Data loaded:\")\n",
        "print(f\"  Train: {len(train_df):,} pairs\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs\")\n",
        "print(f\"  Test: {len(test_df):,} pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 1/5] Preparing features...\n",
            "\n",
            "[Preparing ranking data...]\n",
            "  - One-hot encoding categorical features...\n",
            "  - Total features after encoding: 129\n",
            "  Features: (2847898, 129)\n",
            "  Labels: (2847898,)\n",
            "  Number of queries (source visits): 684,981\n",
            "  Avg candidates per query: 4.2\n",
            "  Total samples: 2,847,898\n",
            "\n",
            "[Preparing ranking data...]\n",
            "  - One-hot encoding categorical features...\n",
            "  - Total features after encoding: 129\n",
            "  Features: (610264, 129)\n",
            "  Labels: (610264,)\n",
            "  Number of queries (source visits): 148,301\n",
            "  Avg candidates per query: 4.1\n",
            "  Total samples: 610,264\n",
            "\n",
            "[Preparing ranking data...]\n",
            "  - One-hot encoding categorical features...\n",
            "  - Total features after encoding: 129\n",
            "  Features: (610265, 129)\n",
            "  Labels: (610265,)\n",
            "  Number of queries (source visits): 151,724\n",
            "  Avg candidates per query: 4.0\n",
            "  Total samples: 610,265\n",
            "\n",
            "âœ“ Features prepared:\n",
            "  Total features: 129\n",
            "    - Categorical: 6\n",
            "    - Numerical: 48\n",
            "    - Boolean: 52\n",
            "  Train shape: (2847898, 129)\n",
            "  Validation shape: (610264, 129)\n",
            "  Test shape: (610265, 129)\n"
          ]
        }
      ],
      "source": [
        "# Prepare features\n",
        "print(\"\\n[Step 1/5] Preparing features...\")\n",
        "\n",
        "X_train, y_train, group_train, feature_names = prepare_features(train_df)\n",
        "X_val, y_val, group_val, _ = prepare_features(val_df)\n",
        "X_test, y_test, group_test, _ = prepare_features(test_df)\n",
        "\n",
        "print(f\"\\nâœ“ Features prepared:\")\n",
        "print(f\"  Total features: {len(feature_names)}\")\n",
        "print(f\"    - Categorical: {len(CATEGORICAL_FEATURES)}\")\n",
        "print(f\"    - Numerical: {len(NUMERICAL_FEATURES)}\")\n",
        "print(f\"    - Boolean: {len(BOOLEAN_FEATURES)}\")\n",
        "print(f\"  Train shape: {X_train.shape}\")\n",
        "print(f\"  Validation shape: {X_val.shape}\")\n",
        "print(f\"  Test shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample feature names (showing enhanced features):\n",
            "\n",
            "  New Sentiment Features:\n",
            "    - src_sentiment_score\n",
            "    - dst_sentiment_score\n",
            "    - sentiment_transition\n",
            "    - sentiment_similarity\n",
            "    - is_sentiment_upgrade\n",
            "\n",
            "  New User Behavioral Features:\n",
            "    - user_avg_rating\n",
            "    - user_rating_std\n",
            "    - user_total_reviews\n",
            "    - user_unique_restaurants\n",
            "    - user_visit_frequency\n",
            "\n",
            "  New Operating Hours Features:\n",
            "    - delta_hours\n",
            "    - src_is_open_at_time\n",
            "    - dst_is_open_at_time\n",
            "    - hours_overlap\n",
            "    - delta_hours_bucket_0-3h\n"
          ]
        }
      ],
      "source": [
        "# Show sample of feature names\n",
        "print(\"\\nSample feature names (showing enhanced features):\")\n",
        "print(\"\\n  New Sentiment Features:\")\n",
        "sentiment_features = [f for f in feature_names if 'sentiment' in f.lower()]\n",
        "for f in sentiment_features[:5]:\n",
        "    print(f\"    - {f}\")\n",
        "\n",
        "print(\"\\n  New User Behavioral Features:\")\n",
        "user_features = [f for f in feature_names if f.startswith('user_')]\n",
        "for f in user_features[:5]:\n",
        "    print(f\"    - {f}\")\n",
        "\n",
        "print(\"\\n  New Operating Hours Features:\")\n",
        "hours_features = [f for f in feature_names if 'hours' in f.lower() or 'open' in f.lower()]\n",
        "for f in hours_features[:5]:\n",
        "    print(f\"    - {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "print(\"\\n[Step 2/5] Training XGBoost ranking model...\")\n",
        "print(\"  This may take 20-40 minutes depending on data size...\")\n",
        "\n",
        "model = train_model(\n",
        "    X_train, y_train, group_train,\n",
        "    X_val, y_val, group_val\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ Model training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on test set\n",
        "print(\"\\n[Step 3/5] Generating predictions...\")\n",
        "predictions_df = predict_ranking(model, test_df, top_k=10)\n",
        "\n",
        "print(f\"\\nâœ“ Predictions generated:\")\n",
        "print(f\"  Total predictions: {len(predictions_df):,}\")\n",
        "print(f\"  Unique source restaurants: {predictions_df['src_gmap_id'].n_unique():,}\")\n",
        "print(f\"  Average predictions per source: {len(predictions_df)/predictions_df['src_gmap_id'].n_unique():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "print(\"\\n[Step 4/5] Evaluating model performance...\")\n",
        "metrics = evaluate_ranking(predictions_df, k_values=[1, 5, 10])\n",
        "\n",
        "print(\"\\nâœ“ Evaluation complete:\")\n",
        "print(\"\\n  Performance Metrics:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n  Interpretation:\")\n",
        "print(f\"    - Recall@10: {metrics.get('recall@10', 0):.1%} of actual next visits are in top-10 predictions\")\n",
        "print(f\"    - MRR: {metrics.get('mrr', 0):.2f} average rank of first correct prediction\")\n",
        "print(f\"    - nDCG@10: {metrics.get('ndcg@10', 0):.4f} ranking quality (higher is better)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature importance\n",
        "print(\"\\n[Step 5/5] Analyzing feature importance...\")\n",
        "feature_importance = analyze_feature_importance(model, feature_names)\n",
        "\n",
        "print(\"\\nâœ“ Feature importance analysis complete!\")\n",
        "print(\"\\n  Top feature categories:\")\n",
        "sorted_groups = sorted(feature_importance['group_totals'].items(), \n",
        "                      key=lambda x: x[1], reverse=True)\n",
        "for group, total_score in sorted_groups[:5]:\n",
        "    print(f\"    {group:20s}: {total_score:8.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and results\n",
        "output_dir = PROCESSED_DIR\n",
        "save_model_and_results(\n",
        "    model, predictions_df, metrics, feature_importance, output_dir\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ Model and results saved!\")\n",
        "print(f\"  Model: {output_dir / 'models' / 'xgboost_ranker.json'}\")\n",
        "print(f\"  Predictions: {output_dir / 'models' / 'predictions' / 'xgboost_predictions.parquet'}\")\n",
        "print(f\"  Metrics: {output_dir / 'models' / 'metrics' / 'xgboost_metrics.json'}\")\n",
        "print(f\"  Feature importance: {output_dir / 'models' / 'metrics' / 'feature_importance.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "**What we've accomplished:**\n",
        "\n",
        "1. âœ… **Data Ingestion**: Loaded and normalized raw JSON data with enhanced metadata extraction\n",
        "2. âœ… **Sequence Creation**: Generated consecutive visit pairs (Aâ†’B)\n",
        "3. âœ… **Quality Filtering**: Filtered to high-quality user sequences\n",
        "4. âœ… **Enhanced Feature Engineering**: Created 75+ features including:\n",
        "   - Review sentiment analysis\n",
        "   - User behavioral profiles\n",
        "   - Cuisine complementarity\n",
        "   - Operating hours analysis\n",
        "   - Topic extraction\n",
        "   - Service options\n",
        "5. âœ… **Temporal Splitting**: Split data chronologically for proper validation\n",
        "6. âœ… **Model Training**: Trained XGBoost ranking model\n",
        "7. âœ… **Evaluation**: Calculated Recall@K, MRR, nDCG@K metrics\n",
        "8. âœ… **Feature Importance**: Analyzed which features matter most\n",
        "9. âœ… **Transition Matrix**: Generated Aâ†’B probability matrix for visualization\n",
        "\n",
        "**Output Files:**\n",
        "- `data/processed/ga/models/xgboost_ranker.json` - Trained model\n",
        "- `data/processed/ga/models/predictions/xgboost_predictions.parquet` - Test predictions\n",
        "- `data/processed/ga/models/metrics/xgboost_metrics.json` - Evaluation metrics\n",
        "- `data/processed/ga/models/metrics/feature_importance.json` - Feature importance\n",
        "- `data/processed/ga/transition_matrix/` - Transition matrices for visualization\n",
        "\n",
        "**Next Steps:**\n",
        "1. Review feature importance to understand what drives transitions\n",
        "2. Explore transition matrix to see top Aâ†’B predictions\n",
        "3. Use trained model for inference on new restaurant pairs\n",
        "4. Integrate with visualization dashboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase B2: Transition Matrix Generation (Optional)\n",
        "\n",
        "**What this does:**\n",
        "- Generates Aâ†’B transition probability matrix for visualization\n",
        "- Creates top-K predictions for each source restaurant\n",
        "- Exports in multiple formats (Parquet, JSON, sparse matrix)\n",
        "\n",
        "**Expected time:** 10-30 minutes (depending on number of restaurants)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.transition_matrix import build_transition_matrix, export_transition_matrix\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE B2: TRANSITION MATRIX GENERATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "biz_df = pl.read_parquet(BIZ_PARQUET)\n",
        "reviews_df = pl.read_parquet(REVIEWS_PARQUET)\n",
        "\n",
        "# Load trained model\n",
        "model_path = output_dir / \"models\" / \"xgboost_ranker.json\"\n",
        "if model_path.exists():\n",
        "    import xgboost as xgb\n",
        "    model = xgb.Booster()\n",
        "    model.load_model(str(model_path))\n",
        "    \n",
        "    print(f\"\\nâœ“ Model loaded from {model_path}\")\n",
        "    \n",
        "    # Build transition matrix (sample subset for efficiency)\n",
        "    print(\"\\nBuilding transition matrix...\")\n",
        "    print(\"  Note: This processes a sample of restaurants for efficiency\")\n",
        "    \n",
        "    transition_matrix_df = build_transition_matrix(\n",
        "        model, biz_df, feature_names, reviews_df, \n",
        "        top_k=10, max_candidates=100\n",
        "    )\n",
        "    \n",
        "    # Export\n",
        "    export_transition_matrix(transition_matrix_df, output_dir)\n",
        "    \n",
        "    print(\"\\nâœ“ Transition matrix generated!\")\n",
        "    print(f\"  Output: {output_dir / 'transition_matrix'}\")\n",
        "else:\n",
        "    print(\"\\nâš  Model not found. Skipping transition matrix generation.\")\n",
        "    print(\"  Run Phase B1 first to train the model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 80)\n",
        "print(\"PIPELINE COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nâœ“ All phases completed successfully!\")\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"  - Recall@10: {metrics.get('recall@10', 0):.4f}\")\n",
        "print(f\"  - MRR: {metrics.get('mrr', 0):.4f}\")\n",
        "print(f\"  - nDCG@10: {metrics.get('ndcg@10', 0):.4f}\")\n",
        "\n",
        "print(f\"\\nOutput Files:\")\n",
        "print(f\"  - Model: {output_dir / 'models' / 'xgboost_ranker.json'}\")\n",
        "print(f\"  - Metrics: {output_dir / 'models' / 'metrics' / 'xgboost_metrics.json'}\")\n",
        "print(f\"  - Feature Importance: {output_dir / 'models' / 'metrics' / 'feature_importance.json'}\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Ready for visualization and inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
