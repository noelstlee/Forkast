{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Full Pipeline: Training with Enhanced Features\n",
    "\n",
    "This notebook walks through the complete XGBoost training pipeline with all the enhanced features:\n",
    "- Review sentiment analysis\n",
    "- User behavioral profiling\n",
    "- Cuisine complementarity\n",
    "- Operating hours analysis\n",
    "- Topic extraction\n",
    "- Service options\n",
    "\n",
    "**Inputs (from SSD):**\n",
    "- `/Volumes/SunnySSD/review-Georgia.json` (7.2GB)\n",
    "- `/Volumes/SunnySSD/meta-Georgia.json` (168MB)\n",
    "\n",
    "**Outputs (to SSD by default):**\n",
    "- Processed Parquet files in `/Volumes/SunnySSD/Forkast_processed/ga/` (or local if SSD unavailable)\n",
    "- Cached review analysis and user profiles for faster subsequent runs\n",
    "- Trained XGBoost model\n",
    "- Feature importance analysis\n",
    "- Transition probability matrix\n",
    "- Evaluation metrics\n",
    "\n",
    "**Note:** Set `USE_SSD_FOR_OUTPUTS = False` in the setup cell to use local disk instead of SSD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Paths and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "base_dir = Path.cwd().parent\n",
    "sys.path.append(str(base_dir / 'src'))\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: Choose where to save processed data\n",
    "# ============================================================================\n",
    "USE_SSD_FOR_OUTPUTS = True  # Set to False to use local disk instead\n",
    "\n",
    "# Define paths\n",
    "SSD_PATH = Path(\"/Volumes/SunnySSD\")\n",
    "BASE_PROCESSED_DIR = base_dir / \"data\" / \"processed\" / \"ga\"\n",
    "\n",
    "# Choose output directory based on configuration\n",
    "if USE_SSD_FOR_OUTPUTS and SSD_PATH.exists():\n",
    "    PROCESSED_DIR = SSD_PATH / \"Forkast_processed\" / \"ga\"\n",
    "    print(f\" Using SSD for processed data output: {PROCESSED_DIR}\")\n",
    "else:\n",
    "    PROCESSED_DIR = BASE_PROCESSED_DIR\n",
    "    if USE_SSD_FOR_OUTPUTS:\n",
    "        print(f\"  SSD not found at {SSD_PATH}, falling back to local: {PROCESSED_DIR}\")\n",
    "    else:\n",
    "        print(f\"Using local disk for processed data output: {PROCESSED_DIR}\")\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# SSD input files (raw data)\n",
    "REVIEW_JSON = SSD_PATH / \"review-Georgia.json\"\n",
    "META_JSON = SSD_PATH / \"meta-Georgia.json\"\n",
    "\n",
    "# Output paths (will use SSD if USE_SSD_FOR_OUTPUTS=True)\n",
    "REVIEWS_PARQUET = PROCESSED_DIR / \"reviews_ga.parquet\"\n",
    "BIZ_PARQUET = PROCESSED_DIR / \"biz_ga.parquet\"\n",
    "\n",
    "# Cache directory for review analysis (can also go to SSD)\n",
    "CACHE_DIR = PROCESSED_DIR / \"cache\"\n",
    "\n",
    "print(\"\\nâœ“ Setup complete\")\n",
    "print(f\"  Base directory: {base_dir}\")\n",
    "print(f\"  SSD input path: {SSD_PATH}\")\n",
    "print(f\"  Processed output: {PROCESSED_DIR}\")\n",
    "print(f\"  Cache directory: {CACHE_DIR}\")\n",
    "print(f\"  Review JSON exists: {REVIEW_JSON.exists()}\")\n",
    "print(f\"  Meta JSON exists: {META_JSON.exists()}\")\n",
    "print(f\"\\nğŸ’¡ Tip: Set USE_SSD_FOR_OUTPUTS=False to use local disk instead\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A1: Data Ingestion & Normalization\n",
    "\n",
    "**What this does:**\n",
    "- Loads raw JSON files from SSD\n",
    "- Filters to Georgia geographic bounds\n",
    "- Normalizes categories and prices\n",
    "- **Extracts operating hours and service options** (NEW!)\n",
    "- Converts to efficient Parquet format\n",
    "\n",
    "**Expected time:** 10-20 minutes for 7GB review file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE A1: DATA INGESTION\n",
      "================================================================================\n",
      "\n",
      "[1/2] Ingesting metadata...\n",
      "\n",
      "================================================================================\n",
      "PHASE A1: INGESTING METADATA\n",
      "================================================================================\n",
      "Input: /Volumes/SunnySSD/meta-Georgia.json\n",
      "Output: /Volumes/SunnySSD/Forkast_processed/ga/biz_ga.parquet\n",
      "\n",
      "[1/6] Reading JSON file...\n",
      "  Loaded 166,381 raw businesses\n",
      "\n",
      "[2/6] Filtering to Georgia geographic bounds...\n",
      "  Retained 166,334 businesses in Georgia\n",
      "\n",
      "[3/6] Parsing price buckets...\n",
      "\n",
      "[4/6] Detecting closed businesses...\n",
      "\n",
      "[5/9] Normalizing categories...\n",
      "\n",
      "[6/9] Filtering to food-only businesses...\n",
      "  Retained 27,757 food-related businesses\n",
      "\n",
      "[7/9] Parsing operating hours...\n",
      "\n",
      "[8/9] Extracting service options...\n",
      "\n",
      "[9/9] Finalizing schema...\n",
      "  Final count: 27,710 unique businesses\n",
      "\n",
      "Writing to /Volumes/SunnySSD/Forkast_processed/ga/biz_ga.parquet...\n",
      "\n",
      "âœ“ Metadata ingestion complete!\n",
      "  Output size: 5.4 MB\n",
      "\n",
      "âœ“ Metadata ingested:\n",
      "  Total businesses: 27,710\n",
      "  Columns: ['gmap_id', 'name', 'lat', 'lon', 'category_main', 'category_all', 'avg_rating', 'num_reviews', 'price_bucket', 'is_closed', 'relative_results', 'operating_hours_parsed', 'days_open_count', 'avg_hours_per_day', 'has_late_night', 'is_24hr', 'is_weekend_only', 'has_delivery', 'has_takeout', 'has_dinein', 'accepts_reservations', 'has_quick_visit', 'requires_mask']\n",
      "  New metadata columns: operating_hours_parsed, has_delivery, has_takeout, etc.\n"
     ]
    }
   ],
   "source": [
    "# Reload the module to get the latest changes\n",
    "import importlib\n",
    "import data.ingest\n",
    "importlib.reload(data.ingest)\n",
    "from data.ingest import ingest_metadata, ingest_reviews\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE A1: DATA INGESTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ingest metadata\n",
    "print(\"\\n[1/2] Ingesting metadata...\")\n",
    "biz_df, valid_ids = ingest_metadata(\n",
    "    str(META_JSON),\n",
    "    str(BIZ_PARQUET)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Metadata ingested:\")\n",
    "print(f\"  Total businesses: {len(biz_df):,}\")\n",
    "print(f\"  Columns: {list(biz_df.columns)}\")\n",
    "print(f\"  New metadata columns: operating_hours_parsed, has_delivery, has_takeout, etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample metadata with new columns:\n",
      "shape: (5, 8)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ gmap_id    â”† name       â”† category_m â”† has_delive â”† has_takeou â”† has_late_ â”† is_24hr â”† days_open â”‚\n",
      "â”‚ ---        â”† ---        â”† ain        â”† ry         â”† t          â”† night     â”† ---     â”† _count    â”‚\n",
      "â”‚ str        â”† str        â”† ---        â”† ---        â”† ---        â”† ---       â”† bool    â”† ---       â”‚\n",
      "â”‚            â”†            â”† str        â”† bool       â”† bool       â”† bool      â”†         â”† i8        â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0x88fb9f97 â”† Coco and   â”† restaurant â”† true       â”† true       â”† true      â”† false   â”† 5         â”‚\n",
      "â”‚ aca0a781:0 â”† Moss       â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ xc7c75f293 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ 0x88f9d476 â”† Wendy's    â”† burger     â”† true       â”† true       â”† true      â”† false   â”† 7         â”‚\n",
      "â”‚ a685e82d:0 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ x31465f105 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ 0x88f5ed55 â”† Moonie's   â”† bbq        â”† true       â”† true       â”† false     â”† false   â”† 7         â”‚\n",
      "â”‚ ec039545:0 â”† Texas BBQ  â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ x90b2361c0 â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ 0x88f4fbb2 â”† The Juicy  â”† seafood    â”† true       â”† true       â”† false     â”† false   â”† null      â”‚\n",
      "â”‚ 79eb0bb5:0 â”† Crab       â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ x704a9dabf â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ 0x88f5bd2a â”† Manna      â”† asian      â”† true       â”† true       â”† false     â”† false   â”† 7         â”‚\n",
      "â”‚ 31ea3eed:0 â”† Korean     â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ xfe2fc1223 â”† Restaurant â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â”‚ â€¦          â”†            â”†            â”†            â”†            â”†           â”†         â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Preview metadata with new columns\n",
    "print(\"\\nSample metadata with new columns:\")\n",
    "print(biz_df.select([\n",
    "    \"gmap_id\", \"name\", \"category_main\", \"has_delivery\", \n",
    "    \"has_takeout\", \"has_late_night\", \"is_24hr\", \"days_open_count\"\n",
    "]).head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/2] Ingesting reviews...\n",
      "  This may take 10-20 minutes for the 7GB file...\n",
      "================================================================================\n",
      "PHASE A1: INGESTING REVIEWS\n",
      "================================================================================\n",
      "Input: /Volumes/SunnySSD/review-Georgia.json\n",
      "Output: /Volumes/SunnySSD/Forkast_processed/ga/reviews_ga.parquet\n",
      "\n",
      "[1/5] Reading JSON file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 24,060,125 raw reviews\n",
      "\n",
      "[2/5] Converting timestamps...\n",
      "\n",
      "[3/5] Filtering invalid timestamps...\n",
      "  Retained 24,060,120 reviews with valid timestamps\n",
      "\n",
      "[4/5] Creating derived columns...\n",
      "\n",
      "  Filtering out reviews with missing user_id or rating...\n",
      "  Removed 167,538 reviews with null user_id or rating\n",
      "  Retained 23,892,582 reviews\n",
      "\n",
      "  Filtering to 27,710 valid businesses...\n",
      "  Retained 10,494,609 reviews\n",
      "\n",
      "[5/5] Deduplicating...\n",
      "  Final count: 10,339,035 unique reviews\n",
      "\n",
      "Writing to /Volumes/SunnySSD/Forkast_processed/ga/reviews_ga.parquet...\n",
      "\n",
      "âœ“ Reviews ingestion complete!\n",
      "  Output size: 787.2 MB\n",
      "\n",
      "âœ“ Reviews ingested:\n",
      "  Total reviews: 10,339,035\n",
      "  Unique users: 2,546,362\n",
      "  Unique restaurants: 27,710\n",
      "  Date range: 2001-01-06 00:00:00 to 2021-09-08 01:43:37\n"
     ]
    }
   ],
   "source": [
    "# Ingest reviews\n",
    "print(\"\\n[2/2] Ingesting reviews...\")\n",
    "print(\"  This may take 10-20 minutes for the 7GB file...\")\n",
    "\n",
    "reviews_df = ingest_reviews(\n",
    "    str(REVIEW_JSON),\n",
    "    str(REVIEWS_PARQUET),\n",
    "    biz_gmap_ids=valid_ids\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Reviews ingested:\")\n",
    "print(f\"  Total reviews: {len(reviews_df):,}\")\n",
    "print(f\"  Unique users: {reviews_df['user_id'].n_unique():,}\")\n",
    "print(f\"  Unique restaurants: {reviews_df['gmap_id'].n_unique():,}\")\n",
    "print(f\"  Date range: {reviews_df['ts'].min()} to {reviews_df['ts'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A2: User Sequence Derivation\n",
    "\n",
    "**What this does:**\n",
    "- Creates user visit sequences sorted by timestamp\n",
    "- Generates consecutive visit pairs (A â†’ B)\n",
    "- Filters pairs within 0-168 hours (1 week window)\n",
    "\n",
    "**Expected time:** 5-10 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE A2: USER SEQUENCE DERIVATION\n",
      "================================================================================\n",
      "\n",
      "Using processed directory: /Volumes/SunnySSD/Forkast_processed/ga\n",
      "Loading data...\n",
      "  Loaded 10,339,035 reviews\n",
      "  Loaded 27,710 businesses\n",
      "================================================================================\n",
      "DERIVING USER SEQUENCES\n",
      "================================================================================\n",
      "\n",
      "[1/4] Joining reviews with business metadata...\n",
      "  Joined 10,339,035 reviews with business data\n",
      "\n",
      "[2/4] Sorting by user and timestamp...\n",
      "\n",
      "[3/4] Creating sequence indices...\n",
      "\n",
      "[4/4] Final sequence count: 10,339,035\n",
      "\n",
      "Writing to /Volumes/SunnySSD/Forkast_processed/ga/user_sequences_ga.parquet...\n",
      "  Output size: 256.8 MB\n",
      "\n",
      "================================================================================\n",
      "DERIVING CONSECUTIVE PAIRS\n",
      "================================================================================\n",
      "\n",
      "[1/5] Creating shifted columns for next visit...\n",
      "\n",
      "[2/5] Filtering out null destinations (last visit in sequence)...\n",
      "  Retained 7,792,673 pairs\n",
      "\n",
      "[3/5] Calculating time delta...\n",
      "\n",
      "[4/5] Filtering by time window...\n",
      "  Retained 4,152,155 pairs within 0-168 hour window\n",
      "\n",
      "[5/5] Final pair count: 4,152,155\n",
      "\n",
      "Writing to /Volumes/SunnySSD/Forkast_processed/ga/pairs_ga.parquet...\n",
      "  Output size: 193.6 MB\n",
      "\n",
      "================================================================================\n",
      "SEQUENCE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "USER SEQUENCES:\n",
      "  Total visits: 10,339,035\n",
      "  Unique users: 2,546,362\n",
      "  Unique businesses: 27,710\n",
      "\n",
      "SEQUENCE LENGTH DISTRIBUTION:\n",
      "  Mean: 4.1\n",
      "  Median: 1\n",
      "  Max: 648\n",
      "  Users with 2+ visits: 1,135,876 (44.6%)\n",
      "\n",
      "CONSECUTIVE PAIRS:\n",
      "  Total pairs: 4,152,155\n",
      "  Unique users: 744,680\n",
      "  Unique src businesses: 26,893\n",
      "  Unique dst businesses: 26,890\n",
      "\n",
      "TIME DELTA DISTRIBUTION:\n",
      "  Mean: 16.5 hours\n",
      "  Median: 0.0 hours\n",
      "  Min: 0.00 hours\n",
      "  Max: 168.0 hours\n",
      "\n",
      "TOP CATEGORY TRANSITIONS:\n",
      "  burger          â†’ burger         : 159,031\n",
      "  burger          â†’ american       : 102,706\n",
      "  american        â†’ american       : 102,402\n",
      "  american        â†’ burger         : 101,947\n",
      "  burger          â†’ fast_food      : 94,725\n",
      "  fast_food       â†’ burger         : 76,092\n",
      "  fast_food       â†’ fast_food      : 72,132\n",
      "  burger          â†’ mexican        : 63,426\n",
      "  mexican         â†’ burger         : 63,042\n",
      "  american        â†’ fast_food      : 59,154\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ“âœ“âœ“ PHASE A2 COMPLETE âœ“âœ“âœ“\n",
      "\n",
      "\n",
      "âœ“ Sequences created:\n",
      "  Total pairs: 4,152,155\n",
      "  Unique source restaurants: 26,893\n",
      "  Unique destination restaurants: 26,890\n",
      "  Average time gap: 16.5 hours\n"
     ]
    }
   ],
   "source": [
    "# Reload module to get latest changes\n",
    "import importlib\n",
    "import data.sequences\n",
    "importlib.reload(data.sequences)\n",
    "from data.sequences import main as sequences_main\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE A2: USER SEQUENCE DERIVATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nUsing processed directory: {PROCESSED_DIR}\")\n",
    "\n",
    "# Run sequence derivation with explicit path\n",
    "# This ensures it uses the same PROCESSED_DIR as configured in setup\n",
    "sequences_main(processed_dir=PROCESSED_DIR)\n",
    "\n",
    "# Load and preview results\n",
    "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_ga.parquet\")\n",
    "\n",
    "print(f\"\\nâœ“ Sequences created:\")\n",
    "print(f\"  Total pairs: {len(pairs_df):,}\")\n",
    "print(f\"  Unique source restaurants: {pairs_df['src_gmap_id'].n_unique():,}\")\n",
    "print(f\"  Unique destination restaurants: {pairs_df['dst_gmap_id'].n_unique():,}\")\n",
    "print(f\"  Average time gap: {pairs_df['delta_hours'].mean():.1f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample consecutive visit pairs:\n",
      "shape: (10, 6)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ user_id        â”† src_gmap_id    â”† dst_gmap_id    â”† delta_hours â”† src_ts         â”† dst_ts         â”‚\n",
      "â”‚ ---            â”† ---            â”† ---            â”† ---         â”† ---            â”† ---            â”‚\n",
      "â”‚ str            â”† str            â”† str            â”† f64         â”† datetime[Î¼s]   â”† datetime[Î¼s]   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 10000000713488 â”† 0x88f5758473a8 â”† 0x88f5759cc6ea â”† 0.019444    â”† 2019-05-14     â”† 2019-05-14     â”‚\n",
      "â”‚ 6560887        â”† bc37:0x9367635 â”† f6a9:0x16a87fc â”†             â”† 02:00:43       â”† 02:01:53       â”‚\n",
      "â”‚                â”† abâ€¦            â”† b6â€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000000713488 â”† 0x88f5759cc6ea â”† 0x88f59b65e7b2 â”† 0.013333    â”† 2019-05-14     â”† 2019-05-14     â”‚\n",
      "â”‚ 6560887        â”† f6a9:0x16a87fc â”† e153:0x208b4ad â”†             â”† 02:01:53       â”† 02:02:41       â”‚\n",
      "â”‚                â”† b6â€¦            â”† 7bâ€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000002095889 â”† 0x886069e02d76 â”† 0x886068a3bba5 â”† 0.010833    â”† 2019-01-18     â”† 2019-01-18     â”‚\n",
      "â”‚ 5295779        â”† edc3:0x3edd947 â”† 2f37:0x7a7c672 â”†             â”† 08:54:52       â”† 08:55:31       â”‚\n",
      "â”‚                â”† 21â€¦            â”† c1â€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000002095889 â”† 0x886068a3bba5 â”† 0x8860662a7353 â”† 0.030833    â”† 2019-01-18     â”† 2019-01-18     â”‚\n",
      "â”‚ 5295779        â”† 2f37:0x7a7c672 â”† 7617:0x555a125 â”†             â”† 08:55:31       â”† 08:57:22       â”‚\n",
      "â”‚                â”† c1â€¦            â”† 33â€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000002095889 â”† 0x88606899517f â”† 0x88f4e9c68ee5 â”† 0.074444    â”† 2019-06-13     â”† 2019-06-13     â”‚\n",
      "â”‚ 5295779        â”† 7c39:0x33b774d â”† c59d:0x1e1d13e â”†             â”† 16:46:52       â”† 16:51:20       â”‚\n",
      "â”‚                â”† f5â€¦            â”† baâ€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000002095889 â”† 0x88f4e9c68ee5 â”† 0x88f540feacdf â”† 0.035833    â”† 2019-06-13     â”† 2019-06-13     â”‚\n",
      "â”‚ 5295779        â”† c59d:0x1e1d13e â”† ed3f:0x18f90c6 â”†             â”† 16:51:20       â”† 16:53:29       â”‚\n",
      "â”‚                â”† baâ€¦            â”† 79â€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000003646453 â”† 0x88f5095f563d â”† 0x88f538384621 â”† 130.966389  â”† 2018-07-06     â”† 2018-07-11     â”‚\n",
      "â”‚ 0353686        â”† f8cb:0x4ebe1ca â”† c095:0x5ffd453 â”†             â”† 04:33:52       â”† 15:31:51       â”‚\n",
      "â”‚                â”† ccâ€¦            â”† c5â€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000003984331 â”† 0x88f5be87ef51 â”† 0x88f5bc57a0f7 â”† 142.004167  â”† 2018-01-19     â”† 2018-01-25     â”‚\n",
      "â”‚ 3841630        â”† 1c85:0x92f9ad1 â”† 8431:0xde92824 â”†             â”† 17:52:58       â”† 15:53:13       â”‚\n",
      "â”‚                â”† a4â€¦            â”† 81â€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000003984331 â”† 0x88f5bc57a0f7 â”† 0x88f5ba23a8bb â”† 0.013056    â”† 2018-01-25     â”† 2018-01-25     â”‚\n",
      "â”‚ 3841630        â”† 8431:0xde92824 â”† d8b9:0xe391237 â”†             â”† 15:53:13       â”† 15:54:00       â”‚\n",
      "â”‚                â”† 81â€¦            â”† 7fâ€¦            â”†             â”†                â”†                â”‚\n",
      "â”‚ 10000003984331 â”† 0x88f5ba23a8bb â”† 0x88f5953d7e77 â”† 0.006389    â”† 2018-01-25     â”† 2018-01-25     â”‚\n",
      "â”‚ 3841630        â”† d8b9:0xe391237 â”† 4547:0xa3adf61 â”†             â”† 15:54:00       â”† 15:54:23       â”‚\n",
      "â”‚                â”† 7fâ€¦            â”† 09â€¦            â”†             â”†                â”†                â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Preview sample pairs\n",
    "print(\"\\nSample consecutive visit pairs:\")\n",
    "print(pairs_df.select([\n",
    "    \"user_id\", \"src_gmap_id\", \"dst_gmap_id\", \n",
    "    \"delta_hours\", \"src_ts\", \"dst_ts\"\n",
    "]).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A2.5: Data Quality Filtering\n",
    "\n",
    "**What this does:**\n",
    "- Re-categorizes generic 'restaurant' businesses\n",
    "- Filters to users with 5+ visits\n",
    "- Removes pairs with very short time deltas\n",
    "\n",
    "**Expected time:** 2-5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE A2.5: DATA QUALITY FILTERING\n",
      "================================================================================\n",
      "\n",
      "Using processed directory: /Volumes/SunnySSD/Forkast_processed/ga\n",
      "================================================================================\n",
      "PHASE A2.5: DATA QUALITY FILTERING\n",
      "================================================================================\n",
      "\n",
      "Inputs:\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/biz_ga.parquet\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/user_sequences_ga.parquet\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/pairs_ga.parquet\n",
      "\n",
      "Outputs:\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/biz_ga.parquet (updated)\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/user_sequences_filtered_ga.parquet\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/pairs_filtered_ga.parquet\n",
      "\n",
      "[Loading data...]\n",
      "\n",
      "================================================================================\n",
      "STEP 1: RE-CATEGORIZING 'RESTAURANT' BUSINESSES\n",
      "================================================================================\n",
      "\n",
      "Original 'restaurant' businesses: 4,490\n",
      "\n",
      "[1/3] Inferring categories from business names...\n",
      "\n",
      "[2/3] Re-categorization results:\n",
      "  Successfully re-categorized: 1,883\n",
      "  Moved to 'other': 2,607\n",
      "\n",
      "[3/3] New category distribution (top 15):\n",
      "   1. fast_food           :  3,396 ( 12.3%)\n",
      "   2. american            :  2,759 ( 10.0%)\n",
      "   3. other               :  2,607 (  9.4%)\n",
      "   4. mexican             :  2,301 (  8.3%)\n",
      "   5. pizza               :  2,200 (  7.9%)\n",
      "   6. burger              :  2,136 (  7.7%)\n",
      "   7. bar                 :  1,587 (  5.7%)\n",
      "   8. breakfast           :  1,285 (  4.6%)\n",
      "   9. seafood             :  1,131 (  4.1%)\n",
      "  10. bbq                 :  1,056 (  3.8%)\n",
      "  11. chinese             :  1,032 (  3.7%)\n",
      "  12. bakery              :    871 (  3.1%)\n",
      "  13. asian               :    829 (  3.0%)\n",
      "  14. sushi               :    822 (  3.0%)\n",
      "  15. cafe                :    741 (  2.7%)\n",
      "\n",
      "Saving updated business data to /Volumes/SunnySSD/Forkast_processed/ga/biz_ga.parquet...\n",
      "  Output size: 5.4 MB\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FILTERING SEQUENCES (MIN 5 VISITS)\n",
      "================================================================================\n",
      "\n",
      "Original sequences: 10,339,035\n",
      "Original users: 2,546,362\n",
      "\n",
      "[1/2] Counting visits per user...\n",
      "  Users with 5+ visits: 471,601\n",
      "\n",
      "[2/2] Filtering sequences...\n",
      "\n",
      "Filtered sequences: 7,198,744\n",
      "Filtered users: 471,601\n",
      "Retention: 69.6% of sequences\n",
      "\n",
      "Saving filtered sequences to /Volumes/SunnySSD/Forkast_processed/ga/user_sequences_filtered_ga.parquet...\n",
      "  Output size: 155.0 MB\n",
      "\n",
      "================================================================================\n",
      "STEP 3: FILTERING PAIRS (MIN 0.2 HOURS)\n",
      "================================================================================\n",
      "\n",
      "Original pairs: 4,152,155\n",
      "\n",
      "[1/2] Time delta distribution (before):\n",
      "  <= 0.2 hours: 3,039,034\n",
      "  0.2-1 hours: 91,204\n",
      "  1-6 hours: 69,331\n",
      "  6-24 hours: 151,274\n",
      "  1-7 days: 801,312\n",
      "\n",
      "[2/2] Filtering pairs with delta_hours > 0.2...\n",
      "\n",
      "Filtered pairs: 1,113,121\n",
      "Removed: 3,039,034 (73.2%)\n",
      "Retention: 26.8%\n",
      "\n",
      "================================================================================\n",
      "STEP 4: REGENERATING PAIRS FROM FILTERED SEQUENCES\n",
      "================================================================================\n",
      "\n",
      "[1/4] Creating shifted columns for next visit...\n",
      "\n",
      "[2/4] Filtering out null destinations...\n",
      "  Retained 6,727,143 pairs\n",
      "\n",
      "[3/4] Calculating time delta and filtering...\n",
      "  Retained 985,006 pairs within 0.2-168 hour window\n",
      "\n",
      "[4/4] Final pair count: 985,006\n",
      "\n",
      "Writing to /Volumes/SunnySSD/Forkast_processed/ga/pairs_filtered_ga.parquet...\n",
      "  Output size: 49.7 MB\n",
      "\n",
      "================================================================================\n",
      "FINAL STATISTICS AFTER PHASE A2.5\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š BUSINESSES:\n",
      "  Total: 27,710\n",
      "  Categories: 26\n",
      "  'other' category: 2,607\n",
      "\n",
      "ğŸ‘¥ USER SEQUENCES:\n",
      "  Total visits: 7,198,744\n",
      "  Unique users: 471,601\n",
      "  Unique businesses: 27,654\n",
      "\n",
      "  Sequence length distribution:\n",
      "    Mean: 15.3\n",
      "    Median: 10\n",
      "    Min: 5\n",
      "    Max: 648\n",
      "\n",
      "ğŸ”— CONSECUTIVE PAIRS:\n",
      "  Total pairs: 985,006\n",
      "  Unique users: 274,432\n",
      "  Unique src businesses: 25,604\n",
      "  Unique dst businesses: 25,596\n",
      "\n",
      "  Time delta distribution:\n",
      "    Mean: 62.9 hours\n",
      "    Median: 49.6 hours\n",
      "    Min: 0.20 hours\n",
      "    Max: 168.0 hours\n",
      "\n",
      "  Top 10 category transitions:\n",
      "     1. burger          â†’ burger         : 28,510\n",
      "     2. american        â†’ american       : 23,115\n",
      "     3. american        â†’ burger         : 21,231\n",
      "     4. burger          â†’ american       : 20,961\n",
      "     5. fast_food       â†’ burger         : 18,153\n",
      "     6. burger          â†’ fast_food      : 16,978\n",
      "     7. mexican         â†’ burger         : 13,323\n",
      "     8. fast_food       â†’ american       : 13,101\n",
      "     9. burger          â†’ mexican        : 12,927\n",
      "    10. fast_food       â†’ fast_food      : 12,851\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ“âœ“âœ“ PHASE A2.5 COMPLETE âœ“âœ“âœ“\n",
      "\n",
      "\n",
      "âœ“ Quality filtering complete:\n",
      "  Filtered pairs: 985,006\n",
      "  Retention rate: 23.7%\n"
     ]
    }
   ],
   "source": [
    "# Reload module to get latest changes\n",
    "import importlib\n",
    "import data.filter_quality\n",
    "importlib.reload(data.filter_quality)\n",
    "from data.filter_quality import main as filter_main\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE A2.5: DATA QUALITY FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nUsing processed directory: {PROCESSED_DIR}\")\n",
    "\n",
    "# Run quality filtering with explicit path\n",
    "# This ensures it uses the same PROCESSED_DIR as configured in setup\n",
    "filter_main(processed_dir=PROCESSED_DIR)\n",
    "\n",
    "# Load filtered pairs\n",
    "filtered_pairs = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
    "\n",
    "print(f\"\\nâœ“ Quality filtering complete:\")\n",
    "print(f\"  Filtered pairs: {len(filtered_pairs):,}\")\n",
    "print(f\"  Retention rate: {len(filtered_pairs)/len(pairs_df)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A3: Feature Engineering (Enhanced!)\n",
    "\n",
    "**What this does:**\n",
    "\n",
    "**Original features (1-7):**\n",
    "1. Spatial: distance, neighborhood, direction\n",
    "2. Temporal: time gaps, day of week, meal type\n",
    "3. Quality: ratings, rating differences\n",
    "4. Price: price levels, price differences\n",
    "5. Category: cuisine type matching\n",
    "6. Relationship: relative_results ranking\n",
    "\n",
    "**NEW Enhanced features (8-14):**\n",
    "7. **Review Sentiment**: Average sentiment scores, sentiment transitions, review length\n",
    "8. **User Behavioral**: User preferences, explorer flags, loyalty scores\n",
    "9. **Cuisine Complementarity**: Complementary pairs (pizzaâ†’dessert), meal progressions\n",
    "10. **Operating Hours**: Open/closed checks, hours overlap, late night transitions\n",
    "11. **Topic Features**: Dessert/drinks mentions, topic transitions\n",
    "12. **Service Options**: Delivery/takeout flags, service matching\n",
    "\n",
    "**Expected time:** 15-30 minutes (includes sentiment analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE A3: ENHANCED FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "Loaded data:\n",
      "  Pairs: 985,006\n",
      "  Businesses: 27,710\n",
      "  Reviews: 10,339,035\n",
      "\n",
      "[Step 1/3] Adding all features to positive pairs...\n",
      "  This includes sentiment analysis and user profiling...\n",
      "  Using cache directory: /Volumes/SunnySSD/Forkast_processed/ga/cache\n",
      "\n",
      "================================================================================\n",
      "ADDING FEATURES TO POSITIVE PAIRS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "REVIEW ANALYSIS\n",
      "============================================================\n",
      "  âœ“ Loaded cached review analysis\n",
      "\n",
      "âœ“ Review analysis complete!\n",
      "  Sentiment features: 27,710 restaurants\n",
      "  Topic features: 27,710 restaurants\n",
      "\n",
      "============================================================\n",
      "USER PROFILING (cached)\n",
      "============================================================\n",
      "  âœ“ Loaded cached user profiles (1,365,435 users)\n",
      "\n",
      "[1/7] Adding spatial features...\n",
      "  - Calculating haversine distances...\n",
      "  - Creating distance buckets...\n",
      "  - Calculating directions...\n",
      "  âœ“ Added spatial features\n",
      "\n",
      "[2/7] Adding temporal features...\n",
      "  - Creating time delta buckets...\n",
      "  - Extracting hour and day of week...\n",
      "  - Identifying meal times...\n",
      "  âœ“ Added temporal features\n",
      "\n",
      "[3/7] Adding quality features...\n",
      "  âœ“ Added quality features\n",
      "\n",
      "[4/7] Adding price features...\n",
      "  - Joining with business data for prices...\n",
      "  âœ“ Added price features\n",
      "\n",
      "[5/7] Adding category features...\n",
      "  âœ“ Added category features\n",
      "\n",
      "[6/7] Adding relationship features...\n",
      "  - Joining with business data for relative_results...\n",
      "  - Checking relative_results membership...\n",
      "  âœ“ Added relationship features\n",
      "\n",
      "[8/14] Adding review sentiment features...\n",
      "  âœ“ Added sentiment features\n",
      "\n",
      "[9/14] Adding user behavioral features...\n",
      "  âœ“ Added user behavioral features\n",
      "\n",
      "[12/14] Adding review topic features...\n",
      "  âœ“ Added review topic features\n",
      "\n",
      "[10/14] Adding cuisine complementarity features...\n",
      "  âœ“ Added cuisine complementarity features\n",
      "\n",
      "[11/14] Adding operating hours features...\n",
      "  âœ“ Added operating hours features\n",
      "\n",
      "[13/14] Adding service options features...\n",
      "  âœ“ Added service options features\n",
      "\n",
      "[14/14] Feature engineering complete!\n",
      "  Total features: 122\n",
      "  Total positive pairs: 985,006\n",
      "\n",
      "âœ“ Features added to 985,006 positive pairs\n",
      "  Total features: 122\n"
     ]
    }
   ],
   "source": [
    "# Reload features module to get latest changes\n",
    "import importlib\n",
    "import data.features\n",
    "importlib.reload(data.features)\n",
    "from data.features import add_all_features, generate_negative_samples\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE A3: ENHANCED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data\n",
    "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
    "biz_df = pl.read_parquet(BIZ_PARQUET)\n",
    "reviews_df = pl.read_parquet(REVIEWS_PARQUET)\n",
    "\n",
    "print(f\"\\nLoaded data:\")\n",
    "print(f\"  Pairs: {len(pairs_df):,}\")\n",
    "print(f\"  Businesses: {len(biz_df):,}\")\n",
    "print(f\"  Reviews: {len(reviews_df):,}\")\n",
    "\n",
    "print(f\"\\n[Step 1/3] Adding all features to positive pairs...\")\n",
    "print(\"  This includes sentiment analysis and user profiling...\")\n",
    "print(f\"  Using cache directory: {CACHE_DIR}\")\n",
    "\n",
    "# Use analysis context to share cached results between positive and negative samples\n",
    "analysis_context = {}\n",
    "user_profile_cache = CACHE_DIR / \"user_profiles.parquet\"\n",
    "\n",
    "pairs_with_features = add_all_features(\n",
    "    pairs_df, \n",
    "    biz_df, \n",
    "    reviews_df,\n",
    "    review_cache_path=CACHE_DIR,\n",
    "    user_profile_cache_path=user_profile_cache,\n",
    "    max_reviews_per_biz=200,  # Limit reviews per business for faster processing\n",
    "    analysis_context=analysis_context\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Features added to {len(pairs_with_features):,} positive pairs\")\n",
    "print(f\"  Total features: {len(pairs_with_features.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Enhanced Features Preview:\n",
      "shape: (5, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ src_senti â”† dst_senti â”† sentiment â”† user_is_e â”† â€¦ â”† hours_ove â”† is_late_n â”† topic_tra â”† service_ â”‚\n",
      "â”‚ ment_scor â”† ment_scor â”† _transiti â”† xplorer   â”†   â”† rlap      â”† ight_tran â”† nsition_d â”† match    â”‚\n",
      "â”‚ e         â”† e         â”† on        â”† ---       â”†   â”† ---       â”† sition    â”† essert    â”† ---      â”‚\n",
      "â”‚ ---       â”† ---       â”† ---       â”† bool      â”†   â”† bool      â”† ---       â”† ---       â”† bool     â”‚\n",
      "â”‚ f32       â”† f32       â”† f32       â”†           â”†   â”†           â”† bool      â”† bool      â”†          â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 0.394242  â”† 0.226775  â”† -0.167467 â”† false     â”† â€¦ â”† true      â”† false     â”† false     â”† true     â”‚\n",
      "â”‚ 0.415869  â”† 0.363042  â”† -0.052827 â”† false     â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
      "â”‚ 0.375567  â”† 0.389334  â”† 0.013767  â”† false     â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
      "â”‚ 0.123254  â”† 0.403555  â”† 0.2803    â”† false     â”† â€¦ â”† true      â”† false     â”† false     â”† false    â”‚\n",
      "â”‚ 0.516366  â”† 0.478168  â”† -0.038198 â”† true      â”† â€¦ â”† true      â”† false     â”† false     â”† true     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# Preview new features\n",
    "print(\"\\nNew Enhanced Features Preview:\")\n",
    "new_feature_cols = [\n",
    "    \"src_sentiment_score\", \"dst_sentiment_score\", \"sentiment_transition\",\n",
    "    \"user_is_explorer\", \"user_visit_frequency\", \"cuisine_pair_type\",\n",
    "    \"is_dessert_followup\", \"hours_overlap\", \"is_late_night_transition\",\n",
    "    \"topic_transition_dessert\", \"service_match\"\n",
    "]\n",
    "\n",
    "# Get columns that exist\n",
    "existing_cols = [col for col in new_feature_cols if col in pairs_with_features.columns]\n",
    "\n",
    "if existing_cols:\n",
    "    print(pairs_with_features.select(existing_cols).head(5))\n",
    "else:\n",
    "    print(\"  Checking available columns...\")\n",
    "    all_cols = pairs_with_features.columns\n",
    "    print(f\"  Total columns: {len(all_cols)}\")\n",
    "    print(f\"  Sample columns: {all_cols[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2/3] Generating negative samples with features...\n",
      "  Generating 4 negative samples per positive pair...\n",
      "  Using cached analysis data from previous step...\n",
      "\n",
      "================================================================================\n",
      "GENERATING NEGATIVE SAMPLES (4:1 ratio)\n",
      "================================================================================\n",
      "\n",
      "Negative sample distribution per positive:\n",
      "  - Geographic (10km): 2\n",
      "  - Relative results: 1\n",
      "  - Same category: 1\n",
      "  - Total: 4\n",
      "\n",
      "[1/4] Creating business lookup structures...\n",
      "  âœ“ Indexed 27,710 businesses\n",
      "\n",
      "[2/4] Generating 3,940,024 negative samples...\n",
      "  (This will take ~985 minutes for 985,006 positive pairs)\n",
      "  Progress: 5,000/985,006 (0.5%) - ETA: 5.7 min\n",
      "  Progress: 10,000/985,006 (1.0%) - ETA: 5.8 min\n",
      "  Progress: 15,000/985,006 (1.5%) - ETA: 5.8 min\n",
      "  Progress: 20,000/985,006 (2.0%) - ETA: 5.8 min\n",
      "  Progress: 25,000/985,006 (2.5%) - ETA: 5.8 min\n",
      "  Progress: 30,000/985,006 (3.0%) - ETA: 5.7 min\n",
      "  Progress: 35,000/985,006 (3.6%) - ETA: 5.7 min\n",
      "  Progress: 40,000/985,006 (4.1%) - ETA: 5.7 min\n",
      "  Progress: 45,000/985,006 (4.6%) - ETA: 5.6 min\n",
      "  Progress: 50,000/985,006 (5.1%) - ETA: 5.6 min\n",
      "  Progress: 55,000/985,006 (5.6%) - ETA: 5.6 min\n",
      "  Progress: 60,000/985,006 (6.1%) - ETA: 5.5 min\n",
      "  Progress: 65,000/985,006 (6.6%) - ETA: 5.5 min\n",
      "  Progress: 70,000/985,006 (7.1%) - ETA: 5.4 min\n",
      "  Progress: 75,000/985,006 (7.6%) - ETA: 5.4 min\n",
      "  Progress: 80,000/985,006 (8.1%) - ETA: 5.4 min\n",
      "  Progress: 85,000/985,006 (8.6%) - ETA: 5.4 min\n",
      "  Progress: 90,000/985,006 (9.1%) - ETA: 5.3 min\n",
      "  Progress: 95,000/985,006 (9.6%) - ETA: 5.3 min\n",
      "  Progress: 100,000/985,006 (10.2%) - ETA: 5.3 min\n",
      "  Progress: 105,000/985,006 (10.7%) - ETA: 5.3 min\n",
      "  Progress: 110,000/985,006 (11.2%) - ETA: 5.2 min\n",
      "  Progress: 115,000/985,006 (11.7%) - ETA: 5.2 min\n",
      "  Progress: 120,000/985,006 (12.2%) - ETA: 5.2 min\n",
      "  Progress: 125,000/985,006 (12.7%) - ETA: 5.1 min\n",
      "  Progress: 130,000/985,006 (13.2%) - ETA: 5.1 min\n",
      "  Progress: 135,000/985,006 (13.7%) - ETA: 5.1 min\n",
      "  Progress: 140,000/985,006 (14.2%) - ETA: 5.0 min\n",
      "  Progress: 145,000/985,006 (14.7%) - ETA: 5.0 min\n",
      "  Progress: 150,000/985,006 (15.2%) - ETA: 5.0 min\n",
      "  Progress: 155,000/985,006 (15.7%) - ETA: 4.9 min\n",
      "  Progress: 160,000/985,006 (16.2%) - ETA: 4.9 min\n",
      "  Progress: 165,000/985,006 (16.8%) - ETA: 4.9 min\n",
      "  Progress: 170,000/985,006 (17.3%) - ETA: 4.9 min\n",
      "  Progress: 175,000/985,006 (17.8%) - ETA: 4.8 min\n",
      "  Progress: 180,000/985,006 (18.3%) - ETA: 4.8 min\n",
      "  Progress: 185,000/985,006 (18.8%) - ETA: 4.8 min\n",
      "  Progress: 190,000/985,006 (19.3%) - ETA: 4.7 min\n",
      "  Progress: 195,000/985,006 (19.8%) - ETA: 4.7 min\n",
      "  Progress: 200,000/985,006 (20.3%) - ETA: 4.7 min\n",
      "  Progress: 205,000/985,006 (20.8%) - ETA: 4.6 min\n",
      "  Progress: 210,000/985,006 (21.3%) - ETA: 4.6 min\n",
      "  Progress: 215,000/985,006 (21.8%) - ETA: 4.6 min\n",
      "  Progress: 220,000/985,006 (22.3%) - ETA: 4.6 min\n",
      "  Progress: 225,000/985,006 (22.8%) - ETA: 4.5 min\n",
      "  Progress: 230,000/985,006 (23.4%) - ETA: 4.5 min\n",
      "  Progress: 235,000/985,006 (23.9%) - ETA: 4.5 min\n",
      "  Progress: 240,000/985,006 (24.4%) - ETA: 4.5 min\n",
      "  Progress: 245,000/985,006 (24.9%) - ETA: 4.4 min\n",
      "  Progress: 250,000/985,006 (25.4%) - ETA: 4.4 min\n",
      "  Progress: 255,000/985,006 (25.9%) - ETA: 4.4 min\n",
      "  Progress: 260,000/985,006 (26.4%) - ETA: 4.3 min\n",
      "  Progress: 265,000/985,006 (26.9%) - ETA: 4.3 min\n",
      "  Progress: 270,000/985,006 (27.4%) - ETA: 4.3 min\n",
      "  Progress: 275,000/985,006 (27.9%) - ETA: 4.2 min\n",
      "  Progress: 280,000/985,006 (28.4%) - ETA: 4.2 min\n",
      "  Progress: 285,000/985,006 (28.9%) - ETA: 4.2 min\n",
      "  Progress: 290,000/985,006 (29.4%) - ETA: 4.2 min\n",
      "  Progress: 295,000/985,006 (29.9%) - ETA: 4.1 min\n",
      "  Progress: 300,000/985,006 (30.5%) - ETA: 4.1 min\n",
      "  Progress: 305,000/985,006 (31.0%) - ETA: 4.1 min\n",
      "  Progress: 310,000/985,006 (31.5%) - ETA: 4.0 min\n",
      "  Progress: 315,000/985,006 (32.0%) - ETA: 4.0 min\n",
      "  Progress: 320,000/985,006 (32.5%) - ETA: 4.0 min\n",
      "  Progress: 325,000/985,006 (33.0%) - ETA: 3.9 min\n",
      "  Progress: 330,000/985,006 (33.5%) - ETA: 3.9 min\n",
      "  Progress: 335,000/985,006 (34.0%) - ETA: 3.9 min\n",
      "  Progress: 340,000/985,006 (34.5%) - ETA: 3.8 min\n",
      "  Progress: 345,000/985,006 (35.0%) - ETA: 3.8 min\n",
      "  Progress: 350,000/985,006 (35.5%) - ETA: 3.8 min\n",
      "  Progress: 355,000/985,006 (36.0%) - ETA: 3.8 min\n",
      "  Progress: 360,000/985,006 (36.5%) - ETA: 3.7 min\n",
      "  Progress: 365,000/985,006 (37.1%) - ETA: 3.7 min\n",
      "  Progress: 370,000/985,006 (37.6%) - ETA: 3.7 min\n",
      "  Progress: 375,000/985,006 (38.1%) - ETA: 3.6 min\n",
      "  Progress: 380,000/985,006 (38.6%) - ETA: 3.6 min\n",
      "  Progress: 385,000/985,006 (39.1%) - ETA: 3.6 min\n",
      "  Progress: 390,000/985,006 (39.6%) - ETA: 3.5 min\n",
      "  Progress: 395,000/985,006 (40.1%) - ETA: 3.5 min\n",
      "  Progress: 400,000/985,006 (40.6%) - ETA: 3.5 min\n",
      "  Progress: 405,000/985,006 (41.1%) - ETA: 3.5 min\n",
      "  Progress: 410,000/985,006 (41.6%) - ETA: 3.4 min\n",
      "  Progress: 415,000/985,006 (42.1%) - ETA: 3.4 min\n",
      "  Progress: 420,000/985,006 (42.6%) - ETA: 3.4 min\n",
      "  Progress: 425,000/985,006 (43.1%) - ETA: 3.3 min\n",
      "  Progress: 430,000/985,006 (43.7%) - ETA: 3.3 min\n",
      "  Progress: 435,000/985,006 (44.2%) - ETA: 3.3 min\n",
      "  Progress: 440,000/985,006 (44.7%) - ETA: 3.3 min\n",
      "  Progress: 445,000/985,006 (45.2%) - ETA: 3.2 min\n",
      "  Progress: 450,000/985,006 (45.7%) - ETA: 3.2 min\n",
      "  Progress: 455,000/985,006 (46.2%) - ETA: 3.2 min\n",
      "  Progress: 460,000/985,006 (46.7%) - ETA: 3.1 min\n",
      "  Progress: 465,000/985,006 (47.2%) - ETA: 3.1 min\n",
      "  Progress: 470,000/985,006 (47.7%) - ETA: 3.1 min\n",
      "  Progress: 475,000/985,006 (48.2%) - ETA: 3.0 min\n",
      "  Progress: 480,000/985,006 (48.7%) - ETA: 3.0 min\n",
      "  Progress: 485,000/985,006 (49.2%) - ETA: 3.0 min\n",
      "  Progress: 490,000/985,006 (49.7%) - ETA: 2.9 min\n",
      "  Progress: 495,000/985,006 (50.3%) - ETA: 2.9 min\n",
      "  Progress: 500,000/985,006 (50.8%) - ETA: 2.9 min\n",
      "  Progress: 505,000/985,006 (51.3%) - ETA: 2.9 min\n",
      "  Progress: 510,000/985,006 (51.8%) - ETA: 2.8 min\n",
      "  Progress: 515,000/985,006 (52.3%) - ETA: 2.8 min\n",
      "  Progress: 520,000/985,006 (52.8%) - ETA: 2.8 min\n",
      "  Progress: 525,000/985,006 (53.3%) - ETA: 2.7 min\n",
      "  Progress: 530,000/985,006 (53.8%) - ETA: 2.7 min\n",
      "  Progress: 535,000/985,006 (54.3%) - ETA: 2.7 min\n",
      "  Progress: 540,000/985,006 (54.8%) - ETA: 2.7 min\n",
      "  Progress: 545,000/985,006 (55.3%) - ETA: 2.6 min\n",
      "  Progress: 550,000/985,006 (55.8%) - ETA: 2.6 min\n",
      "  Progress: 555,000/985,006 (56.3%) - ETA: 2.6 min\n",
      "  Progress: 560,000/985,006 (56.9%) - ETA: 2.5 min\n",
      "  Progress: 565,000/985,006 (57.4%) - ETA: 2.5 min\n",
      "  Progress: 570,000/985,006 (57.9%) - ETA: 2.5 min\n",
      "  Progress: 575,000/985,006 (58.4%) - ETA: 2.4 min\n",
      "  Progress: 580,000/985,006 (58.9%) - ETA: 2.4 min\n",
      "  Progress: 585,000/985,006 (59.4%) - ETA: 2.4 min\n",
      "  Progress: 590,000/985,006 (59.9%) - ETA: 2.4 min\n",
      "  Progress: 595,000/985,006 (60.4%) - ETA: 2.3 min\n",
      "  Progress: 600,000/985,006 (60.9%) - ETA: 2.3 min\n",
      "  Progress: 605,000/985,006 (61.4%) - ETA: 2.3 min\n",
      "  Progress: 610,000/985,006 (61.9%) - ETA: 2.2 min\n",
      "  Progress: 615,000/985,006 (62.4%) - ETA: 2.2 min\n",
      "  Progress: 620,000/985,006 (62.9%) - ETA: 2.2 min\n",
      "  Progress: 625,000/985,006 (63.5%) - ETA: 2.1 min\n",
      "  Progress: 630,000/985,006 (64.0%) - ETA: 2.1 min\n",
      "  Progress: 635,000/985,006 (64.5%) - ETA: 2.1 min\n",
      "  Progress: 640,000/985,006 (65.0%) - ETA: 2.1 min\n",
      "  Progress: 645,000/985,006 (65.5%) - ETA: 2.0 min\n",
      "  Progress: 650,000/985,006 (66.0%) - ETA: 2.0 min\n",
      "  Progress: 655,000/985,006 (66.5%) - ETA: 2.0 min\n",
      "  Progress: 660,000/985,006 (67.0%) - ETA: 1.9 min\n",
      "  Progress: 665,000/985,006 (67.5%) - ETA: 1.9 min\n",
      "  Progress: 670,000/985,006 (68.0%) - ETA: 1.9 min\n",
      "  Progress: 675,000/985,006 (68.5%) - ETA: 1.8 min\n",
      "  Progress: 680,000/985,006 (69.0%) - ETA: 1.8 min\n",
      "  Progress: 685,000/985,006 (69.5%) - ETA: 1.8 min\n",
      "  Progress: 690,000/985,006 (70.1%) - ETA: 1.8 min\n",
      "  Progress: 695,000/985,006 (70.6%) - ETA: 1.7 min\n",
      "  Progress: 700,000/985,006 (71.1%) - ETA: 1.7 min\n",
      "  Progress: 705,000/985,006 (71.6%) - ETA: 1.7 min\n",
      "  Progress: 710,000/985,006 (72.1%) - ETA: 1.6 min\n",
      "  Progress: 715,000/985,006 (72.6%) - ETA: 1.6 min\n",
      "  Progress: 720,000/985,006 (73.1%) - ETA: 1.6 min\n",
      "  Progress: 725,000/985,006 (73.6%) - ETA: 1.5 min\n",
      "  Progress: 730,000/985,006 (74.1%) - ETA: 1.5 min\n",
      "  Progress: 735,000/985,006 (74.6%) - ETA: 1.5 min\n",
      "  Progress: 740,000/985,006 (75.1%) - ETA: 1.5 min\n",
      "  Progress: 745,000/985,006 (75.6%) - ETA: 1.4 min\n",
      "  Progress: 750,000/985,006 (76.1%) - ETA: 1.4 min\n",
      "  Progress: 755,000/985,006 (76.6%) - ETA: 1.4 min\n",
      "  Progress: 760,000/985,006 (77.2%) - ETA: 1.3 min\n",
      "  Progress: 765,000/985,006 (77.7%) - ETA: 1.3 min\n",
      "  Progress: 770,000/985,006 (78.2%) - ETA: 1.3 min\n",
      "  Progress: 775,000/985,006 (78.7%) - ETA: 1.3 min\n",
      "  Progress: 780,000/985,006 (79.2%) - ETA: 1.2 min\n",
      "  Progress: 785,000/985,006 (79.7%) - ETA: 1.2 min\n",
      "  Progress: 790,000/985,006 (80.2%) - ETA: 1.2 min\n",
      "  Progress: 795,000/985,006 (80.7%) - ETA: 1.1 min\n",
      "  Progress: 800,000/985,006 (81.2%) - ETA: 1.1 min\n",
      "  Progress: 805,000/985,006 (81.7%) - ETA: 1.1 min\n",
      "  Progress: 810,000/985,006 (82.2%) - ETA: 1.0 min\n",
      "  Progress: 815,000/985,006 (82.7%) - ETA: 1.0 min\n",
      "  Progress: 820,000/985,006 (83.2%) - ETA: 1.0 min\n",
      "  Progress: 825,000/985,006 (83.8%) - ETA: 1.0 min\n",
      "  Progress: 830,000/985,006 (84.3%) - ETA: 0.9 min\n",
      "  Progress: 835,000/985,006 (84.8%) - ETA: 0.9 min\n",
      "  Progress: 840,000/985,006 (85.3%) - ETA: 0.9 min\n",
      "  Progress: 845,000/985,006 (85.8%) - ETA: 0.8 min\n",
      "  Progress: 850,000/985,006 (86.3%) - ETA: 0.8 min\n",
      "  Progress: 855,000/985,006 (86.8%) - ETA: 0.8 min\n",
      "  Progress: 860,000/985,006 (87.3%) - ETA: 0.8 min\n",
      "  Progress: 865,000/985,006 (87.8%) - ETA: 0.7 min\n",
      "  Progress: 870,000/985,006 (88.3%) - ETA: 0.7 min\n",
      "  Progress: 875,000/985,006 (88.8%) - ETA: 0.7 min\n",
      "  Progress: 880,000/985,006 (89.3%) - ETA: 0.6 min\n",
      "  Progress: 885,000/985,006 (89.8%) - ETA: 0.6 min\n",
      "  Progress: 890,000/985,006 (90.4%) - ETA: 0.6 min\n",
      "  Progress: 895,000/985,006 (90.9%) - ETA: 0.5 min\n",
      "  Progress: 900,000/985,006 (91.4%) - ETA: 0.5 min\n",
      "  Progress: 905,000/985,006 (91.9%) - ETA: 0.5 min\n",
      "  Progress: 910,000/985,006 (92.4%) - ETA: 0.5 min\n",
      "  Progress: 915,000/985,006 (92.9%) - ETA: 0.4 min\n",
      "  Progress: 920,000/985,006 (93.4%) - ETA: 0.4 min\n",
      "  Progress: 925,000/985,006 (93.9%) - ETA: 0.4 min\n",
      "  Progress: 930,000/985,006 (94.4%) - ETA: 0.3 min\n",
      "  Progress: 935,000/985,006 (94.9%) - ETA: 0.3 min\n",
      "  Progress: 940,000/985,006 (95.4%) - ETA: 0.3 min\n",
      "  Progress: 945,000/985,006 (95.9%) - ETA: 0.2 min\n",
      "  Progress: 950,000/985,006 (96.4%) - ETA: 0.2 min\n",
      "  Progress: 955,000/985,006 (97.0%) - ETA: 0.2 min\n",
      "  Progress: 960,000/985,006 (97.5%) - ETA: 0.2 min\n",
      "  Progress: 965,000/985,006 (98.0%) - ETA: 0.1 min\n",
      "  Progress: 970,000/985,006 (98.5%) - ETA: 0.1 min\n",
      "  Progress: 975,000/985,006 (99.0%) - ETA: 0.1 min\n",
      "  Progress: 980,000/985,006 (99.5%) - ETA: 0.0 min\n",
      "  Progress: 985,000/985,006 (100.0%) - ETA: 0.0 min\n",
      "  âœ“ Generated 3,378,080 negative samples\n",
      "\n",
      "[3/4] Converting to DataFrame...\n",
      "\n",
      "[4/4] Adding features to negative samples...\n",
      "\n",
      "================================================================================\n",
      "ADDING FEATURES TO POSITIVE PAIRS\n",
      "================================================================================\n",
      "\n",
      "[1/7] Adding spatial features...\n",
      "  - Calculating haversine distances...\n",
      "  - Creating distance buckets...\n",
      "  - Calculating directions...\n",
      "  âœ“ Added spatial features\n",
      "\n",
      "[2/7] Adding temporal features...\n",
      "  - Creating time delta buckets...\n",
      "  - Extracting hour and day of week...\n",
      "  - Identifying meal times...\n",
      "  âœ“ Added temporal features\n",
      "\n",
      "[3/7] Adding quality features...\n",
      "  âœ“ Added quality features\n",
      "\n",
      "[4/7] Adding price features...\n",
      "  - Joining with business data for prices...\n",
      "  âœ“ Added price features\n",
      "\n",
      "[5/7] Adding category features...\n",
      "  âœ“ Added category features\n",
      "\n",
      "[6/7] Adding relationship features...\n",
      "  - Joining with business data for relative_results...\n",
      "  - Checking relative_results membership...\n",
      "  âœ“ Added relationship features\n",
      "\n",
      "[10/14] Adding cuisine complementarity features...\n",
      "  âœ“ Added cuisine complementarity features\n",
      "\n",
      "[11/14] Adding operating hours features...\n",
      "  âœ“ Added operating hours features\n",
      "\n",
      "[13/14] Adding service options features...\n",
      "  âœ“ Added service options features\n",
      "\n",
      "[14/14] Feature engineering complete!\n",
      "  Total features: 73\n",
      "  Total positive pairs: 3,378,080\n",
      "\n",
      "[8/14] Adding review sentiment features...\n",
      "  âœ“ Added sentiment features\n",
      "\n",
      "[9/14] Adding user behavioral features...\n",
      "  âœ“ Added user behavioral features\n",
      "\n",
      "[12/14] Adding review topic features...\n",
      "  âœ“ Added review topic features\n",
      "\n",
      "âœ“ Negative sampling complete!\n",
      "  Total negative pairs: 3,378,080\n",
      "  Negative:Positive ratio: 3.4:1\n",
      "\n",
      "âœ“ Generated 3,378,080 negative pairs\n",
      "  âœ“ Features already added during generation (using cached analysis)\n"
     ]
    }
   ],
   "source": [
    "# Generate negative samples and add features (full pipeline)\n",
    "print(\"\\n[Step 2/3] Generating negative samples with features...\")\n",
    "print(\"  Generating 4 negative samples per positive pair...\")\n",
    "print(f\"  Using cached analysis data from previous step...\")\n",
    "\n",
    "# Reuse cached analysis context from positive pairs generation\n",
    "# (analysis_context should be available from cell 13)\n",
    "user_profile_cache = CACHE_DIR / \"user_profiles.parquet\"\n",
    "\n",
    "negative_pairs = generate_negative_samples(\n",
    "    pairs_with_features, \n",
    "    biz_df, \n",
    "    n_negatives=4,\n",
    "    random_seed=42,\n",
    "    sentiment_df=analysis_context.get(\"sentiment_df\"),\n",
    "    topics_df=analysis_context.get(\"topics_df\"),\n",
    "    user_profiles=analysis_context.get(\"user_profiles_df\"),\n",
    "    reviews_df=analysis_context.get(\"reviews_subset_df\"),\n",
    "    review_cache_path=CACHE_DIR,\n",
    "    user_profile_cache_path=user_profile_cache,\n",
    "    max_reviews_per_biz=200\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(negative_pairs):,} negative pairs\")\n",
    "print(f\"  âœ“ Features already added during generation (using cached analysis)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 3/3] Combining positive and negative samples...\n",
      "\n",
      "âœ“ Features saved:\n",
      "  Total pairs (pos + neg): 4,363,086\n",
      "  Positive: 985,006\n",
      "  Negative: 3,378,080\n",
      "  File: /Volumes/SunnySSD/Forkast_processed/ga/features_ga.parquet\n",
      "  Size: 640.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Combine and save features\n",
    "print(\"\\n[Step 3/3] Combining positive and negative samples...\")\n",
    "\n",
    "# Ensure schema compatibility\n",
    "target_schema = pairs_with_features.schema\n",
    "target_columns = set(target_schema.keys())\n",
    "\n",
    "# First, select only columns that exist in target schema (drop extras)\n",
    "columns_to_select = [col for col in negative_pairs.columns if col in target_columns]\n",
    "negative_pairs = negative_pairs.select(columns_to_select)\n",
    "\n",
    "# Now ensure all target columns exist and have correct types\n",
    "cast_exprs = []\n",
    "for col_name, dtype in target_schema.items():\n",
    "    if col_name in negative_pairs.columns:\n",
    "        if negative_pairs[col_name].dtype != dtype:\n",
    "            cast_exprs.append(pl.col(col_name).cast(dtype))\n",
    "        else:\n",
    "            cast_exprs.append(pl.col(col_name))\n",
    "    else:\n",
    "        # Fill missing columns with defaults\n",
    "        if dtype == pl.Boolean:\n",
    "            cast_exprs.append(pl.lit(False).cast(dtype).alias(col_name))\n",
    "        elif dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:\n",
    "            cast_exprs.append(pl.lit(0).cast(dtype).alias(col_name))\n",
    "        elif dtype in [pl.Float32, pl.Float64]:\n",
    "            cast_exprs.append(pl.lit(0.0).cast(dtype).alias(col_name))\n",
    "        else:\n",
    "            cast_exprs.append(pl.lit(None).cast(dtype).alias(col_name))\n",
    "\n",
    "if cast_exprs:\n",
    "    negative_pairs = negative_pairs.with_columns(cast_exprs)\n",
    "\n",
    "# Ensure column order matches target schema\n",
    "negative_pairs = negative_pairs.select(list(target_schema.keys()))\n",
    "\n",
    "# Combine\n",
    "all_features_df = pl.concat([pairs_with_features, negative_pairs])\n",
    "\n",
    "features_output = PROCESSED_DIR / \"features_ga.parquet\"\n",
    "all_features_df.write_parquet(features_output, compression=\"snappy\")\n",
    "\n",
    "print(f\"\\nâœ“ Features saved:\")\n",
    "print(f\"  Total pairs (pos + neg): {len(all_features_df):,}\")\n",
    "print(f\"  Positive: {len(pairs_with_features):,}\")\n",
    "print(f\"  Negative: {len(negative_pairs):,}\")\n",
    "print(f\"  File: {features_output}\")\n",
    "print(f\"  Size: {features_output.stat().st_size / 1024 / 1024:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase A4: Temporal Data Splitting\n",
    "\n",
    "**What this does:**\n",
    "- Splits data chronologically by `src_ts` timestamp\n",
    "- 70% train, 15% validation, 15% test\n",
    "- Ensures no data leakage (future data not in training)\n",
    "\n",
    "**Expected time:** 2-5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE A4: TEMPORAL DATA SPLITTING (XGBOOST ONLY)\n",
      "================================================================================\n",
      "\n",
      "Using processed directory: /Volumes/SunnySSD/Forkast_processed/ga\n",
      "\n",
      "[Loading data...]\n",
      "  XGBoost: 4,363,086 samples\n",
      "\n",
      "[Splitting XGBoost data...]\n",
      "\n",
      "================================================================================\n",
      "SPLITTING XGBOOST DATA (TEMPORAL)\n",
      "================================================================================\n",
      "\n",
      "[1/4] Sorting by timestamp...\n",
      "\n",
      "[2/4] Calculating split indices...\n",
      "  Total samples: 4,363,086\n",
      "  Train size: 3,054,160 (70.0%)\n",
      "  Val size: 654,462 (15.0%)\n",
      "  Test size: 654,464 (15.0%)\n",
      "\n",
      "[3/4] Splitting data...\n",
      "\n",
      "[4/4] Temporal ranges:\n",
      "  Train: 2005-12-09 00:00:00 to 2019-11-11 10:04:14\n",
      "  Val:   2019-11-11 10:04:14 to 2020-08-23 14:26:59\n",
      "  Test:  2020-08-23 14:26:59 to 2021-09-04 12:21:12\n",
      "\n",
      "  Label distribution:\n",
      "  Train - Pos: 685,853, Neg: 2,368,307\n",
      "  Val   - Pos: 148,053, Neg: 506,409\n",
      "  Test  - Pos: 151,100, Neg: 503,364\n",
      "\n",
      "[Saving XGBoost splits...]\n",
      "  âœ“ Saved train.parquet (3,054,160 samples)\n",
      "  âœ“ Saved val.parquet (654,462 samples)\n",
      "  âœ“ Saved test.parquet (654,464 samples)\n",
      "\n",
      "[Copying business metadata...]\n",
      "  âœ“ Copied biz_ga.parquet to xgboost_data/\n",
      "\n",
      "âœ“ Data split complete:\n",
      "  Train: 3,054,160 pairs (70.0%)\n",
      "  Validation: 654,462 pairs (15.0%)\n",
      "  Test: 654,464 pairs (15.0%)\n",
      "\n",
      "  Positive/negative ratio:\n",
      "    Train: 685,853 positive, 2,368,307 negative (22.5% positive)\n",
      "    Val: 148,053 positive, 506,409 negative (22.6% positive)\n",
      "    Test: 151,100 positive, 503,364 negative (23.1% positive)\n"
     ]
    }
   ],
   "source": [
    "# Reload module to get latest changes\n",
    "import importlib\n",
    "import data.split_data\n",
    "importlib.reload(data.split_data)\n",
    "from data.split_data import split_xgboost_only\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE A4: TEMPORAL DATA SPLITTING (XGBOOST ONLY)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nUsing processed directory: {PROCESSED_DIR}\")\n",
    "\n",
    "# Run data splitting (XGBoost only - no LSTM data) with explicit path\n",
    "# This ensures it uses the same PROCESSED_DIR as configured in setup\n",
    "split_xgboost_only(processed_dir=PROCESSED_DIR)\n",
    "\n",
    "# Load split data\n",
    "train_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"train.parquet\")\n",
    "val_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"val.parquet\")\n",
    "test_df = pl.read_parquet(PROCESSED_DIR / \"xgboost_data\" / \"test.parquet\")\n",
    "\n",
    "print(f\"\\nâœ“ Data split complete:\")\n",
    "print(f\"  Train: {len(train_df):,} pairs ({len(train_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_df):,} pairs ({len(val_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_df):,} pairs ({len(test_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)\")\n",
    "print(f\"\\n  Positive/negative ratio:\")\n",
    "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    pos_count = df[\"label\"].sum() if \"label\" in df.columns else 0\n",
    "    neg_count = len(df) - pos_count\n",
    "    print(f\"    {name}: {pos_count:,} positive, {neg_count:,} negative ({pos_count/len(df)*100:.1f}% positive)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REGENERATING FEATURES WITH FIXED NEGATIVE SAMPLING\n",
      "================================================================================\n",
      "\n",
      "Loaded data:\n",
      "  Pairs: 985,006\n",
      "  Businesses: 27,710\n",
      "  Reviews: 10,339,035\n",
      "\n",
      "[1/3] Adding features to positive pairs...\n",
      "  Using cache directory: /Volumes/SunnySSD/Forkast_processed/ga/cache\n",
      "\n",
      "================================================================================\n",
      "ADDING FEATURES TO POSITIVE PAIRS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "REVIEW ANALYSIS\n",
      "============================================================\n",
      "  âœ“ Loaded cached review analysis\n",
      "\n",
      "âœ“ Review analysis complete!\n",
      "  Sentiment features: 27,710 restaurants\n",
      "  Topic features: 27,710 restaurants\n",
      "\n",
      "============================================================\n",
      "USER PROFILING (cached)\n",
      "============================================================\n",
      "  âœ“ Loaded cached user profiles (1,365,435 users)\n",
      "\n",
      "[1/7] Adding spatial features...\n",
      "  - Calculating haversine distances...\n",
      "  - Creating distance buckets...\n",
      "  - Calculating directions...\n",
      "  âœ“ Added spatial features\n",
      "\n",
      "[2/7] Adding temporal features...\n",
      "  - Creating time delta buckets...\n",
      "  - Extracting hour and day of week...\n",
      "  - Identifying meal times...\n",
      "  âœ“ Added temporal features\n",
      "\n",
      "[3/7] Adding quality features...\n",
      "  âœ“ Added quality features\n",
      "\n",
      "[4/7] Adding price features...\n",
      "  - Joining with business data for prices...\n",
      "  âœ“ Added price features\n",
      "\n",
      "[5/7] Adding category features...\n",
      "  âœ“ Added category features\n",
      "\n",
      "[6/7] Adding relationship features...\n",
      "  - Joining with business data for relative_results...\n",
      "  - Checking relative_results membership...\n",
      "  âœ“ Added relationship features\n",
      "\n",
      "[8/14] Adding review sentiment features...\n",
      "  âœ“ Added sentiment features\n",
      "\n",
      "[9/14] Adding user behavioral features...\n",
      "  âœ“ Added user behavioral features\n",
      "\n",
      "[12/14] Adding review topic features...\n",
      "  âœ“ Added review topic features\n",
      "\n",
      "[10/14] Adding cuisine complementarity features...\n",
      "  âœ“ Added cuisine complementarity features\n",
      "\n",
      "[11/14] Adding operating hours features...\n",
      "  âœ“ Added operating hours features\n",
      "\n",
      "[13/14] Adding service options features...\n",
      "  âœ“ Added service options features\n",
      "\n",
      "[14/14] Feature engineering complete!\n",
      "  Total features: 122\n",
      "  Total positive pairs: 985,006\n",
      "\n",
      "[2/3] Generating negative samples with realistic timestamps...\n",
      "\n",
      "================================================================================\n",
      "GENERATING NEGATIVE SAMPLES (4:1 ratio)\n",
      "================================================================================\n",
      "\n",
      "Negative sample distribution per positive:\n",
      "  - Geographic (10km): 2\n",
      "  - Relative results: 1\n",
      "  - Same category: 1\n",
      "  - Total: 4\n",
      "\n",
      "[1/4] Creating business lookup structures...\n",
      "  âœ“ Indexed 27,710 businesses\n",
      "\n",
      "[2/4] Generating 3,940,024 negative samples...\n",
      "  (This will take ~985 minutes for 985,006 positive pairs)\n",
      "  Progress: 5,000/985,006 (0.5%) - ETA: 6.1 min\n",
      "  Progress: 10,000/985,006 (1.0%) - ETA: 6.4 min\n",
      "  Progress: 15,000/985,006 (1.5%) - ETA: 6.2 min\n",
      "  Progress: 20,000/985,006 (2.0%) - ETA: 6.1 min\n",
      "  Progress: 25,000/985,006 (2.5%) - ETA: 6.1 min\n",
      "  Progress: 30,000/985,006 (3.0%) - ETA: 6.1 min\n",
      "  Progress: 35,000/985,006 (3.6%) - ETA: 6.0 min\n",
      "  Progress: 40,000/985,006 (4.1%) - ETA: 6.1 min\n",
      "  Progress: 45,000/985,006 (4.6%) - ETA: 6.0 min\n",
      "  Progress: 50,000/985,006 (5.1%) - ETA: 6.0 min\n",
      "  Progress: 55,000/985,006 (5.6%) - ETA: 5.9 min\n",
      "  Progress: 60,000/985,006 (6.1%) - ETA: 5.9 min\n",
      "  Progress: 65,000/985,006 (6.6%) - ETA: 5.8 min\n",
      "  Progress: 70,000/985,006 (7.1%) - ETA: 5.9 min\n",
      "  Progress: 75,000/985,006 (7.6%) - ETA: 5.8 min\n",
      "  Progress: 80,000/985,006 (8.1%) - ETA: 5.8 min\n",
      "  Progress: 85,000/985,006 (8.6%) - ETA: 5.7 min\n",
      "  Progress: 90,000/985,006 (9.1%) - ETA: 5.7 min\n",
      "  Progress: 95,000/985,006 (9.6%) - ETA: 5.7 min\n",
      "  Progress: 100,000/985,006 (10.2%) - ETA: 5.7 min\n",
      "  Progress: 105,000/985,006 (10.7%) - ETA: 5.6 min\n",
      "  Progress: 110,000/985,006 (11.2%) - ETA: 5.6 min\n",
      "  Progress: 115,000/985,006 (11.7%) - ETA: 5.5 min\n",
      "  Progress: 120,000/985,006 (12.2%) - ETA: 5.5 min\n",
      "  Progress: 125,000/985,006 (12.7%) - ETA: 5.5 min\n",
      "  Progress: 130,000/985,006 (13.2%) - ETA: 5.5 min\n",
      "  Progress: 135,000/985,006 (13.7%) - ETA: 5.4 min\n",
      "  Progress: 140,000/985,006 (14.2%) - ETA: 5.4 min\n",
      "  Progress: 145,000/985,006 (14.7%) - ETA: 5.3 min\n",
      "  Progress: 150,000/985,006 (15.2%) - ETA: 5.3 min\n",
      "  Progress: 155,000/985,006 (15.7%) - ETA: 5.3 min\n",
      "  Progress: 160,000/985,006 (16.2%) - ETA: 5.3 min\n",
      "  Progress: 165,000/985,006 (16.8%) - ETA: 5.3 min\n",
      "  Progress: 170,000/985,006 (17.3%) - ETA: 5.3 min\n",
      "  Progress: 175,000/985,006 (17.8%) - ETA: 5.3 min\n",
      "  Progress: 180,000/985,006 (18.3%) - ETA: 5.3 min\n",
      "  Progress: 185,000/985,006 (18.8%) - ETA: 5.3 min\n",
      "  Progress: 190,000/985,006 (19.3%) - ETA: 5.3 min\n",
      "  Progress: 195,000/985,006 (19.8%) - ETA: 5.2 min\n",
      "  Progress: 200,000/985,006 (20.3%) - ETA: 5.2 min\n",
      "  Progress: 205,000/985,006 (20.8%) - ETA: 5.2 min\n",
      "  Progress: 210,000/985,006 (21.3%) - ETA: 5.1 min\n",
      "  Progress: 215,000/985,006 (21.8%) - ETA: 5.1 min\n",
      "  Progress: 220,000/985,006 (22.3%) - ETA: 5.0 min\n",
      "  Progress: 225,000/985,006 (22.8%) - ETA: 5.0 min\n",
      "  Progress: 230,000/985,006 (23.4%) - ETA: 5.0 min\n",
      "  Progress: 235,000/985,006 (23.9%) - ETA: 4.9 min\n",
      "  Progress: 240,000/985,006 (24.4%) - ETA: 4.9 min\n",
      "  Progress: 245,000/985,006 (24.9%) - ETA: 4.9 min\n",
      "  Progress: 250,000/985,006 (25.4%) - ETA: 4.8 min\n",
      "  Progress: 255,000/985,006 (25.9%) - ETA: 4.8 min\n",
      "  Progress: 260,000/985,006 (26.4%) - ETA: 4.8 min\n",
      "  Progress: 265,000/985,006 (26.9%) - ETA: 4.7 min\n",
      "  Progress: 270,000/985,006 (27.4%) - ETA: 4.7 min\n",
      "  Progress: 275,000/985,006 (27.9%) - ETA: 4.7 min\n",
      "  Progress: 280,000/985,006 (28.4%) - ETA: 4.6 min\n",
      "  Progress: 285,000/985,006 (28.9%) - ETA: 4.6 min\n",
      "  Progress: 290,000/985,006 (29.4%) - ETA: 4.6 min\n",
      "  Progress: 295,000/985,006 (29.9%) - ETA: 4.5 min\n",
      "  Progress: 300,000/985,006 (30.5%) - ETA: 4.5 min\n",
      "  Progress: 305,000/985,006 (31.0%) - ETA: 4.4 min\n",
      "  Progress: 310,000/985,006 (31.5%) - ETA: 4.4 min\n",
      "  Progress: 315,000/985,006 (32.0%) - ETA: 4.4 min\n",
      "  Progress: 320,000/985,006 (32.5%) - ETA: 4.3 min\n",
      "  Progress: 325,000/985,006 (33.0%) - ETA: 4.3 min\n",
      "  Progress: 330,000/985,006 (33.5%) - ETA: 4.3 min\n",
      "  Progress: 335,000/985,006 (34.0%) - ETA: 4.2 min\n",
      "  Progress: 340,000/985,006 (34.5%) - ETA: 4.2 min\n",
      "  Progress: 345,000/985,006 (35.0%) - ETA: 4.2 min\n",
      "  Progress: 350,000/985,006 (35.5%) - ETA: 4.1 min\n",
      "  Progress: 355,000/985,006 (36.0%) - ETA: 4.1 min\n",
      "  Progress: 360,000/985,006 (36.5%) - ETA: 4.1 min\n",
      "  Progress: 365,000/985,006 (37.1%) - ETA: 4.0 min\n",
      "  Progress: 370,000/985,006 (37.6%) - ETA: 4.0 min\n",
      "  Progress: 375,000/985,006 (38.1%) - ETA: 4.0 min\n",
      "  Progress: 380,000/985,006 (38.6%) - ETA: 3.9 min\n",
      "  Progress: 385,000/985,006 (39.1%) - ETA: 3.9 min\n",
      "  Progress: 390,000/985,006 (39.6%) - ETA: 3.9 min\n",
      "  Progress: 395,000/985,006 (40.1%) - ETA: 3.8 min\n",
      "  Progress: 400,000/985,006 (40.6%) - ETA: 3.8 min\n",
      "  Progress: 405,000/985,006 (41.1%) - ETA: 3.8 min\n",
      "  Progress: 410,000/985,006 (41.6%) - ETA: 3.7 min\n",
      "  Progress: 415,000/985,006 (42.1%) - ETA: 3.7 min\n",
      "  Progress: 420,000/985,006 (42.6%) - ETA: 3.7 min\n",
      "  Progress: 425,000/985,006 (43.1%) - ETA: 3.6 min\n",
      "  Progress: 430,000/985,006 (43.7%) - ETA: 3.6 min\n",
      "  Progress: 435,000/985,006 (44.2%) - ETA: 3.6 min\n",
      "  Progress: 440,000/985,006 (44.7%) - ETA: 3.5 min\n",
      "  Progress: 445,000/985,006 (45.2%) - ETA: 3.5 min\n",
      "  Progress: 450,000/985,006 (45.7%) - ETA: 3.5 min\n",
      "  Progress: 455,000/985,006 (46.2%) - ETA: 3.4 min\n",
      "  Progress: 460,000/985,006 (46.7%) - ETA: 3.4 min\n",
      "  Progress: 465,000/985,006 (47.2%) - ETA: 3.4 min\n",
      "  Progress: 470,000/985,006 (47.7%) - ETA: 3.3 min\n",
      "  Progress: 475,000/985,006 (48.2%) - ETA: 3.3 min\n",
      "  Progress: 480,000/985,006 (48.7%) - ETA: 3.3 min\n",
      "  Progress: 485,000/985,006 (49.2%) - ETA: 3.2 min\n",
      "  Progress: 490,000/985,006 (49.7%) - ETA: 3.2 min\n",
      "  Progress: 495,000/985,006 (50.3%) - ETA: 3.2 min\n",
      "  Progress: 500,000/985,006 (50.8%) - ETA: 3.1 min\n",
      "  Progress: 505,000/985,006 (51.3%) - ETA: 3.1 min\n",
      "  Progress: 510,000/985,006 (51.8%) - ETA: 3.1 min\n",
      "  Progress: 515,000/985,006 (52.3%) - ETA: 3.0 min\n",
      "  Progress: 520,000/985,006 (52.8%) - ETA: 3.0 min\n",
      "  Progress: 525,000/985,006 (53.3%) - ETA: 3.0 min\n",
      "  Progress: 530,000/985,006 (53.8%) - ETA: 2.9 min\n",
      "  Progress: 535,000/985,006 (54.3%) - ETA: 2.9 min\n",
      "  Progress: 540,000/985,006 (54.8%) - ETA: 2.9 min\n",
      "  Progress: 545,000/985,006 (55.3%) - ETA: 2.8 min\n",
      "  Progress: 550,000/985,006 (55.8%) - ETA: 2.8 min\n",
      "  Progress: 555,000/985,006 (56.3%) - ETA: 2.8 min\n",
      "  Progress: 560,000/985,006 (56.9%) - ETA: 2.7 min\n",
      "  Progress: 565,000/985,006 (57.4%) - ETA: 2.7 min\n",
      "  Progress: 570,000/985,006 (57.9%) - ETA: 2.7 min\n",
      "  Progress: 575,000/985,006 (58.4%) - ETA: 2.6 min\n",
      "  Progress: 580,000/985,006 (58.9%) - ETA: 2.6 min\n",
      "  Progress: 585,000/985,006 (59.4%) - ETA: 2.6 min\n",
      "  Progress: 590,000/985,006 (59.9%) - ETA: 2.5 min\n",
      "  Progress: 595,000/985,006 (60.4%) - ETA: 2.5 min\n",
      "  Progress: 600,000/985,006 (60.9%) - ETA: 2.5 min\n",
      "  Progress: 605,000/985,006 (61.4%) - ETA: 2.5 min\n",
      "  Progress: 610,000/985,006 (61.9%) - ETA: 2.4 min\n",
      "  Progress: 615,000/985,006 (62.4%) - ETA: 2.4 min\n",
      "  Progress: 620,000/985,006 (62.9%) - ETA: 2.4 min\n",
      "  Progress: 625,000/985,006 (63.5%) - ETA: 2.3 min\n",
      "  Progress: 630,000/985,006 (64.0%) - ETA: 2.3 min\n",
      "  Progress: 635,000/985,006 (64.5%) - ETA: 2.3 min\n",
      "  Progress: 640,000/985,006 (65.0%) - ETA: 2.2 min\n",
      "  Progress: 645,000/985,006 (65.5%) - ETA: 2.2 min\n",
      "  Progress: 650,000/985,006 (66.0%) - ETA: 2.2 min\n",
      "  Progress: 655,000/985,006 (66.5%) - ETA: 2.1 min\n",
      "  Progress: 660,000/985,006 (67.0%) - ETA: 2.1 min\n",
      "  Progress: 665,000/985,006 (67.5%) - ETA: 2.1 min\n",
      "  Progress: 670,000/985,006 (68.0%) - ETA: 2.0 min\n",
      "  Progress: 675,000/985,006 (68.5%) - ETA: 2.0 min\n",
      "  Progress: 680,000/985,006 (69.0%) - ETA: 2.0 min\n",
      "  Progress: 685,000/985,006 (69.5%) - ETA: 1.9 min\n",
      "  Progress: 690,000/985,006 (70.1%) - ETA: 1.9 min\n",
      "  Progress: 695,000/985,006 (70.6%) - ETA: 1.9 min\n",
      "  Progress: 700,000/985,006 (71.1%) - ETA: 1.8 min\n",
      "  Progress: 705,000/985,006 (71.6%) - ETA: 1.8 min\n",
      "  Progress: 710,000/985,006 (72.1%) - ETA: 1.8 min\n",
      "  Progress: 715,000/985,006 (72.6%) - ETA: 1.7 min\n",
      "  Progress: 720,000/985,006 (73.1%) - ETA: 1.7 min\n",
      "  Progress: 725,000/985,006 (73.6%) - ETA: 1.7 min\n",
      "  Progress: 730,000/985,006 (74.1%) - ETA: 1.6 min\n",
      "  Progress: 735,000/985,006 (74.6%) - ETA: 1.6 min\n",
      "  Progress: 740,000/985,006 (75.1%) - ETA: 1.6 min\n",
      "  Progress: 745,000/985,006 (75.6%) - ETA: 1.5 min\n",
      "  Progress: 750,000/985,006 (76.1%) - ETA: 1.5 min\n",
      "  Progress: 755,000/985,006 (76.6%) - ETA: 1.5 min\n",
      "  Progress: 760,000/985,006 (77.2%) - ETA: 1.4 min\n",
      "  Progress: 765,000/985,006 (77.7%) - ETA: 1.4 min\n",
      "  Progress: 770,000/985,006 (78.2%) - ETA: 1.4 min\n",
      "  Progress: 775,000/985,006 (78.7%) - ETA: 1.3 min\n",
      "  Progress: 780,000/985,006 (79.2%) - ETA: 1.3 min\n",
      "  Progress: 785,000/985,006 (79.7%) - ETA: 1.3 min\n",
      "  Progress: 790,000/985,006 (80.2%) - ETA: 1.3 min\n",
      "  Progress: 795,000/985,006 (80.7%) - ETA: 1.2 min\n",
      "  Progress: 800,000/985,006 (81.2%) - ETA: 1.2 min\n",
      "  Progress: 805,000/985,006 (81.7%) - ETA: 1.2 min\n",
      "  Progress: 810,000/985,006 (82.2%) - ETA: 1.1 min\n",
      "  Progress: 815,000/985,006 (82.7%) - ETA: 1.1 min\n",
      "  Progress: 820,000/985,006 (83.2%) - ETA: 1.1 min\n",
      "  Progress: 825,000/985,006 (83.8%) - ETA: 1.0 min\n",
      "  Progress: 830,000/985,006 (84.3%) - ETA: 1.0 min\n",
      "  Progress: 835,000/985,006 (84.8%) - ETA: 1.0 min\n",
      "  Progress: 840,000/985,006 (85.3%) - ETA: 0.9 min\n",
      "  Progress: 845,000/985,006 (85.8%) - ETA: 0.9 min\n",
      "  Progress: 850,000/985,006 (86.3%) - ETA: 0.9 min\n",
      "  Progress: 855,000/985,006 (86.8%) - ETA: 0.8 min\n",
      "  Progress: 860,000/985,006 (87.3%) - ETA: 0.8 min\n",
      "  Progress: 865,000/985,006 (87.8%) - ETA: 0.8 min\n",
      "  Progress: 870,000/985,006 (88.3%) - ETA: 0.7 min\n",
      "  Progress: 875,000/985,006 (88.8%) - ETA: 0.7 min\n",
      "  Progress: 880,000/985,006 (89.3%) - ETA: 0.7 min\n",
      "  Progress: 885,000/985,006 (89.8%) - ETA: 0.6 min\n",
      "  Progress: 890,000/985,006 (90.4%) - ETA: 0.6 min\n",
      "  Progress: 895,000/985,006 (90.9%) - ETA: 0.6 min\n",
      "  Progress: 900,000/985,006 (91.4%) - ETA: 0.5 min\n",
      "  Progress: 905,000/985,006 (91.9%) - ETA: 0.5 min\n",
      "  Progress: 910,000/985,006 (92.4%) - ETA: 0.5 min\n",
      "  Progress: 915,000/985,006 (92.9%) - ETA: 0.4 min\n",
      "  Progress: 920,000/985,006 (93.4%) - ETA: 0.4 min\n",
      "  Progress: 925,000/985,006 (93.9%) - ETA: 0.4 min\n",
      "  Progress: 930,000/985,006 (94.4%) - ETA: 0.4 min\n",
      "  Progress: 935,000/985,006 (94.9%) - ETA: 0.3 min\n",
      "  Progress: 940,000/985,006 (95.4%) - ETA: 0.3 min\n",
      "  Progress: 945,000/985,006 (95.9%) - ETA: 0.3 min\n",
      "  Progress: 950,000/985,006 (96.4%) - ETA: 0.2 min\n",
      "  Progress: 955,000/985,006 (97.0%) - ETA: 0.2 min\n",
      "  Progress: 960,000/985,006 (97.5%) - ETA: 0.2 min\n",
      "  Progress: 965,000/985,006 (98.0%) - ETA: 0.1 min\n",
      "  Progress: 970,000/985,006 (98.5%) - ETA: 0.1 min\n",
      "  Progress: 975,000/985,006 (99.0%) - ETA: 0.1 min\n",
      "  Progress: 980,000/985,006 (99.5%) - ETA: 0.0 min\n",
      "  Progress: 985,000/985,006 (100.0%) - ETA: 0.0 min\n",
      "  âœ“ Generated 3,378,080 negative samples\n",
      "\n",
      "[3/4] Converting to DataFrame...\n",
      "\n",
      "[4/4] Adding features to negative samples...\n",
      "\n",
      "================================================================================\n",
      "ADDING FEATURES TO POSITIVE PAIRS\n",
      "================================================================================\n",
      "\n",
      "[1/7] Adding spatial features...\n",
      "  - Calculating haversine distances...\n",
      "  - Creating distance buckets...\n",
      "  - Calculating directions...\n",
      "  âœ“ Added spatial features\n",
      "\n",
      "[2/7] Adding temporal features...\n",
      "  - Creating time delta buckets...\n",
      "  - Extracting hour and day of week...\n",
      "  - Identifying meal times...\n",
      "  âœ“ Added temporal features\n",
      "\n",
      "[3/7] Adding quality features...\n",
      "  âœ“ Added quality features\n",
      "\n",
      "[4/7] Adding price features...\n",
      "  - Joining with business data for prices...\n",
      "  âœ“ Added price features\n",
      "\n",
      "[5/7] Adding category features...\n",
      "  âœ“ Added category features\n",
      "\n",
      "[6/7] Adding relationship features...\n",
      "  - Joining with business data for relative_results...\n",
      "  - Checking relative_results membership...\n",
      "  âœ“ Added relationship features\n",
      "\n",
      "[10/14] Adding cuisine complementarity features...\n",
      "  âœ“ Added cuisine complementarity features\n",
      "\n",
      "[11/14] Adding operating hours features...\n",
      "  âœ“ Added operating hours features\n",
      "\n",
      "[13/14] Adding service options features...\n",
      "  âœ“ Added service options features\n",
      "\n",
      "[14/14] Feature engineering complete!\n",
      "  Total features: 73\n",
      "  Total positive pairs: 3,378,080\n",
      "\n",
      "[8/14] Adding review sentiment features...\n",
      "  âœ“ Added sentiment features\n",
      "\n",
      "[9/14] Adding user behavioral features...\n",
      "  âœ“ Added user behavioral features\n",
      "\n",
      "[12/14] Adding review topic features...\n",
      "  âœ“ Added review topic features\n",
      "\n",
      "âœ“ Negative sampling complete!\n",
      "  Total negative pairs: 3,378,080\n",
      "  Negative:Positive ratio: 3.4:1\n",
      "\n",
      "[3/3] Combining positive and negative samples...\n",
      "\n",
      "Combining positive and negative samples...\n",
      "\n",
      "âœ… Fixed features saved:\n",
      "  Total pairs: 4,363,086\n",
      "  Positive: 985,006\n",
      "  Negative: 3,378,080\n",
      "  File: /Volumes/SunnySSD/Forkast_processed/ga/features_ga_fixed.parquet\n",
      "  Size: 640.9 MB\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Regenerate features with fixed negative sampling\n",
    "print(\"=\" * 80)\n",
    "print(\"REGENERATING FEATURES WITH FIXED NEGATIVE SAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload the updated modules\n",
    "import importlib\n",
    "import data.features\n",
    "importlib.reload(data.features)\n",
    "from data.features import add_all_features, generate_negative_samples\n",
    "\n",
    "# Load the filtered pairs (before feature engineering)\n",
    "pairs_df = pl.read_parquet(PROCESSED_DIR / \"pairs_filtered_ga.parquet\")\n",
    "biz_df = pl.read_parquet(BIZ_PARQUET)\n",
    "reviews_df = pl.read_parquet(REVIEWS_PARQUET)\n",
    "\n",
    "print(f\"\\nLoaded data:\")\n",
    "print(f\"  Pairs: {len(pairs_df):,}\")\n",
    "print(f\"  Businesses: {len(biz_df):,}\")\n",
    "print(f\"  Reviews: {len(reviews_df):,}\")\n",
    "\n",
    "# Add features to positive pairs with caching\n",
    "print(f\"\\n[1/3] Adding features to positive pairs...\")\n",
    "print(f\"  Using cache directory: {CACHE_DIR}\")\n",
    "\n",
    "analysis_context = {}\n",
    "user_profile_cache = CACHE_DIR / \"user_profiles.parquet\"\n",
    "\n",
    "pairs_with_features = add_all_features(\n",
    "    pairs_df, \n",
    "    biz_df, \n",
    "    reviews_df,\n",
    "    review_cache_path=CACHE_DIR,\n",
    "    user_profile_cache_path=user_profile_cache,\n",
    "    max_reviews_per_biz=200,\n",
    "    analysis_context=analysis_context\n",
    ")\n",
    "\n",
    "# Generate negative samples with FIXED timestamps, reusing cached analysis\n",
    "print(f\"\\n[2/3] Generating negative samples with realistic timestamps...\")\n",
    "negative_pairs = generate_negative_samples(\n",
    "    pairs_with_features, \n",
    "    biz_df, \n",
    "    n_negatives=4, \n",
    "    random_seed=42,\n",
    "    sentiment_df=analysis_context.get(\"sentiment_df\"),\n",
    "    topics_df=analysis_context.get(\"topics_df\"),\n",
    "    user_profiles=analysis_context.get(\"user_profiles_df\"),\n",
    "    reviews_df=analysis_context.get(\"reviews_subset_df\"),\n",
    "    review_cache_path=CACHE_DIR,\n",
    "    user_profile_cache_path=user_profile_cache,\n",
    "    max_reviews_per_biz=200\n",
    ")\n",
    "\n",
    "# Features are already added during negative sample generation when cached data is provided\n",
    "print(f\"\\n[3/3] Combining positive and negative samples...\")\n",
    "\n",
    "# Combine and save\n",
    "print(f\"\\nCombining positive and negative samples...\")\n",
    "# Ensure schema compatibility\n",
    "target_schema = pairs_with_features.schema\n",
    "target_columns = set(target_schema.keys())\n",
    "\n",
    "# Select only columns that exist in target schema\n",
    "columns_to_select = [col for col in negative_pairs.columns if col in target_columns]\n",
    "negative_pairs = negative_pairs.select(columns_to_select)\n",
    "\n",
    "# Ensure all target columns exist and have correct types\n",
    "cast_exprs = []\n",
    "for col_name, dtype in target_schema.items():\n",
    "    if col_name in negative_pairs.columns:\n",
    "        if negative_pairs[col_name].dtype != dtype:\n",
    "            cast_exprs.append(pl.col(col_name).cast(dtype))\n",
    "        else:\n",
    "            cast_exprs.append(pl.col(col_name))\n",
    "    else:\n",
    "        # Fill missing columns with defaults\n",
    "        if dtype == pl.Boolean:\n",
    "            cast_exprs.append(pl.lit(False).cast(dtype).alias(col_name))\n",
    "        elif dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:\n",
    "            cast_exprs.append(pl.lit(0).cast(dtype).alias(col_name))\n",
    "        elif dtype in [pl.Float32, pl.Float64]:\n",
    "            cast_exprs.append(pl.lit(0.0).cast(dtype).alias(col_name))\n",
    "        else:\n",
    "            cast_exprs.append(pl.lit(None).cast(dtype).alias(col_name))\n",
    "\n",
    "if cast_exprs:\n",
    "    negative_pairs = negative_pairs.with_columns(cast_exprs)\n",
    "\n",
    "# Ensure column order matches target schema\n",
    "negative_pairs = negative_pairs.select(list(target_schema.keys()))\n",
    "\n",
    "# Combine\n",
    "all_features_df = pl.concat([pairs_with_features, negative_pairs])\n",
    "\n",
    "# Save updated features\n",
    "features_output = PROCESSED_DIR / \"features_ga_fixed.parquet\"\n",
    "all_features_df.write_parquet(features_output, compression=\"snappy\")\n",
    "\n",
    "print(f\"\\n Fixed features saved:\")\n",
    "print(f\"  Total pairs: {len(all_features_df):,}\")\n",
    "print(f\"  Positive: {len(pairs_with_features):,}\")\n",
    "print(f\"  Negative: {len(negative_pairs):,}\")\n",
    "print(f\"  File: {features_output}\")\n",
    "print(f\"  Size: {features_output.stat().st_size / 1024 / 1024:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Geographic splitting (Non-Atlanta train â†’ Atlanta test)\n",
    "print(\"=\" * 80)\n",
    "print(\"GEOGRAPHIC DATA SPLITTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload the updated split_data module\n",
    "import data.split_data\n",
    "importlib.reload(data.split_data)\n",
    "from data.split_data import split_xgboost_geographic\n",
    "\n",
    "# Use the fixed features file\n",
    "import shutil\n",
    "fixed_features = PROCESSED_DIR / \"features_ga_fixed.parquet\"\n",
    "original_features = PROCESSED_DIR / \"features_ga.parquet\"\n",
    "\n",
    "# Backup original and use fixed version\n",
    "if original_features.exists():\n",
    "    shutil.copy(original_features, PROCESSED_DIR / \"features_ga_backup.parquet\")\n",
    "    print(\"  âœ“ Backed up original features\")\n",
    "\n",
    "shutil.copy(fixed_features, original_features)\n",
    "print(\"  âœ“ Using fixed features for splitting\")\n",
    "\n",
    "# Run geographic splitting with explicit path\n",
    "# This ensures it uses the same PROCESSED_DIR as configured in setup\n",
    "print(f\"\\nUsing processed directory: {PROCESSED_DIR}\")\n",
    "train_df, val_df, test_df = split_xgboost_geographic(processed_dir=PROCESSED_DIR)\n",
    "\n",
    "print(f\"\\n Geographic split complete:\")\n",
    "print(f\"  Train (non-Atlanta): {len(train_df):,} pairs\")\n",
    "print(f\"  Val (non-Atlanta): {len(val_df):,} pairs\")\n",
    "print(f\"  Test (Atlanta): {len(test_df):,} pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Retrain model with fixed data\n",
    "print(\"=\" * 80)\n",
    "print(\"RETRAINING XGBOOST MODEL WITH FIXES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload the updated XGBoost module\n",
    "import models.xgboost_ranker\n",
    "importlib.reload(models.xgboost_ranker)\n",
    "from models.xgboost_ranker import (\n",
    "    load_data, prepare_features, train_model, predict_ranking, \n",
    "    evaluate_ranking, analyze_feature_importance\n",
    ")\n",
    "\n",
    "# Load the geographically split data\n",
    "data_dir = PROCESSED_DIR / \"xgboost_data\"\n",
    "train_df, val_df, test_df = load_data(data_dir)\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Train: {len(train_df):,} pairs\")\n",
    "print(f\"  Validation: {len(val_df):,} pairs\")\n",
    "print(f\"  Test: {len(test_df):,} pairs\")\n",
    "\n",
    "# Prepare features\n",
    "print(f\"\\n[1/4] Preparing features...\")\n",
    "X_train, y_train, group_train, feature_names = prepare_features(train_df)\n",
    "X_val, y_val, group_val, _ = prepare_features(val_df)\n",
    "X_test, y_test, group_test, _ = prepare_features(test_df)\n",
    "\n",
    "print(f\"  Train shape: {X_train.shape}\")\n",
    "print(f\"  Validation shape: {X_val.shape}\")\n",
    "print(f\"  Test shape: {X_test.shape}\")\n",
    "\n",
    "# Train model\n",
    "print(f\"\\n[2/4] Training model...\")\n",
    "model = train_model(X_train, y_train, group_train, X_val, y_val, group_val)\n",
    "\n",
    "# Generate predictions\n",
    "print(f\"\\n[3/4] Generating predictions...\")\n",
    "predictions_df = predict_ranking(model, test_df, top_k=10)\n",
    "\n",
    "# Evaluate model (nDCG disabled by default to prevent kernel crashes)\n",
    "print(f\"\\n[4/4] Evaluating model...\")\n",
    "metrics = evaluate_ranking(predictions_df, k_values=[1, 5, 10], compute_ndcg=False)\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\" FINAL MODEL PERFORMANCE\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"  Recall@1:  {metrics.get('recall@1', 0):.4f}\")\n",
    "print(f\"  Recall@5:  {metrics.get('recall@5', 0):.4f}\")\n",
    "print(f\"  Recall@10: {metrics.get('recall@10', 0):.4f}\")\n",
    "print(f\"  MRR:       {metrics.get('mrr', 0):.4f}\")\n",
    "\n",
    "# Validate performance is realistic (not data leakage)\n",
    "if metrics.get('recall@10', 0) < 0.9:\n",
    "    print(f\"\\n Performance metrics are realistic - no data leakage detected\")\n",
    "else:\n",
    "    print(f\"\\n  Very high performance detected - possible data leakage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GEOGRAPHIC DATA SPLITTING\n",
      "================================================================================\n",
      "  âœ“ Backed up original features\n",
      "  âœ“ Using fixed features for splitting\n",
      "\n",
      "Using processed directory: /Volumes/SunnySSD/Forkast_processed/ga\n",
      "================================================================================\n",
      "PHASE A4: GEOGRAPHIC DATA SPLITTING (NON-ATLANTA â†’ ATLANTA)\n",
      "================================================================================\n",
      "\n",
      "Inputs:\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/features_ga.parquet\n",
      "  - /Volumes/SunnySSD/Forkast_processed/ga/biz_ga.parquet\n",
      "\n",
      "Creating output directories...\n",
      "  âœ“ /Volumes/SunnySSD/Forkast_processed/ga/xgboost_data\n",
      "\n",
      "[Loading data...]\n",
      "  XGBoost features: 4,363,086\n",
      "  Businesses: 27,710\n",
      "\n",
      "================================================================================\n",
      "GEOGRAPHIC SPLIT: NON-ATLANTA TRAIN â†’ ATLANTA TEST\n",
      "================================================================================\n",
      "\n",
      "[1/5] Identifying Atlanta businesses...\n",
      "  Atlanta businesses: 6,887\n",
      "  Total businesses: 27,710\n",
      "  Atlanta percentage: 24.9%\n",
      "\n",
      "[2/5] Splitting data geographically...\n",
      "  Non-Atlanta samples (train/val): 3,349,419\n",
      "  Atlanta samples (test): 1,013,667\n",
      "\n",
      "[3/5] Temporal split of non-Atlanta data...\n",
      "  Train samples: 2,758,345\n",
      "  Val samples: 591,074\n",
      "  Test samples: 1,013,667\n",
      "\n",
      "[4/5] Temporal ranges:\n",
      "  Train: 2005-12-09 00:00:00 to 2020-07-13 22:35:51\n",
      "  Val:   2020-07-13 22:37:48 to 2021-08-09 17:42:28\n",
      "  Test:  2005-12-09 00:00:00 to 2021-09-04 12:21:12\n",
      "\n",
      "[5/5] Label distribution:\n",
      "  Train - Pos: 655,825, Neg: 2,102,520\n",
      "  Val   - Pos: 143,977, Neg: 447,097\n",
      "  Test  - Pos: 185,204, Neg: 828,463\n",
      "\n",
      "================================================================================\n",
      "SAVING XGBOOST SPLITS\n",
      "================================================================================\n",
      "\n",
      "[1/3] Saving training data...\n",
      "  /Volumes/SunnySSD/Forkast_processed/ga/xgboost_data/train.parquet\n",
      "  Size: 409.1 MB\n",
      "\n",
      "[2/3] Saving validation data...\n",
      "  /Volumes/SunnySSD/Forkast_processed/ga/xgboost_data/val.parquet\n",
      "  Size: 89.3 MB\n",
      "\n",
      "[3/3] Saving test data...\n",
      "  /Volumes/SunnySSD/Forkast_processed/ga/xgboost_data/test.parquet\n",
      "  Size: 144.9 MB\n",
      "\n",
      "âœ“ XGBoost splits saved to /Volumes/SunnySSD/Forkast_processed/ga/xgboost_data\n",
      "\n",
      "================================================================================\n",
      "COPYING BUSINESS METADATA\n",
      "================================================================================\n",
      "\n",
      "[1/1] Copying to xgboost_data/...\n",
      "  /Volumes/SunnySSD/Forkast_processed/ga/xgboost_data/biz_ga.parquet\n",
      "  Size: 5.4 MB\n",
      "\n",
      "âœ“ Business metadata copied to xgboost_data/\n",
      "\n",
      "âœ“âœ“âœ“ GEOGRAPHIC SPLIT COMPLETE âœ“âœ“âœ“\n",
      "\n",
      "Summary:\n",
      "  Train (non-Atlanta): 2,758,345 samples\n",
      "  Val (non-Atlanta): 591,074 samples\n",
      "  Test (Atlanta): 1,013,667 samples\n",
      "\n",
      "âœ… Geographic split complete:\n",
      "  Train (non-Atlanta): 2,758,345 pairs\n",
      "  Val (non-Atlanta): 591,074 pairs\n",
      "  Test (Atlanta): 1,013,667 pairs\n",
      "\n",
      "Validation:\n",
      "  Test businesses in Atlanta bounds: 6,887\n",
      "  Total test businesses: 6,887\n",
      "  Atlanta percentage in test: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Geographic splitting (Non-Atlanta train â†’ Atlanta test)\n",
    "print(\"=\" * 80)\n",
    "print(\"GEOGRAPHIC DATA SPLITTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload the updated split_data module\n",
    "import data.split_data\n",
    "importlib.reload(data.split_data)\n",
    "from data.split_data import split_xgboost_geographic\n",
    "\n",
    "# Use the fixed features file\n",
    "import shutil\n",
    "fixed_features = PROCESSED_DIR / \"features_ga_fixed.parquet\"\n",
    "original_features = PROCESSED_DIR / \"features_ga.parquet\"\n",
    "\n",
    "# Backup original and use fixed version\n",
    "if original_features.exists():\n",
    "    shutil.copy(original_features, PROCESSED_DIR / \"features_ga_backup.parquet\")\n",
    "    print(\"  âœ“ Backed up original features\")\n",
    "\n",
    "shutil.copy(fixed_features, original_features)\n",
    "print(\"  âœ“ Using fixed features for splitting\")\n",
    "\n",
    "# Run geographic splitting with explicit path\n",
    "# This ensures it uses the same PROCESSED_DIR as configured in setup\n",
    "print(f\"\\nUsing processed directory: {PROCESSED_DIR}\")\n",
    "train_df, val_df, test_df = split_xgboost_geographic(processed_dir=PROCESSED_DIR)\n",
    "\n",
    "print(f\"\\n Geographic split complete:\")\n",
    "print(f\"  Train (non-Atlanta): {len(train_df):,} pairs\")\n",
    "print(f\"  Val (non-Atlanta): {len(val_df):,} pairs\")\n",
    "print(f\"  Test (Atlanta): {len(test_df):,} pairs\")\n",
    "\n",
    "# Verify the split worked correctly\n",
    "atlanta_bounds = {\n",
    "    'lat_min': 33.6, 'lat_max': 34.0,\n",
    "    'lon_min': -84.6, 'lon_max': -84.2\n",
    "}\n",
    "\n",
    "# Check test set is Atlanta-only\n",
    "test_biz_ids = set(test_df['src_gmap_id'].to_list() + test_df['dst_gmap_id'].to_list())\n",
    "atlanta_test_biz = biz_df.filter(\n",
    "    pl.col('gmap_id').is_in(list(test_biz_ids)) &\n",
    "    (pl.col('lat') >= atlanta_bounds['lat_min']) &\n",
    "    (pl.col('lat') <= atlanta_bounds['lat_max']) &\n",
    "    (pl.col('lon') >= atlanta_bounds['lon_min']) &\n",
    "    (pl.col('lon') <= atlanta_bounds['lon_max'])\n",
    ")\n",
    "\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Test businesses in Atlanta bounds: {len(atlanta_test_biz):,}\")\n",
    "print(f\"  Total test businesses: {len(test_biz_ids):,}\")\n",
    "print(f\"  Atlanta percentage in test: {len(atlanta_test_biz)/len(test_biz_ids)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase B1: XGBoost Ranking Model Training\n",
    "\n",
    "**What this does:**\n",
    "- Loads split data and prepares features\n",
    "- Trains XGBoost ranking model with `rank:pairwise` objective\n",
    "- Evaluates using Recall@K, MRR, nDCG@K\n",
    "- Generates feature importance analysis\n",
    "- Creates transition probability matrix\n",
    "\n",
    "**Expected time:** 20-40 minutes (depending on data size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Predictions for Visualization\n",
    "\n",
    "**What this does:**\n",
    "- Takes the generated XGBoost predictions\n",
    "- Adds source and destination business metadata\n",
    "- Formats columns to match LSTM prediction format exactly\n",
    "- Exports to `outputs/atlanta_xgboost_predictions_with_meta.csv`\n",
    "\n",
    "This ensures the visualization API can load XGBoost predictions with the same format as LSTM predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE B1: XGBOOST RANKING MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading training data...\n",
      "  âœ“ 2,758,345 samples\n",
      "  Positive: 655,825\n",
      "  Negative: 2,102,520\n",
      "\n",
      "[2/3] Loading validation data...\n",
      "  âœ“ 591,074 samples\n",
      "  Positive: 143,977\n",
      "  Negative: 447,097\n",
      "\n",
      "[3/3] Loading test data...\n",
      "  âœ“ 1,013,667 samples\n",
      "  Positive: 185,204\n",
      "  Negative: 828,463\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Train: 2,758,345 pairs\n",
      "  Validation: 591,074 pairs\n",
      "  Test: 1,013,667 pairs\n"
     ]
    }
   ],
   "source": [
    "# Reload the module to get the latest changes\n",
    "import importlib\n",
    "import models.xgboost_ranker\n",
    "importlib.reload(models.xgboost_ranker)\n",
    "from models.xgboost_ranker import (\n",
    "    load_data, prepare_features, train_model, predict_ranking, \n",
    "    evaluate_ranking, analyze_feature_importance, save_model_and_results,\n",
    "    CATEGORICAL_FEATURES, NUMERICAL_FEATURES, BOOLEAN_FEATURES\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE B1: XGBOOST RANKING MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data\n",
    "data_dir = PROCESSED_DIR / \"xgboost_data\"\n",
    "train_df, val_df, test_df = load_data(data_dir)\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Train: {len(train_df):,} pairs\")\n",
    "print(f\"  Validation: {len(val_df):,} pairs\")\n",
    "print(f\"  Test: {len(test_df):,} pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 1/5] Preparing features...\n",
      "\n",
      "[Preparing ranking data with interaction features...]\n",
      "\n",
      "  ğŸ” DIAGNOSTIC: Columns before create_interaction_features: 122\n",
      "  ğŸ”— Creating interaction features...\n",
      "    Added 17 interaction features\n",
      "  ğŸ” DIAGNOSTIC: Columns after create_interaction_features: 139\n",
      "  ğŸ” DIAGNOSTIC: Found 17 interaction columns\n",
      "  ğŸ” DIAGNOSTIC: Examples: ['distance_x_dst_rating', 'distance_x_rating_diff', 'price_x_rating', 'time_x_distance', 'user_pref_x_dst_rating']\n",
      "      distance_x_dst_rating: mean=208.8553, std=370.2505, range=[0.0000, 2812.3706]\n",
      "      distance_x_rating_diff: mean=2.6571, std=110.9482, range=[-2181.6138, 2203.1377]\n",
      "      price_x_rating: mean=4.4586, std=3.0904, range=[0.0000, 20.0000]\n",
      "      time_x_distance: mean=4474.6071, std=9703.8722, range=[0.0000, 91739.1388]\n",
      "      user_pref_x_dst_rating: mean=15.9641, std=4.3548, range=[1.0000, 25.0000]\n",
      "  - One-hot encoding categorical features...\n",
      "  - Total features after encoding: 146\n",
      "    Base features: 100\n",
      "    Interaction features: 17\n",
      "    One-hot encoded: 29\n",
      "  Features: (2758345, 146)\n",
      "  Labels: (2758345,)\n",
      "  Number of queries (source visits): 778,920\n",
      "  Avg candidates per query: 3.5\n",
      "  Total samples: 2,758,345\n",
      "\n",
      "[Preparing ranking data with interaction features...]\n",
      "\n",
      "  ğŸ” DIAGNOSTIC: Columns before create_interaction_features: 122\n",
      "  ğŸ”— Creating interaction features...\n",
      "    Added 17 interaction features\n",
      "  ğŸ” DIAGNOSTIC: Columns after create_interaction_features: 139\n",
      "  ğŸ” DIAGNOSTIC: Found 17 interaction columns\n",
      "  ğŸ” DIAGNOSTIC: Examples: ['distance_x_dst_rating', 'distance_x_rating_diff', 'price_x_rating', 'time_x_distance', 'user_pref_x_dst_rating']\n",
      "      distance_x_dst_rating: mean=210.1210, std=371.5961, range=[0.0000, 2833.6375]\n",
      "      distance_x_rating_diff: mean=2.6960, std=115.2278, range=[-1839.0477, 2027.3162]\n",
      "      price_x_rating: mean=4.3496, std=3.0987, range=[0.0000, 20.0000]\n",
      "      time_x_distance: mean=4548.9588, std=9752.0045, range=[0.0000, 93916.9117]\n",
      "      user_pref_x_dst_rating: mean=16.1691, std=4.3560, range=[1.0000, 25.0000]\n",
      "  - One-hot encoding categorical features...\n",
      "  - Total features after encoding: 146\n",
      "    Base features: 100\n",
      "    Interaction features: 17\n",
      "    One-hot encoded: 29\n",
      "  Features: (591074, 146)\n",
      "  Labels: (591074,)\n",
      "  Number of queries (source visits): 165,598\n",
      "  Avg candidates per query: 3.6\n",
      "  Total samples: 591,074\n",
      "\n",
      "[Preparing ranking data with interaction features...]\n",
      "\n",
      "  ğŸ” DIAGNOSTIC: Columns before create_interaction_features: 122\n",
      "  ğŸ”— Creating interaction features...\n",
      "    Added 17 interaction features\n",
      "  ğŸ” DIAGNOSTIC: Columns after create_interaction_features: 139\n",
      "  ğŸ” DIAGNOSTIC: Found 17 interaction columns\n",
      "  ğŸ” DIAGNOSTIC: Examples: ['distance_x_dst_rating', 'distance_x_rating_diff', 'price_x_rating', 'time_x_distance', 'user_pref_x_dst_rating']\n",
      "      distance_x_dst_rating: mean=25.7900, std=24.3645, range=[0.0000, 245.1220]\n",
      "      distance_x_rating_diff: mean=0.3161, std=10.2645, range=[-178.9119, 177.7584]\n",
      "      price_x_rating: mean=4.6773, std=3.5724, range=[0.0000, 20.0000]\n",
      "      time_x_distance: mean=527.5018, std=631.2673, range=[0.0000, 7876.2463]\n",
      "      user_pref_x_dst_rating: mean=15.9626, std=4.1907, range=[1.0000, 25.0000]\n",
      "  - One-hot encoding categorical features...\n",
      "  - Total features after encoding: 146\n",
      "    Base features: 100\n",
      "    Interaction features: 17\n",
      "    One-hot encoded: 29\n",
      "  Features: (1013667, 146)\n",
      "  Labels: (1013667,)\n",
      "  Number of queries (source visits): 278,086\n",
      "  Avg candidates per query: 3.6\n",
      "  Total samples: 1,013,667\n",
      "\n",
      "âœ“ Features prepared:\n",
      "  Total features: 146\n",
      "    - Categorical: 6\n",
      "    - Numerical: 48\n",
      "    - Boolean: 52\n",
      "  Train shape: (2758345, 146)\n",
      "  Validation shape: (591074, 146)\n",
      "  Test shape: (1013667, 146)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features\n",
    "print(\"\\n[Step 1/5] Preparing features...\")\n",
    "\n",
    "X_train, y_train, group_train, feature_names = prepare_features(train_df)\n",
    "X_val, y_val, group_val, _ = prepare_features(val_df)\n",
    "X_test, y_test, group_test, _ = prepare_features(test_df)\n",
    "\n",
    "print(f\"\\nâœ“ Features prepared:\")\n",
    "print(f\"  Total features: {len(feature_names)}\")\n",
    "print(f\"    - Categorical: {len(CATEGORICAL_FEATURES)}\")\n",
    "print(f\"    - Numerical: {len(NUMERICAL_FEATURES)}\")\n",
    "print(f\"    - Boolean: {len(BOOLEAN_FEATURES)}\")\n",
    "print(f\"  Train shape: {X_train.shape}\")\n",
    "print(f\"  Validation shape: {X_val.shape}\")\n",
    "print(f\"  Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample feature names (showing enhanced features):\n",
      "\n",
      "  New Sentiment Features:\n",
      "    - src_sentiment_score\n",
      "    - dst_sentiment_score\n",
      "    - sentiment_transition\n",
      "    - sentiment_similarity\n",
      "    - is_sentiment_upgrade\n",
      "\n",
      "  New User Behavioral Features:\n",
      "    - user_avg_rating\n",
      "    - user_rating_std\n",
      "    - user_total_reviews\n",
      "    - user_unique_restaurants\n",
      "    - user_visit_frequency\n",
      "\n",
      "  New Operating Hours Features:\n",
      "    - delta_hours\n",
      "    - src_is_open_at_time\n",
      "    - dst_is_open_at_time\n",
      "    - hours_overlap\n",
      "    - delta_hours_squared\n"
     ]
    }
   ],
   "source": [
    "# Show sample of feature names\n",
    "print(\"\\nSample feature names (showing enhanced features):\")\n",
    "print(\"\\n  New Sentiment Features:\")\n",
    "sentiment_features = [f for f in feature_names if 'sentiment' in f.lower()]\n",
    "for f in sentiment_features[:5]:\n",
    "    print(f\"    - {f}\")\n",
    "\n",
    "print(\"\\n  New User Behavioral Features:\")\n",
    "user_features = [f for f in feature_names if f.startswith('user_')]\n",
    "for f in user_features[:5]:\n",
    "    print(f\"    - {f}\")\n",
    "\n",
    "print(\"\\n  New Operating Hours Features:\")\n",
    "hours_features = [f for f in feature_names if 'hours' in f.lower() or 'open' in f.lower()]\n",
    "for f in hours_features[:5]:\n",
    "    print(f\"    - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\n[Step 2/5] Training XGBoost ranking model...\")\n",
    "print(\"  This may take 20-40 minutes depending on data size...\")\n",
    "\n",
    "model = train_model(\n",
    "    X_train, y_train, group_train,\n",
    "    X_val, y_val, group_val\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"\\n[Step 3/5] Generating predictions...\")\n",
    "predictions_df = predict_ranking(model, test_df, top_k=10)\n",
    "\n",
    "print(f\"\\nâœ“ Predictions generated:\")\n",
    "print(f\"  Total predictions: {len(predictions_df):,}\")\n",
    "print(f\"  Unique source restaurants: {predictions_df['src_gmap_id'].n_unique():,}\")\n",
    "print(f\"  Average predictions per source: {len(predictions_df)/predictions_df['src_gmap_id'].n_unique():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions to CSV format compatible with visualization API\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORTING PREDICTIONS FOR VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[1/8] Renaming gmap_id columns...\")\n",
    "# First, rename src_gmap_id and dst_gmap_id to match LSTM format\n",
    "predictions_with_meta = predictions_df.rename({\n",
    "    \"src_gmap_id\": \"source_gmap_id\",\n",
    "    \"dst_gmap_id\": \"dest_gmap_id\",\n",
    "    \"distance_km\": \"link_distance_km\",\n",
    "    \"same_category\": \"same_main_category\"\n",
    "})\n",
    "\n",
    "print(\"[2/8] Adding prediction_id and row index...\")\n",
    "# Add prediction_id\n",
    "predictions_with_meta = predictions_with_meta.with_row_index(\"prediction_id\")\n",
    "\n",
    "print(\"[3/8] Creating business indices...\")\n",
    "# Create business indices from gmap_id hashes for compatibility\n",
    "# Use modulo to keep within Int32 range, or use Int64\n",
    "predictions_with_meta = predictions_with_meta.with_columns([\n",
    "    (pl.col(\"source_gmap_id\").hash() % (2**31 - 1)).cast(pl.Int32()).alias(\"source_business_idx\"),\n",
    "    (pl.col(\"dest_gmap_id\").hash() % (2**31 - 1)).cast(pl.Int32()).alias(\"dest_business_idx\"),\n",
    "])\n",
    "\n",
    "# Add score columns if they don't exist (for compatibility with test data)\n",
    "if \"score_raw\" not in predictions_with_meta.columns:\n",
    "    print(\"[4/8] Computing normalized scores...\")\n",
    "    # Use absolute values for proper normalization (avoid negative percentages)\n",
    "    predictions_with_meta = predictions_with_meta.with_columns([\n",
    "        pl.col(\"score\").alias(\"score_raw\"),\n",
    "        (pl.col(\"score\").abs() / pl.col(\"score\").abs().sum().over(\"source_gmap_id\")).alias(\"score_share\"),\n",
    "        (pl.col(\"rank\").cast(pl.Float64) / pl.count().over(\"source_gmap_id\")).alias(\"score_cum_share\"),\n",
    "        pl.lit(None).cast(pl.Float64()).alias(\"score_z\"),\n",
    "    ])\n",
    "\n",
    "print(\"[5/8] Adding source business metadata...\")\n",
    "# Join source business metadata\n",
    "source_meta = biz_df.select([\n",
    "    pl.col(\"gmap_id\"),\n",
    "    pl.col(\"name\").alias(\"source_name\"),\n",
    "    pl.col(\"lat\").alias(\"source_lat\"),\n",
    "    pl.col(\"lon\").alias(\"source_lon\"),\n",
    "    pl.col(\"category_main\").alias(\"source_category_main\"),\n",
    "    pl.col(\"category_all\").list.join(\"|\").alias(\"source_category_all\"),\n",
    "    pl.col(\"avg_rating\").alias(\"source_avg_rating\"),\n",
    "    pl.col(\"num_reviews\").alias(\"source_num_reviews\"),\n",
    "    pl.col(\"price_bucket\").alias(\"source_price_bucket\"),\n",
    "    pl.col(\"is_closed\").alias(\"source_is_closed\"),\n",
    "])\n",
    "\n",
    "predictions_with_meta = predictions_with_meta.join(\n",
    "    source_meta,\n",
    "    left_on=\"source_gmap_id\",\n",
    "    right_on=\"gmap_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"[6/8] Adding destination business metadata...\")\n",
    "# Join destination business metadata\n",
    "dest_meta = biz_df.select([\n",
    "    pl.col(\"gmap_id\"),\n",
    "    pl.col(\"name\").alias(\"dest_name\"),\n",
    "    pl.col(\"lat\").alias(\"dest_lat\"),\n",
    "    pl.col(\"lon\").alias(\"dest_lon\"),\n",
    "    pl.col(\"category_main\").alias(\"dest_category_main\"),\n",
    "    pl.col(\"category_all\").list.join(\"|\").alias(\"dest_category_all\"),\n",
    "    pl.col(\"avg_rating\").alias(\"dest_avg_rating\"),\n",
    "    pl.col(\"num_reviews\").alias(\"dest_num_reviews\"),\n",
    "    pl.col(\"price_bucket\").alias(\"dest_price_bucket\"),\n",
    "    pl.col(\"is_closed\").alias(\"dest_is_closed\"),\n",
    "])\n",
    "\n",
    "predictions_with_meta = predictions_with_meta.join(\n",
    "    dest_meta,\n",
    "    left_on=\"dest_gmap_id\",\n",
    "    right_on=\"gmap_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"[7/8] Selecting columns...\")\n",
    "# Select columns in the expected order (matching LSTM format)\n",
    "final_columns = [\n",
    "    \"prediction_id\",\n",
    "    \"source_business_idx\",\n",
    "    \"source_gmap_id\",\n",
    "    \"source_name\",\n",
    "    \"source_lat\",\n",
    "    \"source_lon\",\n",
    "    \"source_category_main\",\n",
    "    \"source_category_all\",\n",
    "    \"source_avg_rating\",\n",
    "    \"source_num_reviews\",\n",
    "    \"source_price_bucket\",\n",
    "    \"source_is_closed\",\n",
    "    \"rank\",\n",
    "    \"score_raw\",\n",
    "    \"score_share\",\n",
    "    \"score_cum_share\",\n",
    "    \"score_z\",\n",
    "    \"dest_business_idx\",\n",
    "    \"dest_gmap_id\",\n",
    "    \"dest_name\",\n",
    "    \"dest_lat\",\n",
    "    \"dest_lon\",\n",
    "    \"dest_category_main\",\n",
    "    \"dest_category_all\",\n",
    "    \"dest_avg_rating\",\n",
    "    \"dest_num_reviews\",\n",
    "    \"dest_price_bucket\",\n",
    "    \"dest_is_closed\",\n",
    "    \"link_distance_km\",\n",
    "    \"same_main_category\",\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "existing_cols = [col for col in final_columns if col in predictions_with_meta.columns]\n",
    "predictions_export = predictions_with_meta.select(existing_cols)\n",
    "\n",
    "print(\"[8/8] Saving to CSV...\")\n",
    "# Save to outputs directory\n",
    "output_path = base_dir / \"outputs\" / \"atlanta_xgboost_predictions_with_meta.csv\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "predictions_export.write_csv(output_path)\n",
    "\n",
    "print(f\"\\n Exported predictions successfully!\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Total predictions: {len(predictions_export):,}\")\n",
    "print(f\"  Unique sources: {predictions_export['source_business_idx'].n_unique():,}\")\n",
    "print(f\"  Unique destinations: {predictions_export['dest_business_idx'].n_unique():,}\")\n",
    "print(f\"  Columns: {len(predictions_export.columns)}\")\n",
    "print(f\"\\nColumns included: {existing_cols}\")\n",
    "print(f\"\\nğŸ’¡ This file is now ready to be used by the visualization API!\")\n",
    "print(f\"   The format matches LSTM predictions exactly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"\\n[Step 4/5] Evaluating model performance...\")\n",
    "metrics = evaluate_ranking(predictions_df, k_values=[1, 5, 10])\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete:\")\n",
    "print(\"\\n  Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"    {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n  Interpretation:\")\n",
    "print(f\"    - Recall@10: {metrics.get('recall@10', 0):.1%} of actual next visits are in top-10 predictions\")\n",
    "print(f\"    - MRR: {metrics.get('mrr', 0):.2f} average rank of first correct prediction\")\n",
    "print(f\"    - nDCG@10: {metrics.get('ndcg@10', 0):.4f} ranking quality (higher is better)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 5/5] Analyzing feature importance...\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top 20 Most Important Features:\n",
      "   1. cuisine_pair_type_similar                6117.7104\n",
      "   2. distance_bucket_20+km                    3751.9937\n",
      "   3. dst_rating                               2805.7986\n",
      "   4. same_category                            2609.3408\n",
      "   5. cuisine_pair_type_diverse                2574.6482\n",
      "   6. distance_km                              1311.1937\n",
      "   7. is_rating_downgrade                      1175.1339\n",
      "   8. rating_ratio                             1133.1189\n",
      "   9. dst_is_highly_rated                      935.9001\n",
      "  10. relative_results_rank                    934.7921\n",
      "  11. is_in_relative_results                   905.5430\n",
      "  12. rating_diff                              776.4322\n",
      "  13. is_rating_upgrade                        731.9669\n",
      "  14. user_pref_x_dst_rating                   641.0509\n",
      "  15. distance_km_squared                      636.4898\n",
      "  16. diversity_x_different_cat                621.2051\n",
      "  17. rating_diff_squared                      603.5466\n",
      "  18. src_is_highly_rated                      577.3151\n",
      "  19. src_rating                               517.3604\n",
      "  20. rating_upgrade_x_distance                508.9921\n",
      "\n",
      "Importance by Feature Group:\n",
      "  category            : 11349.5908 total, 1261.0656 avg ( 9 features)\n",
      "  spatial             : 9353.3483 total, 322.5293 avg (29 features)\n",
      "  quality             : 8883.7017 total, 683.3617 avg (13 features)\n",
      "  relationship        : 2328.3440 total, 776.1147 avg ( 3 features)\n",
      "  price               : 827.6360 total,  59.1169 avg (14 features)\n",
      "  user_behavior       : 825.9164 total,  48.5833 avg (17 features)\n",
      "  temporal            : 696.3362 total,  36.6493 avg (19 features)\n",
      "  review_sentiment    : 290.4040 total,  26.4004 avg (11 features)\n",
      "  service_options     : 115.0847 total,   8.8527 avg (13 features)\n",
      "  topics              :  74.8457 total,   9.3557 avg ( 8 features)\n",
      "  operating_hours     :  38.1348 total,   4.2372 avg ( 9 features)\n",
      "  cuisine_complementarity:   0.0000 total,   0.0000 avg ( 0 features)\n",
      "\n",
      "âœ“ Feature importance analysis complete!\n",
      "\n",
      "  Top feature categories:\n",
      "    category            : 11349.5908\n",
      "    spatial             : 9353.3483\n",
      "    quality             : 8883.7017\n",
      "    relationship        : 2328.3440\n",
      "    price               : 827.6360\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance\n",
    "print(\"\\n[Step 5/5] Analyzing feature importance...\")\n",
    "feature_importance = analyze_feature_importance(model, feature_names)\n",
    "\n",
    "print(\"\\nâœ“ Feature importance analysis complete!\")\n",
    "print(\"\\n  Top feature categories:\")\n",
    "sorted_groups = sorted(feature_importance['group_totals'].items(), \n",
    "                      key=lambda x: x[1], reverse=True)\n",
    "for group, total_score in sorted_groups[:5]:\n",
    "    print(f\"    {group:20s}: {total_score:8.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase B2: Transition Matrix Generation\n",
    "\n",
    "**What this does:**\n",
    "- Generates Aâ†’B transition probability matrix for visualization\n",
    "- Creates top-K predictions for each source restaurant\n",
    "- Exports in multiple formats (Parquet, JSON, sparse matrix)\n",
    "\n",
    "**Expected time:** 10-30 minutes (depending on number of restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ“ All phases completed successfully!\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  - Recall@10: {metrics.get('recall@10', 0):.4f}\")\n",
    "print(f\"  - MRR: {metrics.get('mrr', 0):.4f}\")\n",
    "if 'ndcg@10' in metrics:\n",
    "    print(f\"  - nDCG@10: {metrics.get('ndcg@10', 0):.4f}\")\n",
    "\n",
    "print(f\"\\nData Locations:\")\n",
    "print(f\"  - Processed data: {PROCESSED_DIR}\")\n",
    "print(f\"  - Model data: {PROCESSED_DIR / 'xgboost_data'}\")\n",
    "\n",
    "print(f\"\\n Model is ready for evaluation and inference!\")\n",
    "print(f\"   Key metrics indicate {'realistic' if metrics.get('recall@10', 0) < 0.9 else 'very high'} performance.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
