{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Business Sequence Prediction Training\n",
    "\n",
    "This notebook trains an LSTM model to predict the next business a user will visit.\n",
    "\n",
    "## Model Architecture\n",
    "- **Input**: Sequence of business IDs (max length 20)\n",
    "- **Embedding**: 128-dimensional embeddings for 20,002 businesses\n",
    "- **LSTM**: 2 layers with 256 hidden units and 0.3 dropout\n",
    "- **Output**: Softmax over 20,002 businesses\n",
    "\n",
    "## Data Split Strategy\n",
    "- **Train/Val**: Non-Atlanta users only (zero data leakage)\n",
    "- **Test**: Atlanta users only (final inference set)\n",
    "\n",
    "## GPU Configuration\n",
    "- Automatically detects and uses GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "else:\n",
    "    print('⚠️  No GPU detected. Training will be slow on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path('../data/processed/ga/lstm_data')\n",
    "\n",
    "# Load vocabulary\n",
    "with open(DATA_DIR / 'business_vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "pad_idx = vocab['<PAD>']\n",
    "unk_idx = vocab['<UNK>']\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size:,}')\n",
    "print(f'PAD index: {pad_idx}')\n",
    "print(f'UNK index: {unk_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print('Loading data...')\n",
    "train_df = pl.read_parquet(DATA_DIR / 'business_train.parquet')\n",
    "val_df = pl.read_parquet(DATA_DIR / 'business_val.parquet')\n",
    "test_df = pl.read_parquet(DATA_DIR / 'business_test.parquet')\n",
    "\n",
    "print(f'Train: {len(train_df):,} examples')\n",
    "print(f'Val:   {len(val_df):,} examples')\n",
    "print(f'Test:  {len(test_df):,} examples (Atlanta only)')\n",
    "\n",
    "print('\\nSample:')\n",
    "print(train_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessSequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sequences = torch.tensor(df['input_seq'].to_list(), dtype=torch.long)\n",
    "        self.targets = torch.tensor(df['target'].to_list(), dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BusinessSequenceDataset(train_df)\n",
    "val_dataset = BusinessSequenceDataset(val_df)\n",
    "test_dataset = BusinessSequenceDataset(test_df)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f'DataLoaders created:')\n",
    "print(f'  Train batches: {len(train_loader)}')\n",
    "print(f'  Val batches: {len(val_loader)}')\n",
    "print(f'  Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3, pad_idx=0):\n",
    "        super(BusinessLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, \n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        out = self.dropout(last_hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Create model\n",
    "model = BusinessLSTM(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, pad_idx)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Model on device: {next(model.parameters()).device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training settings\n",
    "MAX_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "CHECKPOINT_DIR = Path('../models/business_lstm')\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Training configuration:')\n",
    "print(f'  Max epochs: {MAX_EPOCHS}')\n",
    "print(f'  Early stopping patience: {EARLY_STOPPING_PATIENCE}')\n",
    "print(f'  Learning rate: {LEARNING_RATE}')\n",
    "print(f'  Batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sequences, targets in tqdm(loader, desc='Training'):\n",
    "        sequences, targets = sequences.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in tqdm(loader, desc='Evaluating'):\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def calculate_topk_accuracy(model, loader, device, k_values=[1, 5, 10]):\n",
    "    model.eval()\n",
    "    topk_correct = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in tqdm(loader, desc='Top-K'):\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            outputs = model(sequences)\n",
    "            for k in k_values:\n",
    "                _, topk_pred = outputs.topk(k, dim=1)\n",
    "                correct = topk_pred.eq(targets.view(-1, 1).expand_as(topk_pred)).any(dim=1)\n",
    "                topk_correct[k] += correct.sum().item()\n",
    "            total += targets.size(0)\n",
    "    return {k: topk_correct[k] / total for k in k_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print('Starting training...')\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    print(f'\\nEpoch {epoch}/{MAX_EPOCHS}')\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "    print(f'Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, CHECKPOINT_DIR / 'best_model.pt')\n",
    "        print(f'✓ Saved best model')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'Early stopping: {patience_counter}/{EARLY_STOPPING_PATIENCE}')\n",
    "    \n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "print(f'\\nTraining complete! Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['val_loss'], label='Val')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'], label='Val')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set (Atlanta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(CHECKPOINT_DIR / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f'Loaded best model from epoch {checkpoint[\"epoch\"]}')\n",
    "\n",
    "# Evaluate on test set\n",
    "print('\\nEvaluating on Atlanta test set...')\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Top-K accuracy\n",
    "topk_acc = calculate_topk_accuracy(model, test_loader, device, k_values=[1, 5, 10, 20])\n",
    "print('\\nTop-K Accuracy:')\n",
    "for k, acc in topk_acc.items():\n",
    "    print(f'  Top-{k:2d}: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, loader, device, top_k=10):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in tqdm(loader, desc='Predictions'):\n",
    "            sequences = sequences.to(device)\n",
    "            outputs = model(sequences)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            topk_probs, topk_indices = probs.topk(top_k, dim=1)\n",
    "            all_predictions.extend(topk_indices.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_scores.extend(topk_probs.cpu().numpy())\n",
    "    return all_predictions, all_targets, all_scores\n",
    "\n",
    "predictions, targets, scores = generate_predictions(model, test_loader, device, top_k=10)\n",
    "print(f'Generated {len(predictions):,} predictions')\n",
    "\n",
    "# Save\n",
    "predictions_df = pl.DataFrame({\n",
    "    'target_business_idx': targets,\n",
    "    'predicted_business_indices': predictions,\n",
    "    'prediction_scores': scores\n",
    "})\n",
    "output_path = DATA_DIR / 'atlanta_business_predictions.parquet'\n",
    "predictions_df.write_parquet(output_path)\n",
    "print(f'✓ Saved to {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('BUSINESS LSTM TRAINING SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Best Val Loss: {best_val_loss:.4f}')\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "print('\\nTop-K Accuracy:')\n",
    "for k, acc in topk_acc.items():\n",
    "    print(f'  Top-{k:2d}: {acc*100:.2f}%')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
